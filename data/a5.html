<!doctype html>
<html lang="ja">
    <head>
        <script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
        <meta charset="utf-8" />
        <title>ä»˜éŒ²5</title>
        <style type="text/css">
            p
            {
                padding-left: 2em;
            }
            .margin-large
            {
                margin-left: 30px;
            }
           .margin-abstract {
               margin-left: 60px; /* å·¦ãƒãƒ¼ã‚¸ãƒ³ã‚’åºƒãã™ã‚‹ */
               margin-right: 60px; /* å³ãƒãƒ¼ã‚¸ãƒ³ã‚’åºƒãã™ã‚‹ */
           }
        </style>
    <style>
        .two-columns {
            display: flex;
            flex-direction: row;
            gap: 20px; /* åˆ—é–“ã®ã‚¹ãƒšãƒ¼ã‚¹ */
        }
        .column {
            flex: 1; /* å„åˆ—ãŒå‡ç­‰ã«å¹…ã‚’å–ã‚‹ */
        }
    </style>
<style>
.three-columns {
  display: flex;
  gap: 10px; /* åˆ—é–“ã®ä½™ç™½ã‚’è¨­å®š */
}
.column {
  flex: 1; /* å„åˆ—ã®å¹…ã‚’å‡ç­‰ã«ã™ã‚‹ */
  padding: 10px; /* å†…å´ã®ä½™ç™½ã‚’è¨­å®š */
}
</style>
    <style>
        .styleRef { 
            text-indent: -40px; /* æœ€åˆã®è¡Œã®å­—ä¸‹ã’ã‚’é€†æ–¹å‘ã« */
            margin-left: 10px; /* 2è¡Œç›®ä»¥é™ã®å­—ä¸‹ã’ã‚’èª¿æ•´ */
            ul {
                  list-style-type: none; /* ç®‡æ¡æ›¸ãè¨˜å·ã‚’éè¡¨ç¤º */
                  padding-left: 40px; /* å…¨ä½“ã®å·¦ä½™ç™½ */
            }
            li {
            }
        }
    </style>
    <style>
        .styleBullet { 
            text-indent: -20px; /* æœ€åˆã®è¡Œã®å­—ä¸‹ã’ã‚’é€†æ–¹å‘ã« */
            margin-left: 30px; /* 2è¡Œç›®ä»¥é™ã®å­—ä¸‹ã’ã‚’èª¿æ•´ */
            ul {
                  list-style-type: none; /* ç®‡æ¡æ›¸ãè¨˜å·ã‚’éè¡¨ç¤º */
                  padding-left: 0px; /* å…¨ä½“ã®å·¦ä½™ç™½ */
            }
            li {
            }
        }
    </style>
    <style>
            ol
            {
                margin-left: 30px;
            }
            ul
            {
                margin-left: 30px;
            }
    </style>
    </head>
    <body>
        <h1><center>ä»˜éŒ²5</center><center>æœ€å°äºŒä¹—æ³•ã«ã‚ˆã‚‹æœ€å°åŒ–</center></h1>
<p>
ã“ã®ä»˜éŒ²ã§ã¯ã€æ§˜ã€…ãªåˆ¶ç´„æ¡ä»¶ã®ä¸‹ã§ã®ç·šå½¢æ–¹ç¨‹å¼ã‚’è§£ãæ•°å€¤ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã¤ã„ã¦è«–ã˜ã¾ã™ã€‚ã”è¦§ã®ã¨ãŠã‚Šã€ã“ã®ã‚ˆã†ãªå•é¡Œã¯ç‰¹ç•°å€¤åˆ†è§£ï¼ˆSVDï¼‰ã‚’ç”¨ã„ã¦å®¹æ˜“ã«è§£ãã“ã¨ãŒã§ãã¾ã™ã€‚

<!-- In this appendix we discuss numerical algorithms for solving linear systems of equations
under various constraints. As will be seen such problems are conveniently solved
using the SVD. -->
</p>
<h2><center>A5.1 ç·šå½¢æ–¹ç¨‹å¼ã®è§£</center></h2>
<p>
\(A\mathbf x = \mathbf b\) ã¨ã„ã†å½¢å¼ã®é€£ç«‹æ–¹ç¨‹å¼ã‚’è€ƒãˆã¾ã™ã€‚\(A\) ã‚’ \(m Ã— n\) è¡Œåˆ—ã¨ã—ã¾ã™ã€‚æ¬¡ã®3ã¤ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
<div class="styleBullet">
<ul>
<li>(i) \(m < n\) ã®å ´åˆã€æ–¹ç¨‹å¼ã®æ•°ã‚ˆã‚Šã‚‚æœªçŸ¥æ•°ã®ã»ã†ãŒå¤šããªã‚Šã¾ã™ã€‚ã“ã®å ´åˆã€è§£ã¯ä¸€æ„ã§ã¯ãªãã€è§£ã®ãƒ™ã‚¯ãƒˆãƒ«ç©ºé–“ãŒå­˜åœ¨ã—ã¾ã™ã€‚
</li><br><li>(ii) \(m = n\) ã®å ´åˆã€\(A\) ãŒé€†è¡Œåˆ—ã§ã‚ã‚‹é™ã‚Šã€è§£ã¯ä¸€æ„ã§ã™ã€‚
</li><br><li>(iii) \(m > n\) ã®å ´åˆã€æ–¹ç¨‹å¼ã®æ•°ã®æ–¹ãŒæœªçŸ¥æ•°ã®ã»ã†ãŒå¤šããªã‚Šã¾ã™ã€‚ä¸€èˆ¬ã«ã€\(b\) ãŒ \(A\) ã®åˆ—ã®ç¯„å›²å†…ã«å¶ç„¶å«ã¾ã‚Œãªã„é™ã‚Šã€ã“ã®é€£ç«‹æ–¹ç¨‹å¼ã«ã¯è§£ã¯å­˜åœ¨ã—ã¾ã›ã‚“ã€‚</li>
</ul>
</div>

<!--
Consider a system of equations of the form \(A\mathbf x = \mathbf b\). Let \(A\) be an \(m Ã— n\) matrix. There
are three possibilities:
(i) If \(m < n\) there are more unknowns than equations. In this case, there will not
be a unique solution, but rather a vector space of solutions.
(ii) If \(m = n\) there will be a unique solution as long as A is invertible.
(iii) If \(m > n\) then there are more equations than unknowns. In general the system
will not have a solution unless by chance b lies in the span of the columns of A. -->

</p><p>
<strong>æœ€å°äºŒä¹—è§£ï¼šãƒ•ãƒ«ãƒ©ãƒ³ã‚¯ã®å ´åˆ</strong><br>
\(m â‰¥ n\) ã®å ´åˆã‚’è€ƒãˆã€\(A\) ã®ãƒ©ãƒ³ã‚¯ãŒ \(n\) ã§ã‚ã‚‹ã¨ä»®å®šã—ã¾ã™ã€‚è§£ãŒå­˜åœ¨ã—ãªã„å ´åˆã§ã‚‚ã€å¤šãã®å ´åˆã€ã‚·ã‚¹ãƒ†ãƒ  \(A\mathbf x = \mathbf b\) ã®è§£ã«æœ€ã‚‚è¿‘ã„ãƒ™ã‚¯ãƒˆãƒ« \(\mathbf x\) ã‚’æ±‚ã‚ã‚‹ã“ã¨ã¯ç†ã«ã‹ãªã£ã¦ã„ã¾ã™ã€‚è¨€ã„æ›ãˆã‚Œã°ã€\(||A\mathbf x âˆ’ \mathbf b||\) ãŒæœ€å°ã¨ãªã‚‹ã‚ˆã†ãª \(\mathbf x\) ã‚’æ±‚ã‚ã¾ã™ã€‚ã“ã“ã§ã€\(||\cdot||\) ã¯ãƒ™ã‚¯ãƒˆãƒ«ãƒãƒ«ãƒ ã‚’è¡¨ã—ã¾ã™ã€‚ã“ã®ã‚ˆã†ãª \(\mathbf x\) ã¯ã€éå‰°æ±ºå®šã‚·ã‚¹ãƒ†ãƒ ã®æœ€å°äºŒä¹—è§£ã¨ã—ã¦çŸ¥ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚æœ€å°äºŒä¹—è§£ã¯ã€æ¬¡ã®ã‚ˆã†ã« SVD ã‚’ç”¨ã„ã¦ç°¡å˜ã«æ±‚ã‚ã‚‰ã‚Œã¾ã™ã€‚

<!--
<strpng>Least-squares solutions: full-rank case.</strpng><br>
 We consider the case \(m â‰¥ n\) and assume
for the present that \(A\) is known to be of rank \(n\). If a solution does not exist, then in many
cases it still makes sense to seek a vector \(\mathbf x\) that is closest to providing a solution to the
system \(A\mathbf x = \mathbf b\). In other words, we seek \(\mathbf x\) such that \(||A\mathbf x âˆ’ \mathbf b||\)  is minimized, where \(||\cdot||\) represents the vector norm. Such an \(\mathbf x\) is known as the least-squares solution to
the over-determined system. The least-squares solution is conveniently found using the
SVD as follows.

</p><p>
We seek \(\mathbf x\) that minimizes \(||A\mathbf xâˆ’ \mathbf b|| = ||UDV^T\mathbf xâˆ’\mathbf b||\). Because of the norm-preserving
property of orthogonal transforms, kUDVTx âˆ’ bk = kDVTx âˆ’ UTbk, and this is the
quantity that we want to minimize. Writing y = VTx and bâ€² = UTb, the problem
becomes one of minimizing kDy âˆ’ bâ€²k where D is an m Ã— n matrix with vanishing
off-diagonal entries. This set of equations is of the form
\[
\]

Clearly, the nearest Dy can approach to bâ€² is the vector (bâ€²
1, bâ€²
2, . . . , bâ€²
n, 0, . . . , 0)T, and
this is achieved by setting yi = bâ€²
i/di for i = 1, . . . , n. Note that the assumption
rankA = n ensures that di 6= 0. Finally, one retrieves x from x = Vy. The complete
algorithm is

\(\underline{Objective}\)<br>
Find the least-squares solution to the m Ã— n set of equations Ax = b, where m > n and
rank A = n.
Algorithm
(i) Find the SVD A = UDVT.
(ii) Set bâ€² = UTb.
(iii) Find the vector y defined by yi = bâ€²
i/di, where di is the i-th diagonal entry of D.
(iv) The solution is x = Vy.
Algorithm A5.1. Linear least-squares solution to an over-determined full-rank set of linear equations.
Deficient-rank systems. Sometimes one is called upon to solve a system of equations
that is expected not to be of full column rank. Thus, let r = rankA < n, where n is the
number of columns of A. It is possible that because of noise corruption, the matrix A
actually has rank greater than r, but we wish to enforce the rank r constraint because
of theoretical considerations, derived from the particular problem being considered. In
this case, there will be an (nâˆ’r)-parameter family of solutions to the set of equations,
where r = rankA < n. This family of solutions is appropriately solved using the SVD,
as follows:
This algorithm gives an (nâˆ’r)-parameter family (parametrized by the indeterminate
values i) of least-squares solutions to the deficient-rank system. The justification of
this algorithm is similar to that of algorithm A5.1 for the least-squares solution of fullrank
systems.
Systems of unknown rank. In most cases encountered in this book, the rank of a
system of linear equations will be known theoretically in advance of solution. If the
rank of the system of equations is not known, then one must guess at its rank. In this
case, it is appropriate to set singular values that are small compared with the largest
590 Appendix 5 Least-squares Minimization
Objective
Find the general solution to a set of equations Ax = b where A is an m Ã— n matrix of rank
r < n.
Algorithm
(i) Find the SVD A = UDVT, where the diagonal entries di of D are in descending numerical
order.
(ii) Set bâ€² = UTb.
(iii) Find the vector y defined by yi = bâ€²
i/di for i = 1, . . . , r, and yi = 0 otherwise.
(iv) The solution x of minimum norm kxk is Vy.
(v) The general solution is x = Vy + r+1vr+1 + . . . + nvn, where vr+1, . . . , vn are
the last n âˆ’ r columns of V.
Algorithm A5.2. General solution to deficient-rank system
singular value to zero. Thus, if di/d0 <  where  is a small constant of the order of
the machine precision1 then one sets yi = 0. A least-squares solution is then given by
x = Vy as before.
A5.2 The pseudo-inverse
Given a square diagonal matrix D, we define its pseudo-inverse to be the diagonal matrix
D+ such that
D+
ii =
(
0 if Dii = 0
Dâˆ’1
ii otherwise.
Now, consider an m Ã— n matrix A with m â‰¥ n. Let the SVD of A be A = UDVT. We
define the pseudo-inverse of A to be the matrix
A+ = VD+U
T. (A5.1)
One very simply verifies that the vector y in algorithm A5.1 or algorithm A5.2 is
nothing more than D+bâ€² where bâ€² = UTb. Thus,
Result A5.1. The least-squares solution to an m Ã— n system of equations Ax = b of
rank n is given by x = A+b. In the case of a deficient-rank system, x = A+b is the
solution that minimizes kxk.
As remarked when discussing the SVD, if A has fewer rows than columns, then this
result may be applied after extending A to a square matrix by adding rows of zeros.
Symmetric matrices. For symmetric matrices, one may generalize the pseudoinverse
as follows. This generalization was used in chapter 5, section 5.2.3(p142) for
discussing singular covariance matrices. If A is a non-invertible symmetric matrix, and
XTAX is invertible, then we write A+X def = X(XTAX)âˆ’1XT. One can see that this depends
only on the span of the columns of X. In other words, if X is replaced by XB for any
invertible matrix B, then A+X = A+XB. Otherwise stated, A+X depends only on the (left)
1 Machine precision is the largest floating point value Ç« such that 1.0 + Ç« = 1.0.
A5.2 The pseudo-inverse 591
null-space of X, namely the space of vectors perpendicular to the columns of X. Define
the null-space NLX = {xT | xTX = 0}. One finds that under a simple condition, A+X is
the pseudo-inverse of A:
Result A5.2. Let A be a symmetric matrix, then A+X def = X(XTAX)âˆ’1XT = A+ if and only
if NL(X) = NL(A).
Only a sketch proof is given. The necessity is obvious, since NL(X) and NL(A) are
the null-spaces of the left and right sides of the equation. To prove the converse, one
may assume that the columns of X are orthonormal, since as shown above, only the
null-space of X is of importance. Thus, X may be extended by adding further columns
Xâ€² to form an orthogonal matrix U = [X|Xâ€²]. Now, the rows of Xâ€²T span the null-space
of X, and hence, by assumption, of A. Now, the proof is completed in a few lines by
comparing the definition A+X def = X(XTAX)âˆ’1XT with the definition (A5.1) of the pseudoinverse.
A5.2.1 Linear least-squares using normal equations
The linear least-squares problem may also be solved by a method involving the socalled
normal equations. Once more, we consider the set of linear equations Ax = b
where A is an m Ã— n matrix with m > n. In general, no solution x will exist for
this set of equations. Consequently, the task is to find the vector x that minimizes the
norm kAx âˆ’ bk. As the vector x varies over all values, the product Ax varies over the
complete column space of A, that is, the subspace of IRm spanned by the columns of
A. The task therefore is to find the closest vector to b that lies in the column space of
A, where closeness is defined in terms of vector norm. Let x be the solution to this
problem; thus Ax is the closest point to b. In this case, the difference Ax âˆ’ b must be
a vector orthogonal to the column space of A. This means, explicitly, that Ax âˆ’ b is
perpendicular to each of the columns of A, and hence AT(Axâˆ’b) = 0. Multiplying out
and separating terms gives an equation
(A
T
A)x = A
Tb. (A5.2)
This is a square n Ã— n set of linear equations, called the normal equations. This set
of equations may be solved to find the least-squares solution to the problem Ax = b.
Even if A is not of full rank (rank n), this set of equations should have a solution, since
ATb lies in the column space of ATA. In the case where A has rank n, the matrix ATA is
Objective
Find x that minimizes kAx âˆ’ bk.
Algorithm
(i) Solve the normal equations ATAx = ATb.
(ii) If ATA is invertible, then the solution is x = (ATA)âˆ’1ATb.
Algorithm A5.3. Linear least-squares using the normal equations.
592 Appendix 5 Least-squares Minimization
invertible, and so x may be found by x = (ATA)âˆ’1ATb. Since x = A+b, this implies
the following result, which is also easily verified directly:
Result A5.3. If A is an m Ã— n matrix of rank n, then A+ = (ATA)âˆ’1AT.
This result is useful in theoretical analysis, as well as being a computationally simpler
method than using the SVD to compute a pseudo-inverse if n is small compared with
m (so that computing the inverse of (ATA) is inexpensive compared to computing the
SVD of A).
Vector space norms. One sometimes wishes to minimize Ax âˆ’ b with respect to a
different norm on the vector space IRn. The usual norm in a vector space IRn is given in
terms of the usual inner product. Thus, for two vectors a and b in IRn one may define
the inner product a Â· b to be aTb. The norm of a vector a is then kak = (a Â· a)1/2 =
(aTa)1/2. One notes the properties:
(i) The inner product is a symmetric bilinear form on IRn.
(ii) kak > 0 for all non-zero vectors a âˆˆ IRn.
We say that the inner product is a positive-definite symmetric bilinear form. It is possible
to define other inner products on a vector space IRn. Let C be a real symmetric
positive-definite matrix, and define a new inner product C(a, b) = aTCb. The symmetry
of the inner product follows from the symmetry of C. A norm may be defined by
kakC = (aTCa)1/2, and this is defined and positive-definite, because C is assumed to be
a positive-definite matrix.
Weighted linear least-squares problems. Sometimes one desires to solve a weighted
least-squares problem of the form Axâˆ’b = 0 by minimizing the C-norm kAxâˆ’bkC of
the error. Here C is a positive-definite symmetric matrix defining an inner product and
a norm k Â· kC on IRn. As before, one can argue that the minimum error vector Ax âˆ’ b
must be orthogonal in the inner product defined by C to the column space of A. This
leads to a requirement ATC(Ax âˆ’ b) = 0. Rearranging this one obtains the weighted
normal equations:
(A
T
CA)x = A
T
Cb. (A5.3)
The most common weighting will be where C is a diagonal matrix, corresponding to
independent weights in each of the axial directions in IRn. However, general weighting
matrices C may be used also.
A5.3 Least-squares solution of homogeneous equations
Similar to the previous problem is that of solving a set of equations of the form Ax = 0.
This problem comes up frequently in reconstruction problems. We consider the case
where there are more equations than unknowns â€“ an over-determined set of equations.
The obvious solution x = 0 is not of interest â€“ we seek a non-zero solution to the set
of equations. Observe that if x is a solution to this set of equations, then so is kx for
any scalar k. A reasonable constraint would be to seek a solution for which kxk = 1.
A5.4 Least-squares solution to constrained systems 593
In general, such a set of equations will not have an exact solution. Suppose A has
dimension mÃ—n then there is an exact solution if and only if rank(A) < n â€“ the matrix
A does not have full column rank. In the absence of an exact solution we will normally
seek a least-squares solution. The problem may be stated as
â€¢ Find the x that minimizes kAxk subject to kxk = 1 .
This problem is solvable as follows. Let A = UDVT. The problem then requires us to
minimize kUDVTxk. However, kUDVTxk = kDVTxk and kxk = kVTxk. Thus, we need
to minimize kDVTxk subject to the condition kVTxk = 1. We write y = VTx, and
the problem is: minimize kDyk subject to kyk = 1. Now, D is a diagonal matrix with
its diagonal entries in descending order. It follows that the solution to this problem is
y = (0, 0, . . . , 0, 1)T having one non-zero entry, 1 in the last position. Finally x = Vy
is simply the last column of V. The method is summarized in algorithm A5.4.
Objective
Given a matrix A with at least as many rows as columns, find x that minimizes kAxk subject to
kxk = 1.
Solution
x is the last column of V, where A = UDVT is the SVD of A.
Algorithm A5.4. Least-squares solution of a homogeneous system of linear equations.
As mentioned in section A4.4 the last column of V may alternatively be described as
the eigenvector of ATA corresponding to the smallest eigenvalue.
A5.4 Least-squares solution to constrained systems
In the previous section, we considered a method of least-squares solution of equations
of the form Ax = 0. Such problems may arise from situations where measurements
are made on a set of image features. With exact measurements and an exact imaging
model the mathematical model predicts an exact solution to this system. In the case of
inexact image measurements, or noise, there will not be an exact solution. In this case,
it makes sense to find a least-squares solution.
On other occasions, however, some of the equations represented by rows of the matrix
A are derived from precise mathematical constraints, and should be satisfied exactly.
This set of constraints may be described by a matrix equation Cx = 0, which
should be satisfied exactly. Others of the equations are derived from image measurements
and are subject to noise. This leads to a problem of the following sort:
â€¢ Find the x that minimizes kAxk subject to kxk = 1 and Cx = 0 .
This problem can be solved in the following manner. The condition that x satisfies
Cx = 0 means that x lies perpendicular to each of the rows of C. The set of all such x
is a vector space called the orthogonal complement of the row space of C. We wish to
find this orthogonal complement.
594 Appendix 5 Least-squares Minimization
First, if C has fewer rows than columns, then extend it to a square matrix by adding
rows of zero elements. This has no effect on the set of constraints Cx = 0. Now, let
C = UDVT be the Singular Value Decomposition of C, where D is a diagonal matrix
with r non-zero diagonal entries. In this case, C has rank r and the row-space of C
is generated by the first r rows of VT. The orthogonal complement of the row-space
of C consists of the remaining rows of VT. Define CâŠ¥ to be the matrix V with the first
r columns deleted. Then CCâŠ¥ = 0, and so the set of vectors x satisfying Cx = 0 is
spanned by the columns of CâŠ¥ and we may write any such x as x = CâŠ¥xâ€² for suitable
xâ€². Since CâŠ¥ has orthogonal columns, one observes that kxk = kCâŠ¥xâ€²k = kxâ€²k. The
minimization problem now becomes
â€¢ Find the xâ€² that minimizes kACâŠ¥xâ€²k subject to kxâ€²k = 1 .
This is simply an instance of the problem discussed in section A5.3, solved by
algorithm A5.4. The complete algorithm for solution of the constrained minimization
problem is given as algorithm A5.5.
Objective
Given an m Ã— n matrix A with m â‰¥ n, find the vector x that minimizes kAxk subject to
kxk = 1 and Cx = 0.
Algorithm
(i) If C has fewer rows than columns, then add zero-filled rows to C to make it square.
Compute the SVD C = UDVT where diagonal entries of D are sorted with non-zero ones
first. Let CâŠ¥ be the matrix obtained from V by deleting the first r columns of V, where
r is the number of non-zero entries in D (the rank of C).
(ii) Find the solution to the minimization problem ACâŠ¥xâ€² = 0 using algorithm A5.4. The
solution is given by x = CâŠ¥xâ€².
Algorithm A5.5. Algorithm for constrained minimization.
A5.4.1 More constrained minimization
A further constrained minimization problem arises in the algebraic estimation method
used frequently throughout this book â€“ for instance, for computation of the fundamental
matrix (section 11.3(p282)) or the trifocal tensor (section 16.3(p395)).
The problem is:
â€¢ Minimize kAxk subject to kxk = 1 and x = GË†x for a given matrix G and some
unknown vector Ë†x.
Note that this is very similar to the previous minimization problem of section A5.4,
which was reduced to the form of the present problem in which the matrix G had orthonormal
columns. The condition that x = GË†x for some Ë†x means nothing more than
that x lies in the span of the columns of G. Thus, to solve the present problem using
algorithm A5.5, we need only to replace G by a matrix with the same column space (i.e.
the space spanned by the columns), but with orthonormal columns. If G = UDVT where
D has r non-zero entries (that is G has rank r), then let Uâ€² be the matrix consisting of the
A5.4 Least-squares solution to constrained systems 595
first r columns of U. Then G and Uâ€² have the same column space. As in section A5.4 the
solution is found by setting xâ€² to be the unit vector that minimizes kAUâ€²xâ€²k, then setting
x = Uâ€²xâ€².
If Ë†x is also required, then it may be obtained by solving GË†x = x = Uâ€²xâ€². The solution
is expressed in terms of the pseudo-inverse (section A5.2) as Ë†x = G+x = G+Uâ€²xâ€² ; it
may not be unique if G does not have full column rank. Since G+ = VD+UT, we may
write Ë†x = VD+UTUâ€²xâ€², which simplifies to Ë†x = Vâ€²Dâ€²âˆ’1xâ€² where Vâ€² consists of the first r
columns of Vâ€² and Dâ€² is the upper r Ã— r block of D.
The complete method is summarized in algorithm A5.6.
Objective
Find the vector x that minimizes kAxk subject to the conditions kxk = 1 and x = GË†x, where
G has rank r.
Algorithm
(i) Compute the SVD G = UDVT, where the non-zero values of D appear first down the
diagonal.
(ii) Let Uâ€² be the matrix comprising the first r columns of U.
(iii) Find the unit vector xâ€² that minimizes kAUâ€²xâ€²k, using algorithm A5.4.
(iv) The required solution is x = Uâ€²xâ€².
(v) If desired, one may compute Ë†x as Vâ€²Dâ€²âˆ’1xâ€², where Vâ€² consists of the first r columns of
V and Dâ€² is the upper r Ã— r block of D.
Algorithm A5.6. Algorithm for constrained minimization, subject to a span-space constraint.
A5.4.2 Yet another minimization problem
A very similar problem is
â€¢ Minimize kAxk subject to a condition kCxk = 1.
This problem comes up for instance in the solution to the DLT camera calibration
problem (section 7.3(p184)). In general it will be the case that rankC < n where n is
the dimension of the vector x. Geometrically the problem may be thought of as finding
the â€œlowestâ€ point on a quadratic surface (specified by xTATAx), with the constraint
that the point must lie on the (inhomogeneous) â€œconicâ€ xTCTCx = 1.
We start by taking the SVD of the matrix C, obtaining C = UDVT. The condition
kUDVTxk = 1 is equivalent to kDVTxk = 1, and it is not necessary to compute U
explicitly. Then writing xâ€² = VTx, the problem becomes: minimize kAVxâ€²k subject to
the condition kDxâ€²k = 1 Writing Aâ€² = AV, this becomes: minimize kAâ€²xâ€²k subject to
kDxâ€²k = 1. Thus we have reduced to the case where the constraint matrix is a diagonal
matrix, D.
We suppose that D has r non-zero diagonal entries, and s zero entries, where r +s =
n, the non-zero entries appearing first on the diagonal of D. Then the entries xâ€²
i of xâ€²
for i > r do not affect the value of kDxâ€²
ik, since the corresponding diagonal entries of
D are zero. Then, for a specific choice of the xâ€²
i for i = 1, . . . , r, the other entries xâ€²
i,
596 Appendix 5 Least-squares Minimization
i = r + 1, . . . , n should be chosen so as to minimize the value of kAâ€²xâ€²
ik. We write
Aâ€² = [Aâ€²
1 | Aâ€²
2] where Aâ€²
1 consists of the first r columns of Aâ€² and Aâ€²
2 consists of the
remaining s columns. Similarly, let xâ€²
1 be an r-vector consisting of the first r elements
of xâ€², and let xâ€²
2 consist of the remaining s elements of xâ€². Further, let D1 be the r Ã— r
diagonal matrix consisting of the first r diagonal entries of D. Then Aâ€²xâ€² = Aâ€²
1xâ€²
1+Aâ€²
2xâ€²
2,
and the minimization problem is to minimize
kA
â€²
1xâ€²
1 + A
â€²
2xâ€²
2k (A5.4)
subject to the condition kD1xâ€²
1k = 1. Now, temporarily fixing xâ€²
1, (A5.4) takes the
form of a least-squares minimization problem of the type discussed in section A5.1.
According to result A5.1, the value of xâ€²
2 that minimizes (A5.4) is xâ€²
2 = âˆ’Aâ€²
2
+
Aâ€²
1xâ€²
1.
Substituting this in (A5.4) gives k(Aâ€²
2
Aâ€²
2
+âˆ’I)Aâ€²
1xâ€²
1k, which we are required to minimize,
subject to the condition kD1xâ€²
1k = 1. Finally, writing xâ€²â€² = D1xâ€²
1, the problem reduces
at last to a problem of the form of the familiar minimization problem of algorithm A5.4.
â€¢ Minimize k(Aâ€²
2
Aâ€²
2
+ âˆ’ I)Aâ€²
1
D
âˆ’1
1 xâ€²â€²
1k, subject to kxâ€²â€²k = 1.
We now summarize the algorithm.
Objective
Minimize kAxk subject to kCxk = 1.
Algorithm
(i) Compute the SVD C = UDVT, and write Aâ€² = AV.
(ii) Suppose rankD = r and let Aâ€² = [Aâ€²
1 | Aâ€²
2] where Aâ€²
1 consists of the first r columns of
Aâ€², and Aâ€²
2 is formed from the remaining columns.
(iii) Let D1 be the upper r Ã— r minor of D.
(iv) Compute Aâ€²â€² = (Aâ€²
2
Aâ€²
2
+ âˆ’ I)Aâ€²
1
D
âˆ’1
1 . This is an n Ã— r matrix.
(v) Minimize kAâ€²â€²xâ€²k subject to kxâ€²â€²k = 1 using algorithm A5.4.
(vi) Set xâ€²
1 = D
âˆ’1
1 xâ€²â€², and xâ€²
2 = âˆ’Aâ€²
2
+
Aâ€²
1xâ€²
1. Let xâ€² =

xâ€²
1
xâ€²
2

.
(vii) The solution is given by x = Vxâ€².
Algorithm A5.7. Least-squares solution of homogeneous equations subject to the constraint kCxk = 1.-->
</p><p>
</p><p>
    </body>
</html>