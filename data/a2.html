<!doctype html>
<html lang="ja">
    <head>
        <script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
        <meta charset="utf-8" />
        <title>ä»˜éŒ²2</title>
        <style type="text/css">
            p
            {
                padding-left: 2em;
            }
            .margin-large
            {
                margin-left: 30px;
            }
           .margin-abstract {
               margin-left: 60px; /* å·¦ãƒãƒ¼ã‚¸ãƒ³ã‚’åºƒãã™ã‚‹ */
               margin-right: 60px; /* å³ãƒãƒ¼ã‚¸ãƒ³ã‚’åºƒãã™ã‚‹ */
           }
        </style>
    <style>
        .two-columns {
            display: flex;
            flex-direction: row;
            gap: 20px; /* åˆ—é–“ã®ã‚¹ãƒšãƒ¼ã‚¹ */
        }
        .column {
            flex: 1; /* å„åˆ—ãŒå‡ç­‰ã«å¹…ã‚’å–ã‚‹ */
        }
    </style>
<style>
.three-columns {
  display: flex;
  gap: 10px; /* åˆ—é–“ã®ä½™ç™½ã‚’è¨­å®š */
}
.column {
  flex: 1; /* å„åˆ—ã®å¹…ã‚’å‡ç­‰ã«ã™ã‚‹ */
  padding: 10px; /* å†…å´ã®ä½™ç™½ã‚’è¨­å®š */
}
</style>
    <style>
        .styleRef { 
            text-indent: -40px; /* æœ€åˆã®è¡Œã®å­—ä¸‹ã’ã‚’é€†æ–¹å‘ã« */
            margin-left: 10px; /* 2è¡Œç›®ä»¥é™ã®å­—ä¸‹ã’ã‚’èª¿æ•´ */
            ul {
                  list-style-type: none; /* ç®‡æ¡æ›¸ãè¨˜å·ã‚’éè¡¨ç¤º */
                  padding-left: 40px; /* å…¨ä½“ã®å·¦ä½™ç™½ */
            }
            li {
            }
        }
    </style>
    <style>
        .styleBullet { 
            text-indent: -20px; /* æœ€åˆã®è¡Œã®å­—ä¸‹ã’ã‚’é€†æ–¹å‘ã« */
            margin-left: 30px; /* 2è¡Œç›®ä»¥é™ã®å­—ä¸‹ã’ã‚’èª¿æ•´ */
            ul {
                  list-style-type: none; /* ç®‡æ¡æ›¸ãè¨˜å·ã‚’éè¡¨ç¤º */
                  padding-left: 0px; /* å…¨ä½“ã®å·¦ä½™ç™½ */
            }
            li {
            }
        }
    </style>
    <style>
            ol
            {
                margin-left: 30px;
            }
            ul
            {
                margin-left: 30px;
            }
    </style>
    </head>
    <body>
        <h1><center>ä»˜éŒ²2</center><center>ã‚¬ã‚¦ã‚¹åˆ†å¸ƒ(æ­£è¦åˆ†å¸ƒ)ã¨\(\chi^2\)åˆ†å¸ƒ</center></h1>

<h2><center>A2.1 ã‚¬ã‚¦ã‚¹ç¢ºç‡åˆ†å¸ƒ</center></h2>
<p>
ç¢ºç‡å¤‰æ•° \(x_i\; i = 1, . . . ,N\) ã®ãƒ™ã‚¯ãƒˆãƒ« \(\mathbf X\) ãŒä¸ãˆã‚‰ã‚Œã€å¹³å‡ \(\overline{\mathbf X} = E[\mathbf X]\)ã€ã“ã“ã§ \(E[Â·]\) ã¯æœŸå¾…å€¤ã€\(\Delta \mathbf X = \mathbf X âˆ’ \overline{\mathbf X}\) , å…±åˆ†æ•£è¡Œåˆ— \(\Sigma\)ã¯ \(N Ã— N\) è¡Œåˆ—ã§ã‚ã‚Šã€æ¬¡å¼ã§ä¸ãˆã‚‰ã‚Œã‚‹ã€‚

<!-- Given a vector \(\mathbf X\) of random variables \(x_i\; for i = 1, . . . ,N\), with mean \(overline{\mathbf X} = E[\mathbf X]\), where \(E[Â·]\) represents the expected value, and \(\Delta \mathbf X = \mathbf X âˆ’ \overline{\mathbf X}\), the covariance matrix \(\Sigma\) is an \(N Ã— N\) matrix given by -->
\[
\Sigma = E[\Delta\mathbf X\,\Delta\mathbf X^T]
\]
ã¤ã¾ã‚Šã€\(\sum_{ij} = E[\Delta x_i\,\Delta x_j]\) ã¨ãªã‚Šã¾ã™ã€‚è¡Œåˆ— \(\Sigma\) ã®å¯¾è§’è¦ç´ ã¯å€‹ã€…ã®å¤‰æ•° \(x_i\) ã®åˆ†æ•£ã§ã‚ã‚Šã€éå¯¾è§’è¦ç´ ã¯ç›¸äº’å…±åˆ†æ•£å€¤ã§ã™ã€‚

<!-- so that \(\sum_{ij} = E[\Delta x_i\,\Delta x_j]\). The diagonal entries of the matrix \(\Sigma\) are the variances of the individual variables \(x_i\), whereas the off-diagonal entries are the cross-covariance values. -->

</p><p>
å¤‰æ•° \(x_i\) ã¯ã€\(\mathbf X\) ã®ç¢ºç‡åˆ†å¸ƒãŒæ¬¡ã®å½¢å¼ã§ã‚ã‚‹å ´åˆã€çµåˆã‚¬ã‚¦ã‚¹åˆ†å¸ƒã«å¾“ã†ã¨ã„ã†ã€‚

<!-- The variables xi are said to conform to a joint Gaussian distribution, if the probability
distribution of X is of the form -->

\[
P(\overline{\mathbf X} + \Delta\mathbf X) = (2\pi)^{-N/2} det(\Sigma^{-1})^{1/2} exp
\Big(-(\Delta\mathbf X)^T\Sigma^{-1}(\Delta\mathbf X)/2\Big) \tag{A2.1}
\]

ã‚ã‚‹åŠæ­£å®šå€¤è¡Œåˆ— \(\Sigma^{-1}\) ã«ã¤ã„ã¦ã€\(\overline{\mathbf X}\) ã¨ \(\Sigma\) ã¯åˆ†å¸ƒã®å¹³å‡ã¨å…±åˆ†æ•£ã§ã‚ã‚‹ã“ã¨ãŒæ¤œè¨¼ã§ãã‚‹ã€‚ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã¯ã€ãã®å¹³å‡ã¨å…±åˆ†æ•£ã«ã‚ˆã£ã¦ä¸€æ„ã«æ±ºå®šã•ã‚Œã‚‹ã€‚ä¿‚æ•° \((2\pi)^{-N/2} det(\Sigma^{-1})^{1/2}\) ã¯ã€åˆ†å¸ƒã®å…¨ç©åˆ†ã‚’ 1 ã«ã™ã‚‹ãŸã‚ã«å¿…è¦ãªæ­£è¦åŒ–ä¿‚æ•°ã§ã‚ã‚‹ã€‚

<!-- for some positive-semidefinite matrix \(\Sigma^{-1}\). It may be verified that \(\overline{\mathbf X}\) and \(\Sigma\) are the mean
and covariance of the distribution. A Gaussian distribution is uniquely determined by
its mean and covariance. The factor \((2\pi)^{-N/2} det(\Sigma^{-1})^{1/2}\)  is just the normalizing factor
necessary to make the total integral of the distribution equal to 1. -->

</p><p>
\(\Sigma\) ãŒã‚¹ã‚«ãƒ©ãƒ¼è¡Œåˆ— \(\Sigma=\sigma^2I\) ã§ã‚ã‚‹ç‰¹åˆ¥ãªã‚±ãƒ¼ã‚¹ã§ã¯ã€ã‚¬ã‚¦ã‚¹åˆ†å¸ƒã®ç¢ºç‡å¯†åº¦é–¢æ•°ã¯
å˜ç´”ãªå½¢ã‚’ã¨ã‚‹ã€‚

<!-- In the special case where \(\Sigma\) is a scalar matrix \(\Sigma=\sigma^2I\) the Gaussian PDF takes a
simple form -->
\[
P(\mathbf X) = (\sqrt{2\pi}\sigma)^{-N} exp\Big(-\sum_{i=1}^N (x_i-\overline{x}_i)^2/2\sigma^2\Big)
\]
<!--
where \(\mathbf X = (x_1, x_2, . . . , x_N)^T\). This distribution is called an isotropic Gaussian distribution.<br>
<br>

<strong>Mahalanobis distance. </strong><br>
Note that in this case the value of the PDF at a point X is
simply a function of the Euclidean distance \(\Big(\sum_{i=1}^N (x_i-\overline{x}_i)^2\Big)^{1/2}\) of the point \(\mathbf X\) from the mean \(\overlineX = (Â¯x1, . . . , Â¯xN)T. By analogy with this one may define the Mahalanobis
distance between two vectors X and Y to be
\[
||\mathbf X - \mathbf Y||_\Sigma=\Big((\mathbf X - \mathbf Y)^T\Sigma^{-1}(\mathbf X - \mathbf Y)\Big)^{1/2}
\]

One verifies that for a positive-definite matrix , this defines a metric on IRN. Using
this notation, the general form of the Gaussian PDF may be written as
P(X) â‰ˆ exp

âˆ’kX âˆ’ Xk2
/2

where the normalizing factor has been omitted. Thus, the value of the Gaussian PDF is
a function of the Mahalanobis distance of the point X from the mean.
Change of coordinates. Since  is symmetric and positive-definite, it may be written
as  = UTDU, where U is an orthogonal matrix and D = (2
1, 2
2, . . . , 2
N) is diagonal.
Writing Xâ€² = UX and X
â€² = UX, and substituting in (A2.1), leads to
exp

âˆ’(X âˆ’ X)T
âˆ’1(X âˆ’ X)/2

= exp

âˆ’(X
â€² âˆ’ X
â€²)T
Uâˆ’1U
T(X
â€² âˆ’ X
â€²)/2

= exp

âˆ’(X
â€² âˆ’ X
â€²)T
Dâˆ’1(X
â€² âˆ’ X
â€²)/2

Thus, the orthogonal change of coordinates from X to Xâ€² = UX transforms a general
Gaussian PDF into one with diagonal covariance matrix. A further scaling by i in
each coordinate direction may be applied to transform it to an isotropic Gaussian distribution.
Equivalently stated, a change of coordinates may be applied to transform
Mahalanobis distance to ordinary Euclidean distance.
A2.2 2 distribution
The 2
n distribution is the distribution of the sum of squares of n independent Gaussian
random variables. As applied to a Gaussian random vector v with non-singular covariance
matrix , the value of (v âˆ’ Â¯v)Tâˆ’1(v âˆ’ Â¯v) satisfies a 2
n distribution, where n is
the dimension of v. If the covariance matrix  is singular, then we must replace âˆ’1
with the pseudo-inverse +. In this case
â€¢ If v is a Gaussian random vector with mean Â¯v and covariance matrix , then the
value of (v âˆ’ Â¯v)T+(v âˆ’ Â¯v) satisfies a 2
r distribution, where r = rank.
The cumulative chi-squared distribution is defined as Fn(k2) =
R k2
0 2
n()d. This
represents the probability that the value of the 2
n random variable is less than k2.
Graphs of the 2
n distribution and inverse cumulative 2
n distributions for n = 1, . . . , 4
are shown in figure A2.1 A program for computing the cumulative chi-squared distribution
Fn(k2) is given in [Press-88]. Since it is a monotonically increasing function,
one may compute the inverse function by any simple technique such as subdivision,
and values are tabulated in table A2.1 (compare with figure A2.1).
A2.2 2 distribution 567
2 4 6 8 10
0.1
0.2
0.3
0.4
0.5
0.2 0.4 0.6 0.8 1
2
4
6
8
10
12
14
Fig. A2.1. The 2
n distribution (left) and inverse cumulative 2
n distribution Fâˆ’1
n (right) for n =
1, . . . , 4. In both cases, graphs are for n = 1, . . . , 4 bottom to top (at middle point of horizontal axis).
n 
 = 0.95 
 = 0.99
1 3.84 6.63
2 5.99 9.21
3 7.81 11.34
4 9.49 13.28
Table A2.1. Values of k2 for which Fn(k2), the cumulative 2 distribution with n degrees of freedom,
equals 
, i.e. k2 = Fâˆ’1
n (
), where 
 is the probability. -->
</p><p>
</p><p>
    </body>
</html>