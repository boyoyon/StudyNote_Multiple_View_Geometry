<!doctype html>
<html lang="ja">
    <head>
        <script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
        <meta charset="utf-8" />
        <title>18章</title>
        <style type="text/css">
            p
            {
                padding-left: 2em;
            }
            .margin-large
            {
                margin-left: 30px;
            }
           .margin-abstract {
               margin-left: 60px; /* 左マージンを広くする */
               margin-right: 60px; /* 右マージンを広くする */
           }
        </style>
    <style>
        .two-columns {
            display: flex;
            flex-direction: row;
            gap: 20px; /* 列間のスペース */
        }
        .column {
            flex: 1; /* 各列が均等に幅を取る */
        }
    </style>
<style>
.three-columns {
  display: flex;
  gap: 10px; /* 列間の余白を設定 */
}
.column {
  flex: 1; /* 各列の幅を均等にする */
  padding: 10px; /* 内側の余白を設定 */
}
</style>
    <style>
        .styleRef { 
            text-indent: -40px; /* 最初の行の字下げを逆方向に */
            margin-left: 10px; /* 2行目以降の字下げを調整 */
            ul {
                  list-style-type: none; /* 箇条書き記号を非表示 */
                  padding-left: 40px; /* 全体の左余白 */
            }
            li {
            }
        }
    </style>
    <style>
        .styleBullet { 
            text-indent: -20px; /* 最初の行の字下げを逆方向に */
            margin-left: 30px; /* 2行目以降の字下げを調整 */
            ul {
                  list-style-type: none; /* 箇条書き記号を非表示 */
                  padding-left: 0px; /* 全体の左余白 */
            }
            li {
            }
        }
    </style>
    <style>
            ol
            {
                margin-left: 30px;
            }
            ul
            {
                margin-left: 30px;
            }
    </style>
    </head>
    <body>
        <h1><center>18章 N視点計算手法</center></h1>
<p>
この章では、特にビュー数が多い場合における、画像セットからの射影再構成またはアフィン再構成を推定するための計算方法について説明します。
<!-- This chapter describes computational methods for estimating a projective or affine reconstruction
from a set of images – in particular where the number of views is large.-->
</p><p>
まず、最も一般的なケースである射影再構成のためのバンドル調整から始めます。次に、これをアフィンカメラに特化し、重要な因数分解アルゴリズムを導入します。このアルゴリズムの非剛体シーンへの一般化を示します。次に、平面を含むシーンの場合のバンドル調整の2つ目の特化について説明します。最後に、画像シーケンス全体にわたる点の対応関係を取得し、それらの対応関係から射影再構成を行う方法について論じます。
<!-- We start with the most general case which is that of bundle adjustment for a projective
reconstruction. This is then specialized to affine cameras and the important factorization
algorithm introduced. A generalization of this algorithm to non-rigid scenes
is given. A second specialization of bundle adjustment is then described for the case
of scenes containing planes. Finally we discuss methods for obtaining point correspondences
throughout an image sequence and a projective reconstruction from these
correspondences.-->
</p>
<h2><center>18.1 射影再構成 - バンドル調整</center></h2>
<p>
3次元点集合 \(X_j\) が、行列 \(P^i\) を持つカメラ群から撮影される状況を考えてみましょう。\(i\)番目のカメラから見た \(j\)番目の点の座標を \(x_j^i\) とします。ここでは、次のような再構成問題を解きます。画像座標集合 \(x_j^i\) が与えられたとき、カメラ行列集合 \(P^i\) と、\(P^iX_j = x_j^i\) となる点 \(X_j\) を求めます。\(P^i\) または \(X_j\) にさらなる制約がない場合、このような再構成は射影再構成です。なぜなら、点 \(X_j\) は、真の再構成から任意の3次元射影変換によって異なる可能性があるからです。<br>
<!-- Consider a situation in which a set of 3D points \(X_j\) is viewed by a set of cameras with matrices \(P^i\). Denote by \(x_j^i\) the coordinates of the \(j\)-th point as seen by the \(i\)-th camera. We wish to solve the following reconstruction problem: given the set of image coordinates \(x_j^i\) find the set of camera matrices, \(P^i\), and the points \(X_j\) such that \(P^iX_j = x_j^i\). Without further restriction on the \(P^i\) or \(X_j\) , such a reconstruction is a projective reconstruction, because the points \(X_j\) may differ by an arbitrary 3D projective transformation from the true reconstruction.<br>
<br>
<strong>Bundle adjustment.</strong><br>
If the image measurements are noisy then the equations
xi
j = PiXj will not be satisfied exactly. In this case we seek the Maximum Likelihood
(ML) solution assuming that the measurement noise is Gaussian: we wish to
estimate projection matrices ˆP
i
and 3D points bX j which project exactly to image points
ˆxi
j as ˆxi
j = ˆP
ibX j , and also minimize the image distance between the reprojected point
and detected (measured) image points xi
j for every view in which the 3D point appears,
i.e.
min
ˆP
i
,bXj
X
ij
d(ˆP
ibX j , xi
j)2 (18.1)
where d(x, y) is the geometric image distance between the homogeneous points x and
y. This estimation involving minimizing the reprojection error is known as bundle
adjustment – it involves adjusting the bundle of rays between each camera centre and
434
18.1 Projective reconstruction – bundle adjustment 435
the set of 3D points (and equivalently between each 3D point and the set of camera
centres).
Bundle adjustment should generally be used as a final step of any reconstruction
algorithm. This method has the advantages of being tolerant of missing data while
providing a true ML estimate. At the same time it allows assignment of individual
covariances (or more general PDFs) to each measurement and may also be extended to
include estimates of priors and constraints on camera parameters or point positions. In
short, it would seem to be an ideal algorithm, except for the fact that: (i) it requires a
good initialization to be provided, and (ii) it can become an extremely large minimization
problem because of the number of parameters involved. We will discuss briefly
these two points.
Iterative minimization. Since each camera has 11 degrees of freedom and each 3-
space point 3 degrees of freedom, a reconstruction involving n points over m views
requires minimization over 3n+11m parameters. In fact, since entities are often overparametrized
(e.g. using 12 parameters for the homogeneous P matrix) this may be a
lower bound. If the Levenberg–Marquardt algorithm is used to minimize (18.1) then
matrices of dimension (3n + 11m) × (3n + 11m) must be factored (or sometimes inverted).
Asmand n increase this becomes extremely costly, and eventually impossible.
There are several solutions to this problem:
(i) Reduce n and/orm. Do not include all the views or all the points, and fill these
in later by resectioning or triangulation respectively; or, partition the data into
several sets, bundle adjust each set separately and then merge. Such strategies
are discussed further in section 18.6.
(ii) Interleave. Alternate minimizing reprojection error by varying the cameras
with minimizing reprojection error by varying the points. Since each point is
estimated independently given fixed cameras, and similarly each camera is estimated
independently from fixed points, the largest matrix that must be inverted
is the 11 × 11 matrix used to estimate one camera. Interleaving minimizes
the same cost function as bundle adjustment, so the same solution should be
obtained (provided there is a unique minimum), but it may take longer to converge.
Interleaving is compared with bundle adjustment in [Triggs-00a].
(iii) Sparse methods. These are described in appendix 6(p597).
Initial solution. Several methods for initialization are described in the following sections.
If the problem is restricted to affine cameras then factorization (section 18.2)
gives a closed form optimal solution provided points are imaged in every view. Even
with projective cameras there is an (iterative) factorization method (section 18.4) available
provided points are imaged in every view. If there is more information available
on the data, for example that it is partly coplanar, then again a closed form solution
is possible (section 18.5). Finally, hierarchical methods can be used as described in
section 18.6 for the case where points are not visible in every view.
436 18 N-View Computational Methods
18.2 Affine reconstruction – the factorization algorithm
In this section we describe reconstruction from a set of image point correspondences for
images acquired by affine cameras. As described in section 17.5.1 the reconstruction
in this case is affine.
The factorization algorithm of Tomasi and Kanade [Tomasi-92] to be presented below
and summarized in algorithm 18.1 has the following property:
• Under an assumption of isotropic mean-zero Gaussian noise independent and equal
for each measured point, factorization achieves a Maximum Likelihood affine reconstruction.
This fact was first pointed out by Reid and Murray [Reid-96]. However, the method
requires a measurement of each point in all views. This is a limitation in practice, since
matched points may be absent in some views.
An affine camera may be characterized by having its last row equal to (0, 0, 0, 1).
In this section, however, we will denote it somewhat differently, separating out the
translation and the pure linear transformation part of the camera map. Thus we write
 
x
y
!
= M


X
Y
Z


+ t
where M is a 2 × 3 matrix and t a 2-vector. From here on for ease of readability
x represents an inhomogeneous image point x = (x, y)T, and X an inhomogeneous
world point X = (X, Y, Z)T.
Our goal is to find a reconstruction to minimize geometric error in image coordinate
measurements. That is, we wish to estimate cameras {Mi, ti} and 3D points {Xj} such
that the distance between the estimated image points ˆxi
j = MiXj + ti and measured
image points xi
j is minimized
min
Mi,ti,Xj
X
ij


xi
j − ˆxi
j


2
= min
Mi,ti,Xj
X
ij


xi
j − (MiXj + ti)


2
. (18.2)
As is common in such minimization problems the translation vector ti can be eliminated
in advance by choosing the centroid of the points as the origin of the coordinate
system. This is a consequence of the geometric fact that an affine camera maps the centroid
of a set of 3D points to the centroid of their projections. Thus, if the coordinate
origin is chosen as the centroid of the 3D points and of each set of image points then it
follows that ti = 0. This step requires that the same n points be imaged in all views,
i.e. that there are no views in which the image coordinates of any point are unknown.
An analytical derivation of this result goes like this. The minimization with respect to
ti requires that
@
@ti
X
kj


xk
j − (MkXj + tk)


2
= 0
which after a brief calculation reduces to ti = hxii − MihXi, where the centroids are
18.2 Affine reconstruction – the factorization algorithm 437
Objective
Given n ≥ 4 image point correspondences over m views xi
j , j = 1, . . . , n; i = 1, . . . ,m,
determine affine camera matrices {Mi, ti} and 3D points {Xj} such that the reprojection error
X
ij


xi
j − (MiXj + ti)


2
is minimized over {Mi, ti,Xj}, with Mi a 2 × 3 matrix, Xj a 3-vector, and xi
j = (xi
j , yi
j)T and
ti are 2-vectors.
Algorithm
(i) Computation of translations. Each translation ti is computed as the centroid of points
in image i, namely
ti = hxii =
1
n
X
j
xi
j .
(ii) Centre the data. Centre the points in each image by expressing their coordinates with
respect to the centroid:
xi
j ← xi
j − hxii.
Henceforth work with these centred coordinates.
(iii) Construct the 2m × n measurement matrix W from the centred data, as defined
in (18.5), and compute its SVD W = UDVT.
(iv) Then the matrices Mi are obtained from the first three columns of U multiplied by the
singular values:


M1
M2
...
Mm


= [ 1u1 2u2 3u3 ] .
The vectors ti are as computed in step (i) and the 3D structure is read from the first three
columns of V
[ X1 X2 . . . Xn ] = [ v1 v2 v3 ]
T
.
Algorithm 18.1. The factorization algorithm to determine the MLE for an affine reconstruction from n
image correspondences over m views (under Gaussian image noise).
hxii = 1
n
P
j xi
j and hXi = 1
n
P
j
Xj . The origin of the 3D frame is arbitrary, so may be
chosen to coincide with the centroid hXi, in which case hXi = 0 and
ti = hxii. (18.3)
It follows that if we measure the image coordinates with respect to a coordinate
origin based at the centroid of the projected points, then ti = 0. Thus, we replace each
xi
j by xi
j − hxii. Henceforth we will assume that this has been done, and work with
the centred coordinates. With respect to these new coordinates ti = 0, and so (18.2)
reduces to
min
Mi,Xj
X
ij

 
x
i
j − ˆxi
j


2
= min
Mi,Xj
X
ij


xi
j − MiXj


2
. (18.4)
438 18 N-View Computational Methods
The minimization problem now has a very simple form when written as a matrix.
The measurement matrix W is the 2m × n matrix composed of the centred coordinates
of the measured image points
W =


x1
1 x1
2 . . . x1
n
x2
1 x2
2 . . . x2
n
...
...
. . .
...
xm
1 xm
2 . . . xm
n


. (18.5)
Since each xi
j = MiXj , the complete set of equations may be written as
W =


M1
M2
...
Mm


h
X1 X2 . . . Xn
i
.
In the presence of noise this equation will not be satisfied exactly, so instead we seek a
matrix ˆW as close as possible to W in Frobenius norm, such that ˆW may be decomposed
as
ˆW =


ˆx1
1 ˆx1
2 . . . ˆx1
n
ˆx2
1 ˆx2
2 . . . ˆx2
n
...
...
. . .
...
ˆxm
1 ˆxm
2 . . . ˆxm
n


=


M1
M2
...
Mm


h
X1 X2 . . . Xn
i
. (18.6)
In this case it may be verified that

 
W
−
ˆW


2
F
=
X
ij

Wij − ˆWij
2
=
X
ij


xi
j − ˆxi
j


2
=
X
ij


xi
j − MiXj


2
Comparing this with (18.4) we find that minimizing the required geometric error is
equivalent to finding such a ˆW as close as possible to W in Frobenius norm.
Note that a matrix ˆW satisfying (18.6) is the product of a 2m × 3 motion matrix ˆM,
and a 3 × n structure matrix ˆX; consequently ˆW = ˆMˆX has rank 3. In other words we
seek a rank 3 matrix which is closest to W in Frobenius norm. Such a matrix may
be determined by the SVD of W truncated to rank 3. In more detail, if the SVD of
W = UDVT then ˆW = U2m×3D3×3VT
3×n is the rank 3 matrix which is closest to W in the
Frobenius norm, where U2m×3 consists of the first 3 columns of U, VT
3×n consists of the
first 3 rows of VT, and D3×3 is the diagonal matrix containing the first 3 singular values,
D3×3 = diag(1, 2, 3).
Note that the choice of ˆM and ˆX is not unique. For example ˆM may be chosen as
ˆM = U2m×3D3×3 and ˆX = VT
3×n, or as ˆM = U2m×3, ˆX = D3×3VT
3×n since in either case
ˆW = ˆMˆX = U2m×3D3×3VT
3×n.
Affine ambiguity. In fact for any such choice there is an additional ambiguity since an
arbitrary 3×3 rank 3 matrix A may be inserted in the decomposition as ˆW = ˆMAA−1ˆX =
(ˆMA)(A−1ˆX). This means that the camera matrices Mi, which are obtained from ˆM, and
18.2 Affine reconstruction – the factorization algorithm 439
the 3D points Xj , which are obtained from ˆX, are determined up to multiplication by a
common matrix A. In other words the MLE reconstruction is affine.
This affine reconstruction may be upgraded to a metric reconstruction by supplying
metric information on the scene as described in section 10.4.2(p272), or by using autocalibration
methods as described in chapter 19, or a combination of the two. Note that
in the case of affine cameras only three internal parameters need be specified (compared
to five for projective cameras) and the auto-calibration task is correspondingly simpler.
18.2.1 Affine multiple view tensors
The factorization algorithm provides an optimal method for computing the affine multiview
tensors from image point correspondences. These tensors are the affine fundamental
matrix, affine trifocal tensor, and affine quadrifocal tensor. In each case the algorithm
determines the camera matrices up to an overall affine ambiguity. The tensors
may then be computed directly from the camera matrices (as for instance in chapter 17).
The affine ambiguity of 3-space is irrelevant when computing the tensors since they are
unaffected by affine transformations of 3-space. In fact it is not necessary to compute
the full SVD of W since only the U part of the decomposition is required. If the number
of points n is large compared with the number of views then very great savings can be
made in the computation of the SVD by not determining V (see table A4.1(p587)).
An alternative to using the SVD is to use the eigenvalue decomposition of WWT, since
WWT = (UDVT)(UDVT)T = UD2UT. In the case of three views (computation of the trifocal
tensor) the matrix WWT has dimension only 9 × 9. Thus this approach can mean significant
savings. However, it is numerically inferior, since forming WWT causes the condition
number of the matrix to be squared (see the discussion of SVD in [Golub-89]).
Since we need just the three largest eigenvectors, that may not be such a problem in
this case. However, the savings of this approach will not be so great given an implementation
of the SVD that avoids computing V.
The factorization method may be used to compute any of the multiple-view affine
tensors. For the affine fundamental matrix, however, algorithm 14.1(p351) described
in chapter 14 is more direct. The results of both the methods are identical.
18.2.2 Triangulation and reprojection using subspaces
The factorization algorithm also provides an optimal method for computing the images
of new points or of points not observed in all views. Again the affine ambiguity of
3-space is irrelevant.
A column of W is the set of all corresponding image points for the point Xj and is
referred to as a point’s trajectory. The rank 3 decomposition (18.6) of ˆW as ˆW = ˆMˆX
shows that all trajectories lie in a 3 dimensional subspace. In particular the trajectory
(i.e. all image projections) of a new point X may be obtained as ˆMX. This is simply a
linear weighting of the three columns of ˆM.
Suppose we have observed a new point X in some (not all) views, and wish to predict
its projection in the other views. This is carried out in two steps: first triangulation
to find the pre-image X, and then reprojection as ˆMX to generate its image in
440 18 N-View Computational Methods
all views. Note that the projected points will not coincide exactly with the measured
(noisy) points. In the triangulation step we wish to find the point X that minimizes reprojection
error, and this corresponds to finding the point in the linear subspace spanned
by the columns of ˆM closest to the trajectory. This closest point is found by projecting
the trajectory onto the subspace (in a similar manner to algorithm 4.7(p130)).
In more detail suppose we have computed a set of affine cameras {Mi, ti} then the
triangulation problem may be solved linearly for any number of views. The image
points xi = MiX + ti give a pair of linear equations MiX = xi − ti in the entries of
X. Given sufficiently many such equations (arising from known values of xi) one can
find the linear least-squares solution for X, using algorithm A5.1(p589), the pseudoinverse
(see result A5.1(p590)) or algorithm A5.3(p591). Note that if the data xi is
centred using the same transformation applied in step (ii) of algorithm 18.1, then the
translation vectors ti in the affine triangulation method may be taken to be zero.
In practice triangulation and reprojection provides a method of ‘filling in’ points that
are missed during tracking or multiple view matching.
18.2.3 Affine Reconstruction by Alternation
Suppose a set of image coordinates xi
j are given as in algorithm 18.1, and we wish
to perform affine reconstruction. We have seen that affine triangulation may be carried
out linearly. Thus, if the affine camera matrices represented by {Mi, ti} are known, then
the optimal point positions Xj may be computed by a linear least-squares method such
as the normal equations method of algorithm A5.3(p591).
Conversely, if the points Xj are known, then the equations xi
j = MiXj + ti are linear
in Mi and ti. So it is once again possible simply to solve for {Mi, ti} by linear leastsquares.
This suggests a method of affine reconstruction in which linear least-squares methods
are used to solve alternately for the points Xj and the cameras {Mi, ti}. This method
of alternation is not to be recommended as a general method for reconstruction, or for
solving optimization problems in general. In the case of affine reconstruction, however,
it can be proven to converge rapidly to the optimal solution, starting from a random
starting point. This method of affine reconstruction has the advantage of working with
missing data, or with covariance-weighted data which algorithm 18.1 will not, though
in the missing data or covariance-weighted case, global optimal convergence is not
guaranteed in all cases.
18.3 Non-rigid factorization
Throughout the book it has been assumed that we are viewing a rigid scene and that
only the relative motion between the camera and scene is to be modelled. In this section
we relax this assumption and consider the problem of recovering a reconstruction for
a deforming object. It will be shown that if the deformation is modelled as a linear
combination over basis shapes then the reconstruction and the basis shapes may be
recovered with a simple modification of the factorization algorithm of section 18.2.
An example where this type of situation arises is in a sequence of images of a per18.3
Non-rigid factorization 441
Fig. 18.1. Shape basis. A face template is represented by N equally spaced 2D points (here N = 140).
The central face of the seven is the mean shape and the faces to the left or right are generated by adding
or subtracting, respectively, the basis shape that accounts for the maximum variation in the training set.
In this case the basis spans expressions from surprised to disgruntled. Facial expressions are learnt by
tracking the face of an actor with the template whilst he changes expression but does not vary his head
pose. Each frame of the training sequence generates a set of N 2D points, and the coordinates of these
are rewritten as a 2N-vector. A 2N × f matrix is then composed from these vectors, where f is the
number of training frames, and the basis shapes are computed from the singular vectors of this matrix.
Figure courtesy of Richard Bowden.
son’s head which moves and also changes expression. The motion of the head may
be modelled as a rigid rotation, and the change of expression relative to the fixed head
may be modelled as a linear combination over basis sets. For example the mouth outline
may be represented by a set of points.
Suppose the set of n scene points Xj may be represented as a linear combination of
l basis shapes Bk so that at a particular time i:
h
Xi
1
Xi
2 . . . Xi
n
i
=
Xl
k=1

i
k
h
B1k B2k . . . Bnk
i
=
X
k

i
k
Bk
where here both the scene points Xi
j and the basis points Bjk are inhomogeneous points
represented by 3-vectors, and Bk is a 3×n matrix. Typically the number of basis shapes,
l, is much smaller than the number of points, n. The coefficients 
i
k may be different
at each time i, and the resulting differing combination of basis shapes generates the
deformation. An example is shown in figure 18.1.
In the forward model of image generation each view i is acquired by an affine camera
and gives the image points
xi
j = Mi
X
k

i
k
Bjk + ti.
It will again be assumed that image point matches are available for all views. Our
goal is to estimate cameras {Mi, ti} and 3D structure {
i
k,Bjk} from the measured
image points {xi
j}, such that the distance between the estimated image points ˆxi
j =
MiP
k 
i
k
Bjk + ti and measured image points is minimized
min
Mi,ti,
i
k,Bjk
X
ij


xi
j − ˆxi
j


2
= min
Mi,ti,
i
k,Bjk
X
ij


xi
j − (Mi
X
k

i
k
Bjk + ti)


2
.
As in affine factorization the translation may be eliminated by centring the measured
image points, and it will be assumed from here on that this has been done. Then the
442 18 N-View Computational Methods
problem reduces to
min
Mi,
i
k,Bjk
X
ij

 
x
i
j − ˆxi
j


2
= min
Mi,
i
k,Bjk
X
ij


xi
j − Mi
X
k

i
k
Bjk


2
. (18.7)
The complete set of equations ˆxi
j = MiP
k 
i
k
Bjk may be written
ˆW =


M1 (
1
1
B1 + 
1
2
B2 + . . . 
1
l
Bl)
M2 (
2
1
B1 + 
2
2
B2 + . . . 
2
l
Bl)
...
Mm (
m
1
B1 + 
m
2
B2 + . . . 
m
l
Bl)


=



1
1
M1 
1
2
M1 . . . 
1
l
M1

2
1
M2 
2
2
M2 . . . 
2
l
M2
...
...
. . .
...

m
1
Mm 
m
2
Mm . . . 
m
l
Mm




B1
B2
...
Bl


(18.8)
This rearrangement shows that the 2m × n matrix ˆW may be decomposed as a product
of a 2m × 3l motion matrix ˆM and 3l × n structure matrix ˆB, and consequently ˆW has
maximum rank 3l.
As in rigid factorization a rank 3l decomposition may be obtained from the measurement
matrix W by truncating the SVD of W to rank 3l. Also, as in rigid factorization, the
decomposition ˆW = ˆMˆB is not uniquely defined since an arbitrary 3l × 3l rank 3l matrix
A may be inserted in the decomposition as ˆW = ˆMAA−1ˆB = (ˆMA)(A−1ˆB). In the rigid
case this resulted in a straightforward affine ambiguity in the reconstruction. However,
in the non-rigid case there is the additional requirement that the motion matrix has the
replicated block structure of (18.8), and we return to this below. This block structure is
not required for determining a point’s image motion, as will now be discussed.
18.3.1 Subspaces and tensors
In the case of rigid factorization (18.6), as discussed in section 18.2.2, the trajectories
lie in a 3 dimensional subspace, and any trajectory may be generated as a linear
combination of the columns of ˆM (the 2m × 3 motion matrix). Similarly in the case of
non-rigid factorization, (18.8), the trajectories lie in a 3l dimensional subspace, and any
trajectory may be generated as a linear combination of the columns of ˆM (the 2m × 3l
motion matrix).
Suppose we observe a new point in a subset of the views, how many images are
required before its position can be predicted in all the other views? This is simply a
question of triangulation: in rigid factorization a 3-space point has 3 degrees of freedom,
and must be observed in two views to obtain the necessary 3 measurements. In
non-rigid factorization we need to specify 3l degrees of freedom (the number of rows
in the ˆB matrix), and this requires 3l/2 images. For example, if l = 2 the subspace is six
dimensional (the columns of the ˆB matrix are 6-vectors), and given the image position
in three views, the image position in all views is then determined by an analogue of
affine triangulation (section 18.2.2) even though the object is deforming.
Independently moving objects. Low-rank factorization methods also arise when
there are independently moving objects in the scene. For instance suppose the scene
is divided into two objects, each moving independently of the other, and viewed by
18.3 Non-rigid factorization 443
a b
Fig. 18.2. Non-rigid motion sequence. Top row: alternate frames from a sequence in which a giraffe
gracefully walks and flexes its neck, whilst the camera pans to match its speed. Bottom row: point
tracks showing the motion over (a) the 10 previous, and (b) the 10 forthcoming frames. These tracks are
computed using non-rigid factorization and lie in a six dimensional subspace. Note the very different
trajectories of the (rigid) background from the (deforming) foreground. Yet these are all spanned by the
six basis vectors of the motion matrix. The rank can be accounted for as follows: the sequence motion
is effectively that of two planes of points moving independently relative to the camera. The background
is a rigid object represented by a plane and contributes 2 to the rank. The giraffe in the foreground is
represented as a non-rigid object by a set of l = 2 planar basis shapes and contributes 4 to the rank.
Figures courtesy of Andrew Fitzgibbon and Aeron Morgan.
a moving affine camera. In this case, the columns of the measurement matrix corresponding
to points on one object will have rank 3, and those corresponding to the other
object will also have rank 3. The total rank of the measurement matrix will be 6. In
degenerate configurations in which one object’s points all lie in a plane, its contribution
to the rank will be only 2. This multibody factorization problem has been studied in
some depth in [Costeira-98].
An example of point tracks residing in a low dimensional subspace is shown in
figure 18.2.
The existence of the analogue of the fundamental matrix and trifocal tensor depends
on the dimension of the subspace. For example suppose the subspace has odd dimension
(e.g. l = 3 so it is 9 dimensional) then given point measurements in ⌊3l/2⌋
views (e.g. 4 views) the corresponding point in any other view is constrained to a line,
the analogue of an epipolar line, since there is one fewer measurement than degrees of
freedom of the subspace. However, if the dimension is even (e.g. l = 2 so it is 6 dimensional)
then given point measurements in l/2 views (e.g. 3 views) the corresponding
point in any other view is completely determined. Multi-view tensors can be built for
444 18 N-View Computational Methods
the non-rigid l > 1 subspaces using methods similar to those developed in chapter 17
for l = 1 3-space points.
18.3.2 Recovering the camera motion
In rigid-factorization the camera matrices are obtained relatively easily from the motion
matrix ˆM – all that is required is to remove a global affine ambiguity specified by a 3×3
matrix A as described on page 438.
In the non-rigid case, the analogous problem is not so straightforward. It is a simple
matter to obtain the motion matrix:
(i) Construct the 2m × n measurement matrix W from the centred data, as defined
in (18.5), and compute its SVD W = UDVT.
(ii) Then the motion matrix ˆM is obtained from the first 3l columns of U multiplied
by the singular values as ˆM =
h
1u1 2u2 . . . 3lu3l
i
,
but the matrix obtained by this route will not in general have the block structure
of (18.8). As in the case of rigid-factorization the motion matix is determined up to
post-multiplication by a matrix A, which here is 3l × 3l. The task is then to determine
A such that ˆMA has the required block structure of (18.8) and also to remove the
usual affine ambiguity such that each block conforms to any available constraints on
the camera calibration (for example identical internal parameters over all views).
Various methods for determining A have been investigated (see [Brand-01,
Torresani-01]), but these do not impose the full block structure, and currently there
is not a satisfactory solution to this problem. Once an initial solution has been obtained
by some means, then the correct form can be imposed by bundle adjustment of (18.7).
18.4 Projective factorization
The affine factorization method does not apply directly to projective reconstruction. It
was observed in [Sturm-96], however, that if one knows the “projective depth” of each
of the points then the structure and camera parameters may be estimated by a simple
factorization algorithm similar in style to the affine factorization algorithm.
Consider a set of image points xi
j = PiXj . This equation representing the projective
mapping is to be interpreted as true only up to a constant factor. Writing these constant
factors explicitly, we have i
jxi
j = PiXj . In this equation, and henceforth in the description
of the projective factorization algorithm, the notation xi
j means the 3-vector
(xi
j , yi
j , 1)T representing an image point. Thus the third coordinate is equal to unity,
and xi
j and yi
j are the actual measured image coordinates. Provided that each point is
visible in every view, so that xi
j is known for all i, j, the complete set of equations may
be written as a single matrix equation as follows:


1
1x1
1 1
2x1
2 . . . 1
nx1
n
2
1x2
1 2
2x2
2 . . . m
n x2
n
...
...
. . .
...
m
1 xm
1 m
2 xm
2 . . . m
n xm
n


=


P1
P2
...
Pm


[X1,X2, . . . ,Xn] . (18.9)
18.4 Projective factorization 445
Objective
Given a set of n image points seen in m views:
xi
j ; i = 1, . . . ,m, j = 1, . . . , n
compute a projective reconstruction.
Algorithm
(i) Normalize the image data using isotropic scaling as in section 4.4.4(p107).
(ii) Start with an initial estimate of the projective depths i
j . This may be obtained by
techniques such as an initial projective reconstruction, or else by setting all i
j = 1.
(iii) Normalize the depths i
j by multiplying rows and columns by constant factors. One
method is to do a pass setting the norms of all rows to 1, then a similar pass on columns.
(iv) Form the 3m × n measurement matrix on the left of (18.9), find its nearest rank-4 approximation
using the SVD and decompose to find the camera matrices and 3D points.
(v) Optional iteration. Reproject the points into each image to obtain new estimates of
the depths and repeat from step (ii).
Algorithm 18.2. Projective reconstruction through factorization.
This equation is true only if the correct weighting factors i
j are applied to each of
the measured points xi
j . For the present, let us assume that these depths are known. As
with the affine factorization algorithm, we would like the matrix on the left – denote it
by W – to have rank 4, since it is the product of two matrices with 4 columns and rows
respectively. The actual measurement matrix can be corrected to have rank 4 by using
the SVD. Thus, if W = UDVT, all but the first four diagonal entries of D are set to zero
resulting in ˆD. The corrected measurement matrix is ˆW = UˆDVT. The camera matrices
are retrieved from [PT
1 , PT
2 , . . . , PT
m]T = UˆD and the points from [X1,X2, . . . ,Xn] = VT.
Note that this factorization is not unique, and in fact we may interpose an arbitrary
4 × 4 projective transformation H and its inverse between the two matrices on the right
of (18.9), reflecting the fact that reconstruction has a projective ambiguity.
The steps of the projective factorization method are summarized in algorithm 18.2.
18.4.1 Choosing the depths
The weighting factors i
j are called the projective depths of the points. The justification
of this terminology is the relation of these i
j to the actual depths if camera matrices are
known in a Euclidean frame. Refer to section 6.2.3(p162) and in particular figure 6.6-
(p162). The main difficulty with this projective factorization algorithm is that we need
to know these projective depths up front, but we do not have this knowledge. There are
various techniques to estimate the depths.
(i) Start with an initial projective reconstruction obtained by other means, such as
those discussed in section 18.6 below. Then compute i
j by reprojecting the 3D
points.
(ii) Start with initial depths all equal to 1, compute the reconstruction and reproject
to obtain a new estimate of the depths. This step may be repeated to obtain
446 18 N-View Computational Methods
improved estimates. However, there is no guarantee that the procedure will
converge to a global minimum.
The original paper [Sturm-96] gives a method of computing the depths by stringing
together pairwise estimates of the depth obtained from the fundamental matrix, or the
trifocal tensor. This method is quite similar to obtaining an initial projective reconstruction
by stringing together triples of images (see section 18.6), whilst ensuring that
the scale factors are consistent for a common projective reconstruction.
18.4.2 What is being minimized?
In the case of noise, or incorrect values for i
j , the equations (18.9) are not satisfied exactly.
We determine a corrected measurement matrix ˆW that is closest to W in Frobenius
norm, subject to having rank 4. Denoting the entries of this matrix as ˆi
j ˆxi
j , then the
computed solution minimizes the expression
kW − ˆWk2 =
X
ij
ki
jxi
j − ˆi
j ˆxi
jk2 =
X
ij
(i
jxi
j − ˆi
j ˆxi
j)2 + (i
jyi
j − ˆi
j ˆyi
j)2 + (i
j − ˆi
j)2
(18.10)
Because of the last term, at a minimum ˆi
j must be close to i
j . Assuming they are
equal, (18.10) reduces to
P
ij(i
j)2kxi
j − ˆxi
jk2. Noting that kxi
j − ˆxi
jk is the geometric
distance between the measured and estimated points, what is being minimized is a
weighted sum-of-squares geometric distance, where each point is being weighted by
i
j . If all the geometric depths i
j are close to equal, then the factorization method
minimizes an approximation to geometric distance scaled by the common value of i
j .
18.4.3 Normalizing the depths
Projective depths as defined here are not unique. Indeed suppose that i
jxi
j = PiXj . If
we replace Pi by 
iPi and Xj by jXj , then we find that
(
iji
j)xi
j = (
iPi)(jXj) .
In other words, the projective depths i
j may be replaced by multiplying the i-th row
of (18.9) by a factor 
i and the j-th column by a factor j . In the light of the previous
paragraph, the closer all i
j are to unity, the more exactly the error expression represents
geometric distance. Therefore, it is advantageous to renormalize the values of the i
j
so that they are as close to unity as possible, by multiplying rows and columns of the
measurement matrix by constant values 
i and j . A simple heuristic manner of doing
this is to multiply each row by a factor 
i so that it has unit norm, followed by a similar
pass normalizing the columns. The row and column passes may be iterated.
18.4.4 Normalizing the image coordinates
As with most numerical algorithms involving homogeneous representations of image
coordinates described in this book, it is important to normalize the image coordinates.
A reasonable scheme is the isotropic normalization method described in section 4.4.4-
(p107). One can see the necessity of normalization quite clearly in this case. Consider
two image points x = (200, 300, 1)T and ˆx = (250, 375, 1)T. Obviously these points
18.5 Projective reconstruction using planes 447
are very far apart in a geometric sense. However, the error expression (18.10) measures
not geometric error, but the distance between homogeneous vectors kx − ˆ ˆxk.
Choosing  = 1.25 and ˆ = 1.0, the error is k(250, 375, 1.25)T − (250, 375, 1)Tk
which is proportionally quite small. On the other hand, the distance between points
x = (200, 300, 1)T and ˆx = (199, 301, 1)T, which are much closer geometrically can
not be made so small by choice of  and ˆ (except for small values). The reader may
observe that if the points are scaled down by a factor of 200, then this anomalous situation
no longer occurs. In short, with normalized coordinates, the error is a closer
approximation to geometric error.
18.4.5 When is the assumption i
j = 1 reasonable?
According to result 6.1(p162), if camera matrices are normalized such that
p2
31 + p2
32 + p2
33 = 1, and 3D points are normalized to have last coordinate T = 1, then
ij defined by i
j(xi
j , yi
j , 1) = PiXj are the true depths of the points from the camera
in a Euclidean frame. If all points are equidistant from the cameras throughout a sequence
then we may reasonably assume that each i
j = 1, for (18.9) will have at least
the solution where Pi and Xj are the true cameras and points, normalized in the manner
just stated. More generally, suppose that points are located at different depths, but each
point Xj remains at approximately the same depth dj from the cameras through the
whole sequence. In this case a solution will exist with all i
j = 1 in which the computed
Xj = d−1
j (Xj , Yj , Zj , 1)T. Similarly, by allowing multiplication of the camera
matrices by a factor, we find
• If the ratios of true depths of the different 3D points Xj remain approximately constant
during a sequence, then the assumption i
j = 1 is a good first approximation
to projective depth.
This is for instance the case of an aerial image camera pointing straight down from
constant altitude.
18.5 Projective reconstruction using planes
It was seen in section 17.5.2 that if four points visible in each view are known to
be coplanar then the computation of the multifocal tensors relating the image points
becomes significantly more simple. A major advantage is that a tensor satisfying all
its constraints may be computed using a linear algorithm. We now continue with that
particular line of investigation, and show that the use of linear techniques extends to
estimation of motion and structure for any number of views.
The condition that four of the image correspondences are derived from coplanar
points is equivalent to knowing the homographies between the images induced by a
plane in space, since a homography may be computed from the four points. It is only
the homographies that are important in the following approach. These homographies
may be computed from four or more point correspondences, or line correspondences,
or estimated directly from the images by direct correlation methods.
448 18 N-View Computational Methods
What do the plane-plane homographies tell us? The key to projective reconstruction
using planes is the observation that knowledge of homographies between the images
means we know the first 3 × 3 part of the camera matrices:
P =


M

t1
t2
t3


Hence, it remains only to compute their last columns, namely the vectors t.
Since we are interested at this point only in obtaining a projective reconstruction
of the scene, we may suppose that the plane inducing the homographies is the plane
at infinity, with points Xj = (Xj , Yj , Zj , 0)T. Camera matrices may be written in the
form Pi = [Mi|ti], where M is a 3 × 3 matrix and ti is a column vector. A reasonable
assumption is that the camera centres do not lie on the plane inducing the homographies
(for otherwise the homographies will be degenerate). This means that the matrix M
is non-singular. For simplicity, the first camera may be assumed to have the form
P1 = [I | 0], where I is the identity matrix.
Now, if xi
j is the point in image i corresponding to the 3D point Xj = (Xj , Yj , Zj , 0)T
lying on the homography-inducing plane, then
x1
j = P
1(Xj , Yj , Zj , 0)T = (Xj , Yj , Zj)T
whereas
xi
j = Mi(Xj , Yj , Zj)T = Mix1
j .
Thus Mi represents the homography from the first image to the i-th image induced by
the plane. Conversely, if Mi is the known plane-induced homography that maps a point
in the first image to its matching point in the i-th image, then the set of camera matrices
can be assumed to have the form Pi = [Mi|ti], where the Mi are known and their scale is
fixed, but the final columns ti are not.
Known camera orientation. We have just shown that knowledge of homographies
implies the knowledge of the left-hand 3 × 3 submatrix of each camera matrix. The
same will hold if we know the orientation (and calibration) of all the cameras. For instance,
a reasonable approach to reconstruction, knowing the calibration of each camera,
is to estimate the orientation of each camera separately from the translation (for
example from two or more scene vanishing points). Once the orientation (Ri) and calibration
(Ki) of each camera is known, the left-hand block of each camera matrix is
KiRi.
18.5.1 Direct solution for structure and translation
We describe two separate methods for computation of the projective structure given
plane-induced homographies between images. The first method solves for the 3D
points and the camera motion simultaneously by solving a single linear system. Suppose
point X = (X, Y, Z, 1)T is not on the plane at infinity, that is, the plane inducing
the homographies.
18.5 Projective reconstruction using planes 449
The equation for point projection is
x = PX = [M|t]X = [M|t]
 
eX
1
!
where the (unknown) scale factor  has been explicitly written. More precisely, we
may write



x
y
1


=


mT
1 t1
mT
2 t2
mT
3 t3


 
eX
1
!
=


mT
1
eX
+ t1
mT
2
eX
+ t2
mT
3
eX
+ t3


where mT
i is the i-th row of the matrix M.
The unknown scale factor  may be eliminated by taking the vector product of the
two sides of this equation, resulting in


x
y
1


×


mT
1
eX
+ t1
mT
2
eX
+ t2
mT
3
eX
+ t3


= 0.
This provides two independent equations
x(mT
3
eX
+ t3) − (mT
1
eX
+ t1) = 0
y(mT
3
eX
+ t3) − (mT
2
eX
+ t2) = 0
which are linear in the unknowns eX = (X, Y, Z)T and t = (t1, t2, t3)T. The equations
may be written as
"
xmT
3 −mT
1 −1 0 x
ymT
3 −mT
2 0 −1 y
#


eX
t1
t2
t3


= 0.
Thus, each measured point xi
j = PiXj generates a pair of equations, and with m
views involving n points a 2nm set of equations in 3n + 3m unknowns is generated in
this way. These equations may be solved by linear or linear least-squares techniques to
obtain the structure and motion.
A few remarks about this method are offered.
(i) In contrast to factorization methods (section 18.2) we do not need all points to
be visible in all views. Only equations corresponding to the measured points
are used.
(ii) Since it is assumed that points have final coordinate equal to one, it is necessary
to exclude points that lie on the plane at infinity (the plane inducing the
homography) which have final coordinate equal to zero. A test to detect points
lying on or close to the plane is necessary.
(iii) Both points and cameras are computed at once. For a large number of points
and cameras this may be a very large estimation problem. However, if the point
tracks have a banded form, then sparse solution techniques may be used to solve
the equation set efficiently, as in section A6.7(p613).
450 18 N-View Computational Methods
This method and its implementation are discussed in depth in [Rother-01,
Rother-03]. The details given here are different from those given in [Rother-01], where
the structure and motion computation is carried out in a specific projective frame related
to the matched points on the plane, involving a coordinate change in the images.
18.5.2 Direct motion estimation
The second method for planar reconstruction knowing homographies solves for the
camera matrices first and subsequently computes the point positions.
We start from the set of camera matrices which again can be assumed to have the
form Pi = [Hi|ti], where the Hi are known and their scale is fixed, but the final columns
ti are not. We may assume that P1 = [I | 0], so that t1 = 0. The set of all remaining ti
have 3m − 4 degrees of freedom, since the ti are defined only up to a common scale.
Now assume that several point or line correspondences across two or more views are
known (three views are required for lines). These correspondences must derive from
3D points or lines that do not lie in the reference plane (used to compute the Hi). Each
point correspondence across two views leads to a linear equation in the entries of the
fundamental matrix. Similarly, correspondences of points or lines across three or four
views lead to linear equations in the entries of the trifocal or quadrifocal tensor.
The key point (as explained in section 17.5.2) is that we may express the entries of
the fundamental matrix (or trifocal or quadrifocal tensor) linearly in the entries of the
vectors ti. Therefore each linear relation induced by a point or line correspondence
may be related back to a linear relationship in terms of the entries of the ti. Thus,
for example, a correspondence across views i, j and k gives rise to a set of linear
equations in the entries of the three vectors ti, tj and tk. A set of correspondences
across many views can be broken down into correspondences across sets of consecutive
views. Thus, for example, a single point correspondence across m > 4 views will give
a set of equations of the form
where each row represents a set of equations derived from a quadrifocal tensor relationship.
Each black square represents a block with 3 columns corresponding to one
of the vectors ti. In the diagram above, we choose to wrap the equations around from
the last to the first view to add greater rigidity. Otherwise, the values of the ti can drift
from the first to the last view. Other schemes for selecting groups of views are possible,
and it is not necessary to restrict to consecutive views.
Linear relations may be generated between any subset of sufficiently many images
(2, 3 or 4 depending on which tensor is used to generate the equations). One must trade
off the added stability of the solution against the added computational cost of adding
more equations. A mixture of bifocal, trifocal and quadrifocal constraints may be used
18.5 Projective reconstruction using planes 451
in generating the set of all equations, and it is not necessary that all points be visible in
all views.
Numbers of equations generated. Let the total number of views be m. Consider
a subset of s views (s = 2, 3 or 4) and let n point correspondences be given between
these views. We briefly consider the problem of reconstruction from this subset of s
views in isolation. From these point correspondences we can generate a set of equations
At′ between the entries t′ of the s views, and thence estimate the values of the 3s entries
of t′. In doing this, we can assume that the first view has t = 0, and the vectors t
from the remaining s − 1 views are only determined up to a common scale. Thus,
the A occurring in the equation set At′ has a right null-space of dimension at least 4,
corresponding to the 4 degrees of freedom of the solution. In general, then:
Result 18.1. Ignoring the effects of noisy data, the total rank of the set of equations
generated from n ≥ 2 point correspondences in s views is 3s − 4. This is independent
of the number of point (or line) correspondences used to generate them.
To be exact, the argument above showed that the rank was at most 3s−4. For 2-view,
3-view and 4-view correspondences this is equal to 2, 5 or 8 respectively. However, as
long as there are two correspondences the maximum rank is achieved. This is because
two points are sufficient for reconstruction from s = 2, 3 or 4 views as shown by the
counting arguments of section 17.5.2.
Now consider the total set of m views. The total number of retrievable parameters of
all the ti is 3m − 4. Therefore, for a solution to be possible, the number of equations
must exceed 3m − 4, which gives the following result.
Result 18.2. If S subsets of sk views are chosen from among m views, then in order
to solve for all the vectors ti representing final columns of the camera matrices, it is
necessary that
XS
k=1
(3sk − 4) ≥ 3m − 4 .
One can verify that if 2-view correspondences are to be used, involving equations
derived from the fundamental matrix constraints, then it is not sufficient
to use just pairs of consecutive views in a configuration such as
for in this case, the total number of equations generated is m(3s−4) = m(3 · 2−4) =
2m, whereas the total number of equations required is 3m − 4. Thus for m > 4 there
are not enough equations. This is related to the fact that the fundamental matrices
between consecutive views are not sufficient to define the structure of the sequence of
views. It is necessary to add additional constraints from non-consecutive views, such as
452 18 N-View Computational Methods
.
Note though that the discussion of section 15.4(p383) suggests that it is preferable to
use trifocal or quadrifocal constraints over triplets or quadruplets of views. Implementation
details for this method are given in [Kaucic-01].
18.6 Reconstruction from sequences
In this final section we bring together several ideas from earlier in the book. The
objective here is to compute a reconstruction from a sequence of frames provided by
a video. There are three stages to this problem: (i) compute corresponding features
throughout the sequence; (ii) compute an initial reconstruction which may be used as a
starting point for (iii) bundle adjustment (as described in section 18.1).
Here the features we will consider are interest points, though others such as lines
could equally well be used. The correspondence problem is exacerbated because an
interest point feature will generally not appear in all of the images, and often will be
missing from consecutive images. Bundle adjustment, however, is not hindered by
missing correspondences.
There are several advantages of a video sequence over an arbitrary set of images: (i)
there is an ordering on the images; (ii) the distance between camera centres (the baseline)
for successive frames is small. The small baseline is important because it enables
possible feature matches between successive images to be obtained and assessed more
easily. Matches are more easily obtained because the image points do not move “far”
between views so a proximity search region can be employed; matches are more easily
assessed (as to whether they arise from the same point in 3-space) because nearby
images are similar in appearance. The disadvantage of a small baseline is that the 3D
structure is estimated poorly. However, this disadvantage is mitigated by tracking over
many views in the sequence so that the effective baseline is large.
An overview of the method is given in algorithm 18.3. There are several strategies
that may be used to obtain the initial reconstruction, though this area is still to some
extent a black art. Three possibilities are:
1. Extending the baseline. Suppose a reasonable number of scene points are visible
throughout the sequence. Correspondences may be carried through from the first to
the last frame using the pairwise matches (from F), or the triplet matched points (from
T ). Indeed if the baseline between consecutive frames is small (compared to the structure
depth), then pairwise matches may be obtained using homography computation
(algorithm 4.6(p123)) – this provides a stronger matching constraint (point to point)
than F (point to line).
A trifocal tensor can then be estimated from corresponding points in the first, middle
(say), and end frames of the sequence. This tensor determines a projective reconstruc18.6
Reconstruction from sequences 453
Objective
Given a sequence of frames in a video, compute correspondences and a reconstruction of the
scene structure and the camera for each frame.
Algorithm
(i) Interest points: Compute interest points in each image.
(ii) 2 view correspondences: Compute interest point correspondences and F between consecutive
frames using algorithm 11.4(p291) (frames may be omitted if the baseline
motion is too small).
(iii) 3 view correspondences: Compute interest point correspondences and T between all
consecutive image triplets using algorithm 16.4(p401).
(iv) Initial reconstruction: See text.
(v) Bundle adjust the cameras and 3D structure for the complete sequence.
(vi) Auto-calibration: see chapter 19 (optional).
Algorithm 18.3. Overview of reconstruction from a sequence of images.
tion for those points and frames. The cameras for the intermediate frames may then
be estimated by resectioning, and the scene points not visible throughout the sequence
estimated by triangulation.
2. Hierarchical merging of sub-sequences. The idea here is to partition the sequence
into manageable sub-sequences (there can be several hierarchical layers of partitioning).
A projective reconstruction is then computed for each sub-sequence and these
reconstructions are “zipped” (merged) together.
Consider the problem of merging two triplets which overlap by two views. It is a
simple matter to extend the correspondences over the views: a correspondence which
exists across the triplet 1-2-3 and also across the triplet 2-3-4 may be extended to the
frames 1-2-3-4, since the pair 2-3 overlaps for the triplets. The camera matrices and 3D
structure are then computed for the frames 1-2-3-4, for example by first resectioning
and then bundle adjustment. This process is extended by merging neighbouring groups
of frames until camera matrices and correspondences are established throughout the
sequence. In this manner error can be distributed evenly over the sequence.
3. Incremental bundle adjustment. A fresh bundle adjustment is carried out as the
correspondences from each new frame are added. The disadvantage of this method is
the computational expense and also the possibility that error systematically accumulates.
Of course these three methods may be combined together. For example, the sequence
can be partitioned into a sub-sequence where common points are visible, and
a reconstruction built for the sub-sequence using the extended baseline method. These
sub-sequences may then be combined hierarchically.
In this manner structure and cameras may be computed automatically for sequences
consisting of hundreds of frames. These reconstructions may form the basis for such
tasks as navigation (determining the camera/ego-position) and virtual model genera454
18 N-View Computational Methods
a b
c d
Fig. 18.3. Corridor sequence. (a) A three dimensional reconstruction of points and lines in the scene,
and (b) cameras (represented by their image planes) computed automatically from the images. A texture
mapped triangulated graphical model is then automatically constructed as described in [Baillard-99].
(c) A rendering of the scene from a novel viewpoint, different from any in the sequence. (d) VRML model
of the scene with the cameras represented by their image planes (texture mapped with the original images
from the sequence).
tion. Often it is necessary first to compute a metric reconstruction from the projective
one, using the methods described in chapter 10 and chapter 19. Metric reconstruction
and virtual model generation is illustrated in the following examples.
Example 18.3. Corridor sequence
A camera is mounted on a mobile vehicle for this sequence. The vehicle moves along
the floor turning to the left. The forward translation in this sequence makes structure
recovery difficult, due to the small baseline for triangulation. In this situation, the benefit
of using all frames in the sequence is significant. Figure 18.3 shows the recovered
structure. △
18.6 Reconstruction from sequences 455
Fig. 18.4. Wilshire: 3D points and cameras for 350 frames of a helicopter shot. Cameras are shown
for just the start and end frames for clarity, with the camera path plotted between.
Example 18.4. “Wilshire” sequence
This is a helicopter shot of Wilshire Boulevard, Los Angeles. In this case reconstruction
is hampered by the repeated structure in the scene – many of the feature points
(for example those on the skyscraper windows) have very similar intensity neighbourhoods,
so correlation-based tracking produces many false candidates. However, the
robust geometry-guided matching successfully rejects the incorrect correspondences.
Figure 18.4 shows the structure. △
456 18 N-View Computational Methods
18.7 Closure
It is probably fair to say that no fully satisfactory technique for reconstruction from a
sequence of projective images is known, and many ad-hoc techniques have been used,
with reasonable success. Four views is the limit for the closed-form solutions based
on multiview tensors. For larger numbers of views there is no such neat mathematical
formulation of the problem. One exception to this is the m-view technique based on
duality (see chapter 20), but this techniques is limited to six to eight points, depending
on which dual tensor (fundamental matrix, trifocal tensor or quadrifocal tensor) is used.
Most sequences will contain many more matched points than this.
18.7.1 Literature
The Tomasi–Kanade algorithm was first proposed for orthographic projection,
[Tomasi-92], but was later extended to paraperspective [Poelman-94]. It has been extended
to lines and conics, e.g. [Kahl-98a], but the MLE property no longer applies,
and it is unclear what is being minimized in the affine reconstruction. Others have investigated
subspace methods for multiple affine views in the case of planes [Irani-99],
and the case of multiple objects moving independently [Boult-91, Gear-98]. Non-rigid
factorization was formulated by [Brand-01, Torresani-01], though the elements of the
idea are present in [Bascle-98]. Affine reconstruction with uncertainty (covarianceweighted
data) has been discussed by Irani and Anandan [Irani-00, Anandan-02] The
method of affine reconstruction by alternating triangulation and camera estimation is
mentioned in [Huynh-03], under the name “PowerFactorization.”
The extension of factorization to projective cameras is due to Sturm and Triggs
[Sturm-96]. Methods of iteration using this approach have been proposed by
[Heyden-97a, Triggs-96].
A method of computing multiple cameras based on a plane homography was employed
in [Cross-99], initializing the ti vectors using planar auto-calibration.
Methods for obtaining an initial projective reconstruction from a sequence are
described in [Avidan-98, Beardsley-94, Beardsley-96, Fitzgibbon-98a, Laveau-96a,
Nister-00, Sturm-97b]. [Torr-99], and more recently [Pollefeys-02], discuss the important
problem of scene and motion degeneracies that may be encounted in sequence
reconstruction.
18.7.2 Notes and exercises
(i) The affine factorization algorithm can be applied to obtain a reconstruction in
situations where a set of cameras {Pi} have a common third row, even though
the cameras are not affine. The third row is the principal plane of the camera
(see section 6.2(p158)) and the condition of a common third row is equivalent
to coplanar principal planes. For example if a camera translates in a direction
perpendicular to its principal axis, then all the camera centres will lie on a plane,
and the principal planes are coplanar. The affine factorization algorithm can be
applied in this case because the set of cameras can be transformed as PiH4×4 to
18.7 Closure 457
the affine form by a 4×4 homography H satisfying P
3H4×4 = (0, 0, 0, 1), where
P
3 is the last row of Pi.
More generally, if the camera centres are restricted to a plane then the images
may be synthetically rotated such that the cameras effectively have coplanar
principal planes. For example in the case of planar motion (section 19.8) or
single axis rotation (section 19.9(p490)) if all the images are rotated such that
the principal axis is parallel to the rotation axis (by applying a homography to
each image which maps the horizon to infinity in the case of a vertical rotation
axis), then the principal planes of all the cameras are parallel. However, if the
cameras are not actually affine, then the algorithm will not give the ML estimate
of the reconstruction.-->
</p><p>
</p><p>
    </body>
</html>