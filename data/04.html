<!doctype html>
<html lang="ja">
    <head>
        <script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
        <meta charset="utf-8" />
        <title>4ç« </title>
        <style type="text/css">
            p
            {
                padding-left: 2em;
            }
            .margin-large
            {
                margin-left: 30px;
            }
           .margin-abstract {
               margin-left: 60px; /* å·¦ãƒãƒ¼ã‚¸ãƒ³ã‚’åºƒãã™ã‚‹ */
               margin-right: 60px; /* å³ãƒãƒ¼ã‚¸ãƒ³ã‚’åºƒãã™ã‚‹ */
           }
        </style>
    <style>
        .two-columns {
            display: flex;
            flex-direction: row;
            gap: 20px; /* åˆ—é–“ã®ã‚¹ãƒšãƒ¼ã‚¹ */
        }
        .column {
            flex: 1; /* å„åˆ—ãŒå‡ç­‰ã«å¹…ã‚’å–ã‚‹ */
        }
    </style>
<style>
.three-columns {
  display: flex;
  gap: 10px; /* åˆ—é–“ã®ä½™ç™½ã‚’è¨­å®š */
}
.column {
  flex: 1; /* å„åˆ—ã®å¹…ã‚’å‡ç­‰ã«ã™ã‚‹ */
  padding: 10px; /* å†…å´ã®ä½™ç™½ã‚’è¨­å®š */
}
</style>
    <style>
        .styleRef { 
            text-indent: -40px; /* æœ€åˆã®è¡Œã®å­—ä¸‹ã’ã‚’é€†æ–¹å‘ã« */
            margin-left: 10px; /* 2è¡Œç›®ä»¥é™ã®å­—ä¸‹ã’ã‚’èª¿æ•´ */
            ul {
                  list-style-type: none; /* ç®‡æ¡æ›¸ãè¨˜å·ã‚’éè¡¨ç¤º */
                  padding-left: 40px; /* å…¨ä½“ã®å·¦ä½™ç™½ */
            }
            li {
            }
        }
    </style>
    <style>
        .styleBullet { 
            text-indent: -20px; /* æœ€åˆã®è¡Œã®å­—ä¸‹ã’ã‚’é€†æ–¹å‘ã« */
            margin-left: 30px; /* 2è¡Œç›®ä»¥é™ã®å­—ä¸‹ã’ã‚’èª¿æ•´ */
            ul {
                  list-style-type: none; /* ç®‡æ¡æ›¸ãè¨˜å·ã‚’éè¡¨ç¤º */
                  padding-left: 0px; /* å…¨ä½“ã®å·¦ä½™ç™½ */
            }
            li {
            }
        }
    </style>
    <style>
            ol
            {
                margin-left: 30px;
            }
            ul
            {
                margin-left: 30px;
            }
    </style>
    </head>
    <body>
        <h1><center>4ç«  æ¨å®š â€“ 2æ¬¡å…ƒå°„å½±å¤‰æ›</center></h1>
<p>
æœ¬ç« ã§ã¯ã€æ¨å®šã®å•é¡Œã«ã¤ã„ã¦è€ƒå¯Ÿã—ã¾ã™ã€‚æœ¬ç¨¿ã§ã¯ã€æ¨å®šã¨ã¯ã€ä½•ã‚‰ã‹ã®æ¸¬å®šã«åŸºã¥ã„ã¦ã€ä½•ã‚‰ã‹ã®å¤‰æ›ã‚„ãã®ä»–ã®æ•°å­¦çš„é‡ã‚’è¨ˆç®—ã™ã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚ã“ã®å®šç¾©ã¯ã‚„ã‚„æ›–æ˜§ã§ã‚ã‚‹ãŸã‚ã€ã‚ˆã‚Šå…·ä½“çš„ã«ã™ã‚‹ãŸã‚ã«ã€ã“ã®ç¨®ã®æ¨å®šå•é¡Œã‚’ã„ãã¤ã‹æŒ™ã’ã¦è€ƒå¯Ÿã—ã¾ã™ã€‚

<!-- In this chapter, we consider the problem of estimation. In the present context this
will be taken to mean the computation of some transformation or other mathematical
quantity, based on measurements of some nature. This definition is somewhat vague,
so to make it more concrete, here are a number of estimation problems of the type that
we would like to consider. -->

<div class="styleBullet">
<ul>
<li>(i) <strong>2æ¬¡å…ƒãƒ›ãƒ¢ã‚°ãƒ©ãƒ•ã‚£</strong>ã€€\(\mathbb P^2\) å†…ã®ç‚¹é›†åˆ \(x_i\) ã¨ã€åŒã˜ã \(\mathbb P^2\) å†…ã®å¯¾å¿œã™ã‚‹ç‚¹é›†åˆ \(x_i^\prime\) ãŒä¸ãˆã‚‰ã‚ŒãŸã¨ãã€å„ \(x_i\) ã‚’ \(x_i^\prime\) ã¸å°„å½±å¤‰æ›ã™ã‚‹å°„å½±å¤‰æ›ã‚’è¨ˆç®—ã—ã¾ã™ã€‚å®Ÿéš›ã«ã¯ã€ç‚¹ \(x_i\) ã¨ \(x_i^\prime\) ã¯2ã¤ã®ç”»åƒï¼ˆã¾ãŸã¯åŒã˜ç”»åƒï¼‰ä¸Šã®ç‚¹ã§ã‚ã‚Šã€å„ç”»åƒã¯å°„å½±å¹³é¢ \(\mathbb P^2\) ã¨ã¿ãªã•ã‚Œã¾ã™ã€‚ 
</li><br><li>(ii) <strong>3D ã‹ã‚‰ 2D ã¸ã®ã‚«ãƒ¡ãƒ©æŠ•å½±</strong>ã€€3D ç©ºé–“å†…ã®ç‚¹é›†åˆ \(X_i\) ã¨ã€ç”»åƒå†…ã®å¯¾å¿œã™ã‚‹ç‚¹é›†åˆ \(x_i\) ãŒä¸ãˆã‚‰ã‚ŒãŸå ´åˆã€\(X_i\) ã‚’ \(x_i\) ã«å†™ã™ 3D ã‹ã‚‰ 2D ã¸ã®å°„å½±å†™åƒã‚’æ±‚ã‚ã¾ã™ã€‚ã“ã®ã‚ˆã†ãª 3D ã‹ã‚‰ 2D ã¸ã®å°„å½±å†™åƒã¯ã€ç¬¬ 6 ç« ã§èª¬æ˜ã—ãŸã‚ˆã†ã«ã€å°„å½±ã‚«ãƒ¡ãƒ©ã«ã‚ˆã£ã¦å®Ÿè¡Œã•ã‚Œã‚‹å†™åƒã§ã™ã€‚
</li><br><li>(iii) <strong>åŸºç¤è¡Œåˆ—è¨ˆç®—</strong>ã€€ã‚ã‚‹ç”»åƒå†…ã®ç‚¹é›†åˆ \(x_i\) ã¨ã€åˆ¥ã®ç”»åƒå†…ã®å¯¾å¿œã™ã‚‹ç‚¹ \(x_i^\prime\) ãŒä¸ãˆã‚‰ã‚ŒãŸå ´åˆã€ã“ã‚Œã‚‰ã®å¯¾å¿œé–¢ä¿‚ã¨ä¸€è‡´ã™ã‚‹åŸºç¤è¡Œåˆ— \(F\) ã‚’è¨ˆç®—ã—ã¾ã™ã€‚ç¬¬9ç« ã§èª¬æ˜ã—ãŸåŸºç¤è¡Œåˆ—ã¯ã€ã™ã¹ã¦ã®\(i\)ã«å¯¾ã—ã¦\({x_i^\prime}^TTFx_i = 0\) ã‚’æº€ãŸã™ç‰¹ç•°ãª \(3Ã—3\) è¡Œåˆ— \(F\) ã§ã™ã€‚
</li><br><li>(iv) <strong>ä¸‰ç„¦ç‚¹ãƒ†ãƒ³ã‚½ãƒ«ã®è¨ˆç®—</strong>ã€€3æšã®ç”»åƒã«ã‚ãŸã‚‹ç‚¹ã®å¯¾å¿œé›†åˆ \(x_i \leftrightarrow x_i^\prime \leftrightarrow x_i^{\prime\prime}\) ãŒä¸ãˆã‚‰ã‚ŒãŸå ´åˆã€ä¸‰ç„¦ç‚¹ãƒ†ãƒ³ã‚½ãƒ«ã‚’è¨ˆç®—ã—ã¾ã™ã€‚ç¬¬15ç« ã§èª¬æ˜ã™ã‚‹ä¸‰ç„¦ç‚¹ãƒ†ãƒ³ã‚½ãƒ«ã¯ã€3ã¤ã®ãƒ“ãƒ¥ãƒ¼å†…ã®ç‚¹ã¾ãŸã¯ç·šã‚’é–¢é€£ä»˜ã‘ã‚‹ãƒ†ãƒ³ã‚½ãƒ«\(\mathcal T_i^{jk}\)ã§ã™ã€‚</li>
</ul>
</div>

<!-- (i) <strong>2D homography.</strong> Given a set of points \(x_i\) in \(\mathbb P^2\) and a corresponding set of points \(x_i^\prime\) likewise in \(\mathbb P^2\), compute the projective transformation that takes each \(x_i\) to \(x_i^\prime\). In a practical situation, the points \(x_i\) and \(x_i^\prime\) are points in two images
(or the same image), each image being considered as a projective plane \(\mathbb P^2\).
(ii) <strong>3D to 2D camera projection.</strong> Given a set of points \(X_i\) in 3D space, and a set of corresponding points \(x_i\) in an image, find the 3D to 2D projective mapping that maps \(X_i\) to \(x_i\). Such a 3D to 2D projection is the mapping carried out by a projective camera, as discussed in chapter 6.
(iii) <strong>Fundamental matrix computation.</strong> Given a set of points \(x_i\) in one image, and corresponding points \(x_i^\prime\) in another image, compute the fundamental matrix \(F\) consistent with these correspondences. The fundamental matrix, discussed in chapter 9, is a singular \(3Ã—3\) matrix \(F\) satisfying \({x_i^\prime}^TTFx_i = 0\( for all \(i\).
(iv) <strong>Trifocal tensor computation.</strong> Given a set of point correspondences \(x_i \leftrightarrow x_i^\prime \leftrightarrow x_i^{\rime\prime}\) across three images, compute the trifocal tensor. The trifocal tensor, discussed in chapter 15, is a tensor \(\mathcal T_i^{jk}\) relating points or lines in three views. -->

</p><p>
ã“ã‚Œã‚‰ã®å•é¡Œã«ã¯å¤šãã®å…±é€šç‚¹ãŒã‚ã‚Šã€ã„ãšã‚Œã‹ã®å•é¡Œã«é–¢é€£ã™ã‚‹è€ƒå¯Ÿã¯ä»–ã®å„å•é¡Œã«ã‚‚é–¢é€£ã—ã¦ã„ã¾ã™ã€‚ã—ãŸãŒã£ã¦ã€æœ¬ç« ã§ã¯ã€æœ€åˆã®å•é¡Œã«ã¤ã„ã¦è©³ç´°ã«æ¤œè¨ã—ã¾ã™ã€‚ã“ã®å•é¡Œã®è§£æ±ºæ–¹æ³•ã«ã¤ã„ã¦å­¦ã¶ã“ã¨ã¯ã€ä»–ã®å„å•é¡Œã®è§£æ±ºæ–¹æ³•ã‚’å­¦ã¶ã“ã¨ã«ã‚‚ãªã‚Šã¾ã™ã€‚

<!-- These problems have many features in common, and the considerations that relate to
one of the problems are also relevant to each of the others. Therefore, in this chapter,
the first of these problems will be considered in detail. What we learn about ways of
solving this problem will teach us how to proceed in solving each of the other problems
as well.
</p><p>
Apart from being important for illustrative purposes, the problem of estimating 2D
projective transformations is of importance in its own right. We consider a set of point
correspondences xi â†” xâ€²
i between two images. Our problem is to compute a 3 Ã— 3
matrix H such that Hxi = xâ€²
i for each i.
</p><p>
Number of measurements required. The first question to consider is how many
corresponding points xi â†” xâ€²
i are required to compute the projective transformation H.
A lower bound is available by a consideration of the number of degrees of freedom and
number of constraints. On the one hand, the matrix H contains 9 entries, but is defined
only up to scale. Thus, the total number of degrees of freedom in a 2D projective transformation
is 8. On the other hand, each point-to-point correspondence accounts for two
constraints, since for each point xi in the first image the two degrees of freedom of the
point in the second image must correspond to the mapped point Hxi. A 2D point has
two degrees of freedom corresponding to the x and y components, each of which may
be specified separately. Alternatively, the point is specified as a homogeneous 3-vector,
which also has two degrees of freedom since scale is arbitrary. As a consequence, it is
necessary to specify four point correspondences in order to constrain H fully.
</p><p>
Approximate solutions. It will be seen that if exactly four correspondences are given,
then an exact solution for the matrix H is possible. This is the minimal solution. Such
solutions are important as they define the size of the subsets required in robust estimation
algorithms, such as RANSAC, described in section 4.7. However, since points are
measured inexactly (â€œnoiseâ€), if more than four such correspondences are given, then
these correspondences may not be fully compatible with any projective transformation,
and one will be faced with the task of determining the â€œbestâ€ transformation given the
data. This will generally be done by finding the transformation H that minimizes some
cost function. Different cost functions will be discussed during this chapter, together
with methods for minimizing them. There are two main categories of cost function:
those based on minimizing an algebraic error; and those based on minimizing a geometric
or statistical image distance. These two categories are described in section 4.2.
</p><p>
The Gold Standard algorithm. There will usually be one cost function which is
optimal in the sense that the H that minimizes it gives the best possible estimate of the
transformation under certain assumptions. The computational algorithm that enables
this cost function to be minimized is called the â€œGold Standardâ€ algorithm. The results
of other algorithms are assessed by how well they compare to this Gold Standard. In
the case of estimating a homography between two views the cost function is (4.8), the
assumptions for optimality are given in section 4.3, and the Gold Standard is algorithm
4.3.
</p>
<h2>4.1 The Direct Linear Transformation (DLT) algorithm</h2>
<p>
We begin with a simple linear algorithm for determining H given a set of four 2D to 2D
point correspondences, xi â†” xâ€²
i. The transformation is given by the equation xâ€²
i = Hxi.
Note that this is an equation involving homogeneous vectors; thus the 3-vectors xâ€²
i and
Hxi are not equal, they have the same direction but may differ in magnitude by a nonzero
scale factor. The equation may be expressed in terms of the vector cross product
as xâ€²
i Ã— Hxi = 0. This form will enable a simple linear solution for H to be derived.
4.1 The Direct Linear Transformation (DLT) algorithm 89
If the j-th row of the matrix H is denoted by hjT, then we may write
Hxi =
ï£«
ï£¬ï£­
h1Txi
h2Txi
h3Txi
ï£¶
ï£·ï£¸
.
Writing xâ€²
i = (xâ€²
i, yâ€²
i,wâ€²
i)T, the cross product may then be given explicitly as
xâ€²
i Ã— Hxi =
ï£«
ï£¬ï£­
yâ€²
ih3Txi âˆ’ wâ€²
ih2Txi
wâ€²
ih1Txi âˆ’ xâ€²
ih3Txi
xâ€²
ih2Txi âˆ’ yâ€²
ih1Txi
ï£¶
ï£·ï£¸
.
Since hjTxi = xT
i hj for j = 1, . . . , 3, this gives a set of three equations in the entries
of H, which may be written in the form
ï£®
ï£¯ï£°
0T âˆ’wâ€²
ixT
i yâ€²
ixT
i
wâ€²
ixT
i 0T âˆ’xâ€²
ixT
i
âˆ’yâ€²
ixT
i xâ€²
ixT
i 0T
ï£¹
ï£ºï£»
ï£«
ï£¬ï£­
h1
h2
h3
ï£¶
ï£·ï£¸
= 0. (4.1)
These equations have the form Aih = 0, where Ai is a 3Ã—9 matrix, and h is a 9-vector
made up of the entries of the matrix H,
h =
ï£«
ï£¬ï£­
h1
h2
h3
ï£¶
ï£·ï£¸
, H =
ï£®
ï£¯ï£°
h1 h2 h3
h4 h5 h6
h7 h8 h9
ï£¹
ï£ºï£»
(4.2)
with hi the iâˆ’th element of h. Three remarks regarding these equations are in order
here.
(i) The equation Aih = 0 is an equation linear in the unknown h. The matrix
elements of Ai are quadratic in the known coordinates of the points.
(ii) Although there are three equations in (4.1), only two of them are linearly independent
(since the third row is obtained, up to scale, from the sum of xâ€²
i times
the first row and yâ€²
i times the second). Thus each point correspondence gives
two equations in the entries of H. It is usual to omit the third equation in solving
for H ([Sutherland-63]). Then (for future reference) the set of equations
becomes
"
0T âˆ’wâ€²
ixT
i yâ€²
ixT
i
wâ€²
ixT
i 0T âˆ’xâ€²
ixT
i
#ï£«
ï£¬ï£­
h1
h2
h3
ï£¶
ï£·ï£¸
= 0. (4.3)
This will be written
Aih = 0
where Ai is now the 2 Ã— 9 matrix of (4.3).
(iii) The equations hold for any homogeneous coordinate representation (xâ€²
i, yâ€²
i,wâ€²
i)T
of the point xâ€²
i. One may choose wâ€²
i = 1, which means that (xâ€²
i, yâ€²
i) are the
coordinates measured in the image. Other choices are possible, however, as
will be seen later.
90 4 Estimation â€“ 2D Projective Transformations
Solving for H
Each point correspondence gives rise to two independent equations in the entries of H.
Given a set of four such point correspondences, we obtain a set of equations Ah = 0,
where A is the matrix of equation coefficients built from the matrix rows Ai contributed
from each correspondence, and h is the vector of unknown entries of H. We seek a
non-zero solution h, since the obvious solution h = 0 is of no interest to us. If (4.1) is
used then A has dimension 12 Ã— 9, and if (4.3) the dimension is 8 Ã— 9. In either case
A has rank 8, and thus has a 1-dimensional null-space which provides a solution for h.
Such a solution h can only be determined up to a non-zero scale factor. However, H is
in general only determined up to scale, so the solution h gives the required H. A scale
may be arbitrarily chosen for h by a requirement on its norm such as khk = 1.
4.1.1 Over-determined solution
If more than four point correspondences xi â†” xâ€²
i are given, then the set of equations
Ah = 0 derived from (4.3) is over-determined. If the position of the points is exact
then the matrix A will still have rank 8, a one dimensional null-space, and there is an
exact solution for h. This will not be the case if the measurement of image coordinates
is inexact (generally termed noise) â€“ there will not be an exact solution to the overdetermined
system Ah = 0 apart from the zero solution. Instead of demanding an
exact solution, one attempts to find an approximate solution, namely a vector h that
minimizes a suitable cost function. The question that naturally arises then is: what
should be minimized? Clearly, to avoid the solution h = 0 an additional constraint is
required. Generally, a condition on the norm is used, such as khk = 1. The value of
the norm is unimportant since H is only defined up to scale. Given that there is no exact
solution to Ah = 0, it seems natural to attempt to minimize the norm kAhk instead,
subject to the usual constraint, khk = 1. This is identical to the problem of finding
the minimum of the quotient kAhk/khk. As shown in section A5.3(p592) the solution
is the (unit) eigenvector of ATA with least eigenvalue. Equivalently, the solution is the
unit singular vector corresponding to the smallest singular value of A. The resulting
algorithm, known as the basic DLT algorithm, is summarized in algorithm 4.1.
4.1.2 Inhomogeneous solution
An alternative to solving for h directly as a homogeneous vector is to turn the set of
equations (4.3) into a inhomogeneous set of linear equations by imposing a condition
hj = 1 for some entry of the vector h. Imposing the condition hj = 1 is justified by
the observation that the solution is determined only up to scale, and this scale can be
chosen such that hj = 1. For example, if the last element of h, which corresponds to
H33, is chosen as unity then the resulting equations derived from (4.3) are
"
0 0 0 âˆ’xiwâ€²
i âˆ’yiwâ€²
i âˆ’wiwâ€²
i xiyâ€²
i yiyâ€²
i
xiwâ€²
i yiwâ€²
i wiwâ€²
i 0 0 0 âˆ’xixâ€²
i âˆ’yixâ€²
i
#
Ëœh
=
 
âˆ’wiyâ€²
i
wixâ€²
i
!
where Ëœh is an 8-vector consisting of the first 8 components of h. Concatenating the
equations from four correspondences then generates a matrix equation of the form
4.1 The Direct Linear Transformation (DLT) algorithm 91
Objective
Given n â‰¥ 4 2D to 2D point correspondences {xi â†” xâ€²
i}, determine the 2D homography
matrix H such that xâ€²
i = Hxi.
Algorithm
(i) For each correspondence xi â†” xâ€²
i compute the matrix Ai from (4.1). Only the first two
rows need be used in general.
(ii) Assemble the n 2 Ã— 9 matrices Ai into a single 2n Ã— 9 matrix A.
(iii) Obtain the SVD of A (section A4.4(p585)). The unit singular vector corresponding to
the smallest singular value is the solution h. Specifically, if A = UDVT with D diagonal
with positive diagonal entries, arranged in descending order down the diagonal, then h
is the last column of V.
(iv) The matrix H is determined from h as in (4.2).
Algorithm 4.1. The basic DLT for H (but see algorithm 4.2(p109) which includes normalization).
MËœh = b, where M has 8 columns and b is an 8-vector. Such an equation may be solved
for Ëœh using standard techniques for solving linear equations (such as Gaussian elimination)
in the case where M contains just 8 rows (the minimum case), or by least-squares
techniques (section A5.1(p588)) in the case of an over-determined set of equations.
However, if in fact hj = 0 is the true solution, then no multiplicative scale k can
exist such that khj = 1. This means that the true solution cannot be reached. For this
reason, this method can be expected to lead to unstable results in the case where the
chosen hj is close to zero. Consequently, this method is not recommended in general.
Example 4.1. It will be shown that h9 = H33 is zero if the coordinate origin is mapped
to a point at infinity by H. Since (0, 0, 1)T represents the coordinate origin x0, and
also (0, 0, 1)T represents the line at infinity l, this condition may be written as lTHx0 =
(0, 0, 1)H(0, 0, 1)T = 0, thus H33 = 0. In a perspective image of a scene plane the line
at infinity is imaged as the vanishing line of the plane (see chapter 8), for example the
horizon is the vanishing line of the ground plane. It is not uncommon for the horizon to
pass through the image centre, and for the coordinate origin to coincide with the image
centre. In this case the mapping that takes the image to the world plane maps the origin
to the line at infinity, so that the true solution has H33 = h9 = 0. Consequently, an
h9 = 1 normalization can be a serious failing in practical situations. â–³
4.1.3 Degenerate configurations
Consider a minimal solution in which a homography is computed using four point correspondences,
and suppose that three of the points x1, x2, x3 are collinear. The question
is whether this is significant. If the corresponding points xâ€²
1, xâ€²
2, xâ€²
3 are also collinear
then one might suspect that the homography is not sufficiently constrained, and there
will exist a family of homographies mapping xi to xâ€²
i. On the other hand, if the corresponding
points xâ€²
1, xâ€²
2, xâ€²
3 are not collinear then clearly there can be no transformation
H taking xi to xâ€²
i, since a projective transformation must preserve collinearity. Never92
4 Estimation â€“ 2D Projective Transformations
theless the set of eight homogeneous equations derived from (4.3) must have a non-zero
solution, giving rise to a matrix H. How is this apparent contradiction to be resolved?
The equations (4.3) express the condition that xâ€²
i Ã— Hxi = 0 for i = 1, . . . , 4, and
so the matrix H found by solving the system of 8 equations will satisfy this condition.
Suppose that x1, . . . , x3 are collinear and let l be the line that they lie on, so that lTxi =
0 for i = 1, . . . , 3. Now define Hâˆ— = xâ€²
4lT, which is a 3Ã—3 matrix of rank 1. In this case,
one verifies that Hâˆ—xi = xâ€²
4(lTxi) = 0 for i = 1, . . . , 3, since lTxi = 0. On the other
hand, Hâˆ—x4 = xâ€²
4(lTx4) = kxâ€²
4. Therefore the condition xâ€²
i Ã— Hâˆ—xi = 0 is satisfied for
all i. Note that the vector hâˆ— corresponding to Hâˆ— is given by hâˆ—T = (x4lT, y4lT,w4lT),
and one easily verifies that this vector satisfies (4.3) for all i. The problem with this
solution for Hâˆ— is that Hâˆ— is a rank 1 matrix and hence does not represent a projective
transformation. As a consequence the points Hâˆ—xi = 0 for i = 1, . . . , 3 are not well
defined.
We showed that if x1, x2, x3 are collinear then Hâˆ— = xâ€²
4lT is a solution to (4.1). There
are two cases: either Hâˆ— is the unique solution (up to scale) or there is a further solution
H. In the first case, since Hâˆ— is a singular matrix, there exists no transformation taking
each xi to xâ€²
i. This occurs when x1, . . . , x3 are collinear but xâ€²
1, . . . , xâ€²
3 are not. In the
second case, where a further solution H exists, then any matrix of the form Î± Hâˆ— + Î² H
is a solution. Thus a 2-dimensional family of transformations exist, and it follows that
the 8 equations derived from (4.3) are not independent.
A situation where a configuration does not determine a unique solution for a particular
class of transformation is termed degenerate. Note that the definition of degeneracy
involves both the configuration and the type of transformation. The degeneracy problem
is not limited to a minimal solution, however. If additional (perfect, i.e. error-free)
correspondences are supplied which are also collinear (lie on l), then the degeneracy is
not resolved.
4.1.4 Solutions from lines and other entities
The development to this point, and for the rest of the chapter, is exclusively in terms of
computing homographies from point correspondences. However, an identical development
can be given for computing homographies from line correspondences. Starting
from the line transformation li = HTlâ€²
i, a matrix equation of the form Ah = 0 can be
derived, with a minimal solution requiring four lines in general position. Similarly, a
homography may be computed from conic correspondences and so forth.
There is the question then of how many correspondences are required to compute the
homography (or any other relation). The general rule is that the number of constraints
must equal or exceed the number of degrees of freedom of the transformation. For
example, in 2D each corresponding point or line generates two constraints on H, in
3D each corresponding point or plane generates three constraints. Thus in 2D the
correspondence of four points or four lines is sufficient to compute H, since 4 Ã— 2 = 8,
with 8 the number of degrees of freedom of the homography. In 3D a homography has
15 degrees of freedom, and five points or five planes are required. For a planar affine
transformation (6 dof) only three corresponding points or lines are required, and so on.
A conic provides five constraints on a 2D homography.
4.2 Different cost functions 93
= =
Fig. 4.1. Geometric equivalence of pointâ€“line configurations. A configuration of two points and two
lines is equivalent to five lines with four concurrent, or five points with four collinear.
Care has to be taken when computing H from correspondences of mixed type. For
example, a 2D homography cannot be determined uniquely from the correspondences
of two points and two lines, but can from three points and one line or one point and three
lines, even though in each case the configuration has 8 degrees of freedom. The case of
three lines and one point is geometrically equivalent to four points, since the three lines
define a triangle and the vertices of the triangle uniquely define three points. We have
seen that the correspondence of four points in general position uniquely determines a
homography, which means that the correspondence of three lines and one point also
uniquely determines a homography. Similarly the case of three points and a line is
equivalent to four lines, and again the correspondence of four lines in general position
(i.e. no three concurrent) uniquely determines a homography. However, as a quick
sketch shows (figure 4.1), the case of two points and two lines is equivalent to five
lines with four concurrent, or five points with four collinear. As shown in the previous
section, this configuration is degenerate and a one-parameter family of homographies
map the two-point and two-line configuration to the corresponding configuration.
4.2 Different cost functions
We will now describe a number of cost functions which may be minimized in order to
determine H for over-determined solutions. Methods of minimizing these functions are
described later in the chapter.
4.2.1 Algebraic distance
The DLT algorithm minimizes the norm kAhk. The vector Ç« = Ah is called the residual
vector and it is the norm of this error vector that is minimized. The components of this
vector arise from the individual correspondences that generate each row of the matrix
A. Each correspondence xi â†” xâ€²
i contributes a partial error vector Ç«i from (4.1) or (4.3)
towards the full error vector Ç«. This vector Ç«i is the algebraic error vector associated
with the point correspondence xi â†” xâ€²
i and the homography H. The norm of this vector
is a scalar which is called the algebraic distance:
dalg(xâ€²
i, Hxi)2 = kÇ«ik2 =


"
0T âˆ’wâ€²
ixT
i yâ€²
ixT
i
wâ€²
ixT
i 0T âˆ’xâ€²
ixT
i
#
h


2
. (4.4)
More generally, and briefly, for any two vectors x1 and x2 we may write
dalg(x1, x2)2 = a21
+ a22
where a = (a1, a2, a3)T = x1 Ã— x2.
94 4 Estimation â€“ 2D Projective Transformations
The relation of this distance to a geometric distance is described in section 4.2.4.
Given a set of correspondences, the quantity Ç« = Ah is the algebraic error vector for
the complete set, and one sees that
X
i
dalg(xâ€²
i, Hxi)2 =
X
i kÇ«ik2 = kAhk2 = kÇ«k2. (4.5)
The concept of algebraic distance originated in the conic-fitting work of Bookstein
[Bookstein-79]. Its disadvantage is that the quantity that is minimized is not
geometrically or statistically meaningful. As Bookstein demonstrated, the solutions
that minimize algebraic distance may not be those expected intuitively. Nevertheless,
with a good choice of normalization (as will be discussed in section 4.4) methods which
minimize algebraic distance do give very good results. Their particular advantages are
a linear (and thus a unique) solution, and computational cheapness. Often solutions
based on algebraic distance are used as a starting point for a non-linear minimization
of a geometric or statistical cost function. The non-linear minimization gives the solution
a final â€œpolishâ€.
4.2.2 Geometric distance
Next we discuss alternative error functions based on the measurement of geometric
distance in the image, and minimization of the difference between the measured and
estimated image coordinates.
Notation. Vectors x represent the measured image coordinates; Ë†x represent estimated
values of the points and Â¯x represent true values of the points.
Error in one image. We start by considering error only in the second image, with
points in the first measured perfectly. Clearly, this will not be true in most practical
situations with images. An example where the assumption is more reasonable is in
estimating the projective transformation between a calibration pattern or a world plane,
where points are measured to a very high accuracy, and its image. The appropriate
quantity to be minimized is the transfer error. This is the Euclidean image distance
in the second image between the measured point xâ€² and the point HÂ¯x at which the
corresponding point Â¯x is mapped from the first image. We use the notation d(x, y) to
represent the Euclidean distance between the inhomogeneous points represented by x
and y. Then the transfer error for the set of correspondences is
X
i
d(xâ€²
i, HÂ¯xi)2. (4.6)
The estimated homography Ë†H is the one for which the error (4.6) is minimized.
Symmetric transfer error. In the more realistic case where image measurement errors
occur in both the images, it is preferable that errors be minimized in both images, and
not solely in the one. One way of constructing a more satisfactory error function is to
4.2 Different cost functions 95
consider the forward (H) and backward (Hâˆ’1) transformation, and sum the geometric
errors corresponding to each of these two transformations. Thus, the error is
X
i
d(xi, H
âˆ’1xâ€²
i)2 + d(xâ€²
i, Hxi)2. (4.7)
The first term in this sum is the transfer error in the first image, and the second term is
the transfer error in the second image. Again the estimated homography Ë†H is the one
for which (4.7) is minimized.
4.2.3 Reprojection error â€“ both images
An alternative method of quantifying error in each of the two images involves estimating
a â€œcorrectionâ€ for each correspondence. One asks how much it is necessary
to correct the measurements in each of the two images in order to obtain a perfectly
matched set of image points. One should compare this with the geometric one-image
transfer error (4.6) which measures the correction that it is necessary to make to the
measurements in one image (the second image) in order to get a set of perfectly matching
points.
In the present case, we are seeking a homography Ë†H and pairs of perfectly matched
points Ë†xi and Ë†xâ€²
i that minimize the total error function
X
i
d(xi, Ë†xi)2 + d(xâ€²
i, Ë†xâ€²
i)2 subject to Ë†xâ€²
i = Ë†HË†xi âˆ€i. (4.8)
Minimizing this cost function involves determining both Ë†H and a set of subsidiary correspondences
{Ë†xi} and {Ë†xâ€²
i}. This estimation models, for example, the situation that
measured correspondences xi â†” xâ€²
i arise from images of points on a world plane. We
wish to estimate a point on the world plane bXi from xi â†” xâ€²
i which is then reprojected
to the estimated perfectly matched correspondence Ë†xi â†” Ë†xâ€²
i.
This reprojection error function is compared with the symmetric error function
in figure 4.2. It will be seen in section 4.3 that (4.8) is related to the Maximum Likelihood
estimation of the homography and correspondences.
4.2.4 Comparison of geometric and algebraic distance
We return to the case of errors only in the second image. Let xâ€²
i = (xâ€²
i, yâ€²
i,wâ€²
i)T and
define a vector (Ë†xâ€²
i, Ë†yâ€²
i, Ë† wâ€²
i)T = Ë†xâ€²
i = HÂ¯xi. Using this notation, the left hand side of (4.3)
becomes
Aih = Ç«i =
 
yâ€²
i Ë† wâ€²
i âˆ’ wâ€²
iË†yâ€²
i
wâ€²
iË†xâ€²
i âˆ’ xâ€²
i Ë† wâ€²
i
!
.
This vector is the algebraic error vector associated with the point correspondence xi â†”
xâ€²
i and the camera mapping H. Thus,
dalg(xâ€²
i, Ë†xâ€²
i)2 = (yâ€²
i Ë† wâ€²
i âˆ’ wâ€²
iË†yâ€²
i)2 + (wâ€²
iË†xâ€²
i âˆ’ xâ€²
i Ë† wâ€²
i)2.
For points xâ€²
i and Ë†xâ€²
i the geometric distance is
d(xâ€²
i, Ë†xâ€²
i) =

(xâ€²
i/wâ€²
i âˆ’ Ë†xâ€²
i/ Ë† wâ€²
i)2 + (yâ€²
i/wâ€²
i âˆ’ Ë†yâ€²
i/ Ë† wâ€²
i)2
1/2
96 4 Estimation â€“ 2D Projective Transformations
x
x /
H-1
H-1
x /
image 1 image 2
x
H
x /
image 1 image 2
H
x
d
d
/
d / d
Fig. 4.2. A comparison between symmetric transfer error (upper) and reprojection error (lower) when
estimating a homography. The points x and xâ€² are the measured (noisy) points. Under the estimated
homography the points xâ€² and Hx do not correspond perfectly (and neither do the points x and Hâˆ’1xâ€²).
However, the estimated points, Ë†x and Ë†xâ€²
, do correspond perfectly by the homography Ë†xâ€² = HË†x. Using
the notation d(x, y) for the Euclidean image distance between x and y, the symmetric transfer error is
d(x, Hâˆ’1xâ€²)2 + d(xâ€², Hx)2; the reprojection error is d(x, Ë†x)2 + d(xâ€², Ë†xâ€²)2.
= dalg(xâ€²
i, Ë†xâ€²
i)/ Ë† wâ€²
iwâ€²
i.
Thus, geometric distance is related to, but not quite the same as, algebraic distance.
Note, though, that if Ë† wâ€²
i = wâ€²
i = 1, then the two distances are identical.
One can always assume that wi = 1, thus expressing the points xi in the usual form
xi = (xi, yi, 1)T. For one important class of 2D homographies, the values of Ë† wâ€²
i will
always be 1 as well. A 2D affine transformation is represented by a matrix of the
form (2.10â€“p39)
HA =
ï£®
ï£¯ï£°
h11 h12 h13
h21 h22 h23
0 0 1
ï£¹
ï£ºï£»
. (4.9)
One verifies immediately from Ë†xâ€²
i = HAÂ¯xi that Ë† wâ€²
i = 1 if wi = 1. This demonstrates
that in the case of an affine transformation geometric distance and algebraic distance are
identical. The DLT algorithm is easily adapted to enforce the condition that the last row
of H has the form (0, 0, 1) by setting h7 = h8 = 0. Hence, for affine transformations,
geometric distance can be minimized by the linear DLT algorithm based on algebraic
distance.
4.2.5 Geometric interpretation of reprojection error
The estimation of a homography between two planes can be thought of as fitting a â€œsurfaceâ€
to points in a 4D space, IR4. Each pair of image points x, xâ€² defines a single point
denoted X in a measurement space IR4, formed by concatenating the inhomogeneous
coordinates of x and xâ€². For a given specific homography H, the image correspondences
x â†” xâ€² that satisfy xâ€² Ã— (Hx) = 0 define an algebraic variety1 VH in IR4 which is the
1 A variety is the simultaneous zero-set of one or more multivariate polynomials defined in IRN.
4.2 Different cost functions 97
intersection of two quadric hypersurfaces. The surface is a quadric in IR4 because each
row of (4.1) is a degree 2 polynomial in x, y, xâ€², yâ€². The elements of H determine the
coefficient of each term of the polynomial, and so H specifies the particular quadric.
The two independent equations of (4.1) define two such quadrics.
Given points Xi = (xi, yi, xâ€²
i, yâ€²
i)T in IR4, the task of estimating a homography becomes
the task of finding a variety VH that passes (or most nearly passes) through the
points Xi. In general, of course, it will not be possible to fit a variety precisely. In this
case, let VH be some variety corresponding to a transformation H, and for each point
Xi, let bXi = (Ë†xi, Ë†yi, Ë†xâ€²
i, Ë†yâ€²
i)T be the closest point to Xi lying on the variety VH. One sees
immediately that
kXi âˆ’ bXik2 = (xi âˆ’ Ë†xi)2 + (yi âˆ’ Ë†yi)2 + (xâ€²
i âˆ’ Ë†xâ€²
i)2 + (yâ€²
i âˆ’ Ë†yâ€²
i)2
= d(xi, Ë†xi)2 + d(xâ€²
i, Ë†xâ€²
i)2.
Thus geometric distance in IR4 is equivalent to the reprojection error measured in both
the images, and finding the variety VH and points bXi on VH that minimize the squared
sum of distances to the measured points Xi is equivalent to finding the homography Ë†H
and the estimated points Ë†xi and Ë†xâ€²
i that minimize the reprojection error function (4.8).
The point bX on VH that lies closest to a measured point X is a point where the line
between X and bX is perpendicular to the tangent plane to VH at bX. Thus
d(xi, Ë†xi)2 + d(xâ€²
i, Ë†xâ€²
i)2 = dâŠ¥(Xi, VH)2
where dâŠ¥(X, VH) is the perpendicular distance of the point X to the variety VH. As may
be seen from the conic-fitting analogue discussed below, there may be more than one
such perpendicular from X to VH.
The distance dâŠ¥(X, VH) is invariant to rigid transformations of IR4, and this includes
as a special case rigid transformations of the coordinates (x, y), (xâ€², yâ€²) of each image
individually. This point is returned to in section 4.4.3.
Conic analogue. Before proceeding further we will first sketch an analogous estimation
problem that can be visualized more easily. The problem is fitting a conic to 2D
points, which occupies a useful intermediate position between fitting a straight line
(no curvature, too simple) and fitting a homography (four dimensions, with non-zero
curvature).
Consider the problem of fitting a conic to a set of n > 5 points (xi, yi)T on the
plane such that an error based on geometric distance is minimized. The points may
be thought of as â€œcorrespondencesâ€ xi â†” yi. The transfer distance and reprojection
(perpendicular) distance are illustrated in figure 4.3. It is clear from this figure that dâŠ¥
is less than or equal to the transfer error.
The algebraic distance of a point x from a conic C is defined as dalg(x, C)2 = xTCx.
A linear solution for C can be obtained by minimizing
P
i dalg(xi, C)2 with a suitable
normalization on C. There is no linear expression for the perpendicular distance of
a point (x, y) to a conic C, since through each point in IR2 there are up to 4 lines
perpendicular to C. The solution can be obtained from the roots of a quartic. However,
a function dâŠ¥(x, C) may be defined which returns the shortest distance between a conic
98 4 Estimation â€“ 2D Projective Transformations
y
x
d
d
d
dy
y
x
d
b
a
b
a
C
Fig. 4.3. A conic may be estimated from a set of 2D points by minimizing â€œsymmetric transfer errorâ€
d2
x+d2
y or the sum of squared perpendicular distances d2
âŠ¥. The analogue of transfer error is to consider
x as perfect and measure the distance dy to the conic in the y direction, and similarly for dx. For point
a it is clear that dâŠ¥ â‰¤ dx and dâŠ¥ â‰¤ dy. Also dâŠ¥ is more stable than dx or dy as illustrated by point b
where dx cannot be defined.
and a point. A conic can then be estimated by minimizing
P
i dâŠ¥(xi, C)2 over the five
parameters of C, though this cannot be achieved by a linear solution. Given a conic C
and a measured point x, a corrected point Ë†x is obtained simply by choosing the closest
point on C.
We return now to estimating a homography. In the case of an affine transformation
the variety is the intersection of two hyperplanes, i.e. it is a linear subspace of dimension
2. This follows from the form (4.9) of the affine matrix which for xâ€² = HAx yields
one linear constraint between x, xâ€², y and another between x, y, yâ€², each of which defines
a hyperplane in IR4. An analogue of this situation is line fitting to points on the
plane. In both cases the relation (affine transformation or line) may be estimated by
minimizing the perpendicular distance of points to the variety. In both cases there is a
closed form solution as discussed in the following section.
4.2.6 Sampson error
The geometric error (4.8) is quite complex in nature, and minimizing it requires the
simultaneous estimation of both the homography matrix and the points Ë†xi, Ë†xâ€²
i. This
non-linear estimation problem will be discussed further in section 4.5. Its complexity
contrasts with the simplicity of minimizing the algebraic error (4.4). The geometric
interpretation of geometric error given in section 4.2.5 leads to a further cost function
that lies between the algebraic and geometric cost functions in terms of complexity, but
gives a close approximation to geometric error. We will refer to this cost function as
Sampson error since Sampson [Sampson-82] used this approximation for conic fitting.
As described in section 4.2.5, the vector bX that minimizes the geometric error kX âˆ’ bXk2 is the closest point on the variety VH to the measurement X. This point can not be
estimated directly except via iteration, because of the non-linear nature of the variety
VH. The idea of the Sampson error function is to estimate a first-order approximation
to the point bX, assuming that the cost function is well approximated linearly in the
neighbourhood of the estimated point. The discussion to follow is related directly to
4.2 Different cost functions 99
the 2D homography estimation problem, but applies substantially unchanged to the
other estimation problems discussed in this book.
For a given homography H, any point X = (x, y, xâ€², yâ€²)T that lies on VH will satisfy
the equation (4.3â€“p89), or Ah = 0. To emphasize the dependency on X we will write
this instead as CH(X) = 0, where CH(X) is in this case a 2-vector. To first order, this
cost function may be approximated by a Taylor expansion
CH(X + Î´X) = CH(X) +
âˆ‚CH
âˆ‚X
Î´X. (4.10)
If we write Î´X = bX âˆ’ X and desire bX to lie on the variety VH so that CH(bX) = 0, then
the result is CH(X) + (âˆ‚CH/âˆ‚X)Î´X = 0, which we will henceforth write as JÎ´X = âˆ’Ç«
where J is the partial-derivative matrix, and Ç« is the cost CH(X) associated with X. The
minimization problem that we now face is to find the smallest Î´X that satisfies this
equation, namely:
â€¢ Find the vector Î´X that minimizes kÎ´Xk subject to JÎ´X = âˆ’Ç«.
The standard way to solve problems of this type is to use Lagrange multipliers. A
vector Î» of Lagrange multipliers is introduced, and the problem reduces to that of
finding the extrema of Î´T
X
Î´X âˆ’ 2Î»T(JÎ´X + Ç«), where the factor 2 is simply introduced
for convenience. Taking derivatives with respect to Î´X and equating to zero gives
2Î´T
X âˆ’ 2Î»T
J = 0T
from which we obtain Î´X = JTÎ». The derivative with respect to Î» gives JÎ´X + Ç« = 0,
the original constraint. Substituting for Î´X leads to
JJ
TÎ» = âˆ’Ç«
which may be solved for Î» giving Î» = âˆ’(JJT)âˆ’1Ç«, and so finally
Î´X = âˆ’J
T(JJ
T)âˆ’1Ç«, (4.11)
and bX = X + Î´X. The norm kÎ´Xk2 is the Sampson error:
kÎ´Xk2 = Î´T
X
Î´X = Ç«T(JJ
T)âˆ’1Ç«. (4.12)
Example 4.2. Sampson approximation for a conic
We will compute the Sampson approximation to the geometric distance dâŠ¥(x, C) between
a point x and conic C shown in figure 4.3. In this case the conic variety VC is
defined by the equation xTCx = 0, so that X = (x, y)T is a 2-vector, Ç« = xTCx is a
scalar, and J is the 1 Ã— 2 matrix given by
J =
"
âˆ‚(xTCx)
âˆ‚x
,
âˆ‚(xTCx)
âˆ‚y
#
.
This means that JJT is a scalar. The elements of J may be computed by the chain rule
as
âˆ‚(xTCx)
âˆ‚x
=
âˆ‚(xTCx)
âˆ‚x
âˆ‚x
âˆ‚x
= 2xT
C(1, 0, 0)T = 2(Cx)1
100 4 Estimation â€“ 2D Projective Transformations
where (Cx)i denotes the i-th component of the 3-vector Cx. Then from (4.12)
d2
âŠ¥ = kÎ´Xk2 = Ç«T(JJ
T)âˆ’1Ç« =
Ç«TÇ«
JJT =
(xTCx)2
4((Cx)21
+ (Cx)22
)
â–³
A few points to note:
(i) For the 2D homography estimation problem, X = (x, y, xâ€², yâ€²)T where the 2D
measurements are x = (x, y, 1)T and xâ€² = (xâ€², yâ€², 1)T.
(ii) Ç« = CH(X) is the algebraic error vector Aih â€“ a 2-vector â€“ and Ai is defined in
(4.3â€“p89).
(iii) J = âˆ‚CH(X)/âˆ‚X is a 2 Ã— 4 matrix. For example
J11 = âˆ‚(âˆ’wâ€²
ixT
i h2 + yâ€²
ixT
i h3)/âˆ‚x = âˆ’wâ€²
ih21 + yâ€²
ih31.
(iv) Note the similarity of (4.12) to the algebraic error kÇ«k = Ç«TÇ«. The Sampson
error may be interpreted as being the Mahalanobis norm (see section A2.1-
(p565)), kÇ«kJJT.
(v) One could alternatively use A defined by (4.1â€“p89), in which case J has dimension
3 Ã— 4 and Ç« is a 3-vector. However, in general the Sampson error,
and consequently the solution Î´X, will be independent of whether (4.1â€“p89) or
(4.3â€“p89) is used.
The Sampson error (4.12) is derived here for a single point pair. In applying this to
the estimation of a 2D homography H from several point correspondences xi â†” xâ€²
i, the
errors corresponding to all the point correspondences must be summed, giving
DâŠ¥ =
X
i
Ç«T
i (JiJ
T
i )âˆ’1Ç«i (4.13)
where Ç« and J both depend on H. To estimate H, this expression must be minimized
over all values of H. This is a simple minimization problem in which the set of variable
parameters consists only of the entries (or some other parametrization) of H.
This derivation of the Sampson error assumed that each point had isotropic (circular)
error distribution, the same in each image. The appropriate formulae for more general
Gaussian error distributions are given in the exercises at the end of this chapter.
Linear cost function
The algebraic error vector CH(X) = A(X)h is typically multilinear in the entries of X.
The case where A(X)h is linear is, however, important in its own right. The first point
to note is that in this case, the first-order approximation to geometric error given by the
Taylor expansion in (4.10) is exact (the higher order terms are zero), which means that
the Sampson error is identical to geometric error.
In addition, the variety VH defined by the equation CH(X) = 0, a set of linear equations,
is a hyperplane depending on H. The problem of finding H now becomes a
hyperplane fitting problem â€“ find the best fit to the data Xi among the hyperplanes
parametrized by H.
4.2 Different cost functions 101
As an example of this idea a linear algorithm which minimizes geometric error (4.8)
for an affine transformation is developed in the exercises at the end of this chapter.
4.2.7 Another geometric interpretation
It was shown in section 4.2.5 that finding a homography that takes a set of points xi
to another set xâ€²
i is equivalent to the problem of fitting a variety of a given type to a
set of points in IR4. We now consider a different interpretation in which the set of all
measurements is represented by a single point in a measurement space IRN.
The estimation problems we consider may all be fitted into a common framework.
In abstract terms the estimation problem has two components,
â€¢ a measurement space IRN consisting of measurement vectors X, and
â€¢ a model, which in abstract terms may be thought of simply as a subset S of points in
IRN. A measurement vector X that lies inside this subset is said to satisfy the model.
Typically the subspace that satisfies the model is a submanifold, or variety in IRN.
Now, given a measurement vector X in IRN, the estimation problem is to find the vector
bX, closest to X, that satisfies the model.
It will now be pointed out how the 2D homography estimation problem fits into this
framework.
Error in both images. Let {xi â†” xâ€²
i} be a set of measured matched points for
i = 1, . . . , n. In all, there are 4n measurements, namely two coordinates in each of
two images for n points. Thus, the set of matched points represents a point in IRN,
where N = 4n. The vector made up of the coordinates of all the matched points in
both images will be denoted X.
Of course, not all sets of point pairs xi â†” xâ€²
i are related via a homography H. A set
of point correspondences {xi â†” xâ€²
i} for which there exists a projective transformation
H satisfying xâ€²
i = Hxi for all i constitutes the subset of IRN satisfying the model. In
general, this set of points will form a submanifold S in IRN (in fact a variety) of some
dimension. The dimension of this submanifold is equal to the minimal number of
parameters that may be used to parametrize the submanifold.
One may arbitrarily choose n points Ë†xi in the first image. In addition, a homography
H may be chosen arbitrarily. Once these choices have been made, the points Ë†xâ€²
i in
the second image are determined by Ë†xâ€²
i = HË†xi. Thus, a feasible choice of points is
determined by a set of 2n + 8 parameters: the 2n coordinates of the points Ë†xi, plus
the 8 independent parameters (degrees of freedom) of the transformation H. Thus, the
submanifold S âŠ‚ IRN has dimension 2n + 8, and hence codimension 2n âˆ’ 8.
Given a set of measured point pairs {xi â†” xâ€²
i}, corresponding to a point X in IRN,
and an estimated point bX âˆˆ IRN lying on S, one easily verifies that
kX âˆ’ bX k2 =
X
i
d(xi, Ë†xi)2 + d(xâ€²
i, Ë†xâ€²
i)2.
Thus, finding the point bX on S lying closest to X in IRN is equivalent to minimizing
the cost function given by (4.8). The estimated correct correspondences Ë†xi â†” Ë†xâ€²
i are
102 4 Estimation â€“ 2D Projective Transformations
those corresponding to the closest surface point bX in IRN. Once bX is known H may be
computed.
Error in one image only. In the case of error in one image, one has a set of correspondences
{Â¯xi â†” x
â€²
i}. The points Â¯xi are assumed perfect. The inhomogeneous coordinates
of the xâ€²
i constitute the measurement vector X. Hence, in this case the measurement
space has dimension N = 2n. The vector bX consists of the inhomogeneous coordinates
of the mapped perfect points {HÂ¯x1, HÂ¯x2, . . . , HÂ¯xn}. The set of measurement
vectors satisfying the model is the set bX as H varies over the set of all homography
matrices. Once again this subspace is a variety. Its dimension is 8, since this is the total
number of degrees of freedom of the homography matrix H. As with the previous case,
the codimension is 2n âˆ’ 8. One verifies that
kX âˆ’ bX k2 =
X
i
d(xâ€²
i, HÂ¯xi)2.
Thus, finding the closest point on S to the measurement vector X is equivalent to minimizing
the cost function (4.6).
4.3 Statistical cost functions and Maximum Likelihood estimation
In section 4.2, various cost functions were considered that were related to geometric
distance between estimated and measured points in an image. The use of such cost
functions is now justified and then generalized by a consideration of error statistics of
the point measurements in an image.
In order to obtain a best (optimal) estimate of H it is necessary to have a model for
the measurement error (the â€œnoiseâ€). We are assuming here that in the absence of measurement
error the true points exactly satisfy a homography, i.e. Â¯xâ€²
i = HÂ¯xi. A common
assumption is that image coordinate measurement errors obey a Gaussian (or normal)
probability distribution. This assumption is surely not justified in general, and takes no
account of the presence of outliers (grossly erroneous measurements) in the measured
data. Methods for detecting and removing outliers will be discussed later in section 4.7.
Once outliers have been removed, the assumption of a Gaussian error model, if still not
strictly justified, becomes more tenable. Therefore, for the present, we assume that
image measurement errors obey a zero-mean isotropic Gaussian distribution. This distribution
is described in section A2.1(p565).
Specifically we assume that the noise is Gaussian on each image coordinate with
zero mean and uniform standard deviation Ïƒ. This means that x = Â¯x + x, with x
obeying a Gaussian distribution with variance Ïƒ2. If it is further assumed that the noise
on each measurement is independent, then, if the true point is Â¯x, the probability density
function (PDF) of each measured point x is
Pr(x) =

1
2Ï€Ïƒ2

eâˆ’d(x,Â¯x)2/(22). (4.14)
Error in one image. First we consider the case where the errors are only in the
second image. The probability of obtaining the set of correspondences {Â¯xi â†” xâ€²
i} is
4.3 Statistical cost functions and Maximum Likelihood estimation 103
simply the product of their individual PDFs, since the errors on each point are assumed
independent. Then the PDF of the noise-perturbed data is
Pr({xâ€²
i}|H) =
Y
i

1
2Ï€Ïƒ2

eâˆ’d(x
â€²
i,HÂ¯xi)2/(22) . (4.15)
The symbol Pr({xâ€²
i}|H) is to be interpreted as meaning the probability of obtaining the
measurements {xâ€²
i} given that the true homography is H. The log-likelihood of the set
of correspondences is
log Pr({xâ€²
i}|H) = âˆ’
1
2Ïƒ2
X
i
d(xâ€²
i, HÂ¯xi)2 + constant.
The Maximum Likelihood estimate (MLE) of the homography, Ë†H, maximizes this loglikelihood,
i.e. minimizes
X
i
d(xâ€²
i, HÂ¯xi)2.
Thus, we note that ML estimation is equivalent to minimizing the geometric error function
(4.6).
Error in both images. Following a similar development to the above, if the true
correspondences are {Â¯xi â†” HÂ¯xi = Â¯xâ€²
i}, then the PDF of the noise-perturbed data is
Pr({xi, xâ€²
i}|H, {Â¯xi}) =
Y
i

1
2Ï€Ïƒ2

eâˆ’(d(xi,Â¯xi)2+d(x
â€²
i,HÂ¯xi)2)/(22).
The additional complication here is that we have to seek â€œcorrectedâ€ image measurements
that play the role of the true measurements (HÂ¯x above). Thus the ML estimate of
the projective transformation H and the correspondences {xi â†” xâ€²
i}, is the homography
Ë†H and corrected correspondences {Ë†xi â†” Ë†xâ€²
i} that minimize
X
i
d(xi, Ë†xi)2 + d(xâ€²
i, Ë†xâ€²
i)2
with Ë†xâ€²
i = Ë†HË†xi. Note that in this case, the ML estimate is identical with minimizing the
reprojection error function (4.8).
Mahalanobis distance. In the general Gaussian case, one may assume a vector of
measurements X satisfying a Gaussian distribution function with covariance matrix
. The cases above are equivalent to a covariance matrix which is a multiple of the
identity.
Maximizing the log-likelihood is then equivalent to minimizing the Mahalanobis
distance (see section A2.1(p565))
kX âˆ’ Â¯X k2
 = (X âˆ’ Â¯X)T

âˆ’1(X âˆ’ Â¯X).
In the case where there is error in each image, but assuming that errors in one image
are independent of the error in the other image, the appropriate cost function is
kX âˆ’ Â¯X k2
 + kX
â€² âˆ’ Â¯X
â€²k2
â€²
104 4 Estimation â€“ 2D Projective Transformations
where  and â€² are the covariance matrices of the measurements in the two images.
Finally, if we assume that the errors for all the points xi and xâ€²
i are independent,
with individual covariance matrices i and â€²
i respectively, then the above expression
expands to
X
kxi âˆ’ Â¯xik2
i +
X
kxâ€²
i âˆ’ Â¯xâ€²
ik2
â€²
i
(4.16)
This equation allows the incorporation of the type of anisotropic covariance matrices
that arise for point locations computed as the intersection of two non-perpendicular
lines. In the case where the points are known exactly in one of the two images, errors
being confined to the other image, one of the two summation terms in (4.16) disappears.
4.4 Transformation invariance and normalization
We now start to discuss the properties and performance of the DLT algorithm of
section 4.1 and how it compares with algorithms minimizing geometric error. The
first topic is the invariance of the algorithm to different choices of coordinates in the
image. It is clear that it would generally be undesirable for the result of an algorithm
to be dependent on such arbitrary choices as the origin and scale, or even orientation,
of the coordinate system in an image.
4.4.1 Invariance to image coordinate transformations
Image coordinates are sometimes given with the origin at the top-left of the image,
and sometimes with the origin at the centre. The question immediately occurs whether
this makes a difference to the results of computing the transformation. Similarly, if
the units used to express image coordinates are changed by multiplication by some
factor, then is it possible that the result of the algorithm changes also? More generally,
to what extent is the result of an algorithm that minimizes a cost function to estimate
a homography dependent on the choice of coordinates in the image? Suppose, for
instance, that the image coordinates are changed by some similarity, affine or even
projective transformation before running the algorithm. Will this materially change the
result?
Formally, suppose that coordinates x in one image are replaced by Ëœx = Tx, and
coordinates xâ€² in the other image are replaced by Ëœxâ€² = Tâ€²xâ€², where T and Tâ€² are 3 Ã— 3
homographies. Substituting in the equation xâ€² = Hx, we derive the equation Ëœxâ€² =
Tâ€²HTâˆ’1Ëœx. This relation implies that eH
= Tâ€²HTâˆ’1 is the transformation matrix for the
point correspondences Ëœx â†” Ëœxâ€². An alternative method of finding the transformation
taking xi to xâ€²
i is therefore suggested, as follows.
(i) Transform the image coordinates according to transformations Ëœxi = Txi and
Ëœxâ€²
i = Tâ€²xâ€²
i.
(ii) Find the transformation eH
from the correspondences Ëœxi â†” Ëœxâ€²
i.
(iii) Set H = Tâ€²âˆ’1eH
T.
The transformation matrix H found in this way applies to the original untransformed
point correspondences xi â†” xâ€²
i. What choice should be made for the transformations T
and Tâ€² will be left unspecified for now. The question to be decided now is whether the
4.4 Transformation invariance and normalization 105
outcome of this algorithm is independent of the transformations T and Tâ€² being applied.
Ideally it ought to be, at least when T and Tâ€² are similarity transformations, since the
choice of a different scale, orientation or coordinate origin in the images should not
materially affect the outcome of the algorithm.
In the subsequent sections it will be shown that an algorithm that minimizes geometric
error is invariant to similarity transformations. On the other hand, for the DLT
algorithm as described in section 4.1, the result unfortunately is not invariant to similarity
transformations. The solution is to apply a normalizing transformation to the data
before applying the DLT algorithm. This normalizing transformation will nullify the
effect of the arbitrary selection of origin and scale in the coordinate frame of the image,
and will mean that the combined algorithm is invariant to a similarity transformation
of the image. Appropriate normalizing transformations will be discussed later.
4.4.2 Non-invariance of the DLT algorithm
Consider a set of correspondences xi â†” xâ€²
i and a matrix H that is the result of the DLT
algorithm applied to this set of corresponding points. Consider further a related set
of correspondences Ëœxi â†” Ëœxâ€²
i where Ëœxi = Txi and Ëœxâ€²
i = Tâ€²xâ€²
i, and let eH
be defined by
eH
= Tâ€²HTâˆ’1. Following section 4.4.1, the question to be decided here is the following:
â€¢ Does the DLT algorithm applied to the correspondence set Ëœxi â†” Ëœxâ€²
i yield the transformation
eH?
We will use the following notation: Matrix Ai is the DLT equation matrix (4.3â€“p89)
derived from a point correspondence xi â†” xâ€²
i, and A is the 2n Ã— 9 matrix formed by
stacking the Ai. Matrix ËœAi is similarly defined in terms of the correspondences Ëœxi â†” Ëœxâ€²
i,
where Ëœxi = Txi and Ëœxâ€²
i = Tâ€²xâ€²
i for some projective transformations T and Tâ€².
Result 4.3. Let Tâ€² be a similarity transformation with scale factor s, and let T be an
arbitrary projective transformation. Further, suppose H is any 2D homography and let
ËœH be defined by ËœH = Tâ€²HTâˆ’1. Then kËœAËœhk = skAhk where h and Ëœh are the vectors of
entries of H and ËœH.
Proof. Define the vector Ç«i = xâ€²
i Ã— Hxi. Note that Aih is the vector consisting of the
first two entries of Ç«i. Let Ëœ Ç«i be similarly defined in terms of the transformed quantities
as ËœÇ«i = Ëœxâ€²
i Ã—eH
Ëœxi. One computes:
ËœÇ«i = Ëœxâ€²
i Ã—eH
Ëœxi = T
â€²xâ€²
i Ã— (T
â€²
HT
âˆ’1)Txi
= T
â€²xâ€²
i Ã— T
â€²
Hxi = T
â€²âˆ—(xâ€²
i Ã— Hxi)
= T
â€²âˆ—Ç«i
where Tâ€²âˆ— represents the cofactor matrix of Tâ€² and the second-last equality follows
from lemma A4.2(p581). For a general transformation T, the error vectors Aih and
ËœAiËœh (namely the first two components of Ç«i and ËœÇ«i) are not simply related. However, in
the special case where Tâ€² is a similarity transformation, one may write Tâ€² =
"
sR t
0T 1
#
where R is a rotation matrix, t is a translation and s is a scaling factor. In this case, we
106 4 Estimation â€“ 2D Projective Transformations
see that Tâ€²âˆ— = s
"
R 0
âˆ’tTR s
#
. Applying Tâ€²âˆ— just to the first two components of Ç«i, one
sees that
ËœAiËœh = (ËœÇ«i1, ËœÇ«i2)T = sR(Ç«i1, Ç«i2)T = sRAih.
Since rotation does not affect vector norms, one sees that kËœAËœh k = skAhk, as required.
This result may be expressed in terms of algebraic error as
dalg(Ëœxâ€²
i,eH
Ëœxi) = sdalg(xâ€²
i, Hxi).
Thus, there is a one-to-one correspondence between H and eH
giving rise to the same
error, except for constant scale. It may appear therefore that the matrices H and eH
minimizing the algebraic error will be related by the formula eH
= Tâ€²HTâˆ’1, and hence
one may retrieve H as the product Tâ€²âˆ’1eH
T. This conclusion is false however. For,
although H and eH
so defined give rise to the same error Ç«, the condition kHk = 1,
imposed as a constraint on the solution, is not equivalent to the condition keHk = 1.
Specifically, kHk and keH
k are not related in any simple manner. Thus, there is no oneto-
one correspondence between H and eH
giving rise to the same error Ç«, subject to the
constraint kHk = keH
k = 1. Specifically,
minimize
X
i
dalg(xâ€²
i, Hxi)2 subject to kHk = 1
â‡” minimize
X
i
dalg(Ëœxâ€²
i,eH
Ëœxi)2 subject to kHk = 1
â‡”6 minimize
X
i
dalg(Ëœxâ€²
i,eH
Ëœxi)2 subject to keH
k = 1.
Thus, the method of transformation leads to a different solution for the computed
transformation matrix. This is a rather undesirable feature of the DLT algorithm as it
stands, that the result is changed by a change of coordinates, or even simply a change of
the origin of coordinates. If the constraint under which the norm kAhk is minimized is
invariant under the transformation, however, then one sees that the computed matrices
H and ËœH are related in the right way. Examples of minimization conditions for which H
is transformation-invariant are discussed in the exercises at the end of this chapter.
4.4.3 Invariance of geometric error
It will be shown now that minimizing geometric error to find H is invariant under similarity
(scaled Euclidean) transformations. As before, consider a point correspondence
x â†” xâ€² and a transformation matrix H. Also, define a related set of correspondences
Ëœx â†” Ëœxâ€² where Ëœx = Tx and Ëœxâ€² = Tâ€²xâ€², and leteH
be defined byeH
= Tâ€²HTâˆ’1. Suppose that
T and Tâ€² represent Euclidean transformations of IP2. One verifies that
d(Ëœxâ€²,eH
Ëœx) = d(T
â€²xâ€², T
â€²
HT
âˆ’1Tx) = d(T
â€²xâ€², T
â€²
Hx) = d(xâ€², Hx)
where the last equality holds because Euclidean distance is unchanged under a Euclidean
transformation such as Tâ€². This shows that if H minimizes the geometric error
4.4 Transformation invariance and normalization 107
for a set of correspondences, then eH
minimizes the geometric error for the transformed
set of correspondences, and so minimizing geometric error is invariant under Euclidean
transformations.
For similarity transformations, geometric error is multiplied by the scale factor of
the transformation, hence the minimizing transformations correspond in the same way
as in the Euclidean transformation case. Minimizing geometric error is invariant to
similarity transformations.
4.4.4 Normalizing transformations
As was shown in section 4.4.2, the result of the DLT algorithm for computing 2D
homographies depends on the coordinate frame in which points are expressed. In fact
the result is not invariant to similarity transformations of the image. This suggests
the question whether some coordinate systems are in some way better than others for
computing a 2D homography. The answer to this is an emphatic yes. In this section a
method of normalization of the data is described, consisting of translation and scaling
of image coordinates. This normalization should be carried out before applying the
DLT algorithm. Subsequently an appropriate correction to the result expresses the
computed H with respect to the original coordinate system.
Apart from improved accuracy of results, data normalization provides a second desirable
benefit, namely that an algorithm that incorporates an initial data normalization
step will be invariant with respect to arbitrary choices of the scale and coordinate origin.
This is because the normalization step undoes the effect of coordinate changes,
by effectively choosing a canonical coordinate frame for the measurement data. Thus,
algebraic minimization is carried out in a fixed canonical frame, and the DLT algorithm
is in practice invariant to similarity transformations.
Isotropic scaling. As a first step of normalization, the coordinates in each image are
translated (by a different translation for each image) so as to bring the centroid of the
set of all points to the origin. The coordinates are also scaled so that on the average a
point x is of the form x = (x, y,w)T, with each of x, y and w having the same average
magnitude. Rather than choose different scale factors for each coordinate direction, an
isotropic scaling factor is chosen so that the x and y-coordinates of a point are scaled
equally. To this end, we choose to scale the coordinates so that the average distance of
a point x from the origin is equal to âˆš2. This means that the â€œaverageâ€ point is equal
to (1, 1, 1)T. In summary the transformation is as follows:
(i) The points are translated so that their centroid is at the origin.
(ii) The points are then scaled so that the average distance from the origin is equal
to âˆš2.
(iii) This transformation is applied to each of the two images independently.
Why is normalization essential? The recommended version of the DLT algorithm
with data normalization is given in algorithm 4.2. We will now motivate why this
108 4 Estimation â€“ 2D Projective Transformations
version of the algorithm, incorporating data normalization, should be used in preference
to the basic DLT of algorithm 4.1(p91). Note that normalization is also called
pre-conditioning in the numerical literature.
The DLT method of algorithm 4.1 uses the SVD of A = UDVT to obtain a solution
to the overdetermined set of equations Ah = 0. These equations do not have an exact
solution (since the 2n Ã— 9 matrix A will not have rank 8 for noisy data), but the vector
h, given by the last column of V, provides a solution which minimizes kAhk (subject
to khk = 1). This is equivalent to finding the rank 8 matrix Ë†A which is closest to A in
Frobenius norm and obtaining h as the exact solution of Ë†Ah = 0. The matrix Ë†A is given
by Ë†A = UË†DVT where Ë†D is D with the smallest singular value set to zero. The matrix Ë†A has
rank 8 and minimizes the difference to A in Frobenius norm because
kA âˆ’ Ë†AkF = kUDV
T âˆ’ UË†DV
TkF = kD âˆ’ Ë†DkF.
where k.kF is the Frobenius norm, i.e. the square root of the sum of squares of all
entries.
Without normalization typical image points xi, xâ€²
i are of the order (x, y,w)T =
(100, 100, 1)T, i.e., x, y are much larger than w. In A the entries xxâ€², xyâ€², yxâ€², yyâ€² will
be of order 104, entries xwâ€², ywâ€² etc. of order 102, and entries wwâ€² will be unity. Replacing
A by Ë†A means that some entries are increased and others decreased such that
the square sum of differences of these changes is minimal (and the resulting matrix has
rank 8). However, and this is the key point, increasing the term wwâ€² by 100 means a
huge change in the image points, whereas increasing the term xxâ€² by 100 means only a
slight change. This is the reason why all entries in A must have similar magnitude and
why normalization is essential.
The effect of normalization is related to the condition number of the set of DLT equations,
or more precisely the ratio d1/dnâˆ’1 of the first to the second-last singular value
of the equation matrix A. This point is investigated in more detail in [Hartley-97c]. For
the present it is sufficient to say that for exact data and infinite precision arithmetic the
results will be independent of the normalizing transformation. However, in the presence
of noise the solution will diverge from the correct result. The effect of a large
condition number is to amplify this divergence. This is true even for infinite-precision
arithmetic â€“ this is not a round-off error effect.
The effect that this data normalization has on the results of the DLT algorithm is
shown graphically in figure 4.4. The conclusion to be drawn here is that data normalization
gives dramatically better results. The examples shown in the figure are chosen
to make the effect easily visible. However, a marked advantage remains even in cases
of computation from larger numbers of point correspondences, with points more widely
distributed. To emphasize this point we remark:
â€¢ Data normalization is an essential step in the DLT algorithm. It must not be considered
optional.
Data normalization becomes even more important for less well conditioned problems,
such as the DLT computation of the fundamental matrix or the trifocal tensor, which
will be considered in later chapters.
4.4 Transformation invariance and normalization 109
Objective
Given n â‰¥ 4 2D to 2D point correspondences {xi â†” xâ€²
i}, determine the 2D homography
matrix H such that xâ€²
i = Hxi.
Algorithm
(i) Normalization of x: Compute a similarity transformation T, consisting of a translation
and scaling, that takes points xi to a new set of points Ëœxi such that the centroid of the
points Ëœxi is the coordinate origin (0, 0)T, and their average distance from the origin is âˆš2.
(ii) Normalization of xâ€²: Compute a similar transformation Tâ€² for the points in the second
image, transforming points xâ€²
i to Ëœxâ€²
i.
(iii) DLT: Apply algorithm 4.1(p91) to the correspondences Ëœxi â†” Ëœxâ€²
i to obtain a homography
eH.
(iv) Denormalization: Set H = Tâ€²âˆ’1eHT.
Algorithm 4.2. The normalized DLT for 2D homographies.
a b
Fig. 4.4. Results ofMonte Carlo simulation (see section 5.3(p149) of computation of 2D homographies).
A set of 5 points (denoted by large crosses) was used to compute a 2D homography. Each of the 5 points
is mapped (in the noise-free case) to the point with the same coordinates, so that homography H is the
identity mapping. Now, 100 trials were made with each point being subject to 0.1 pixel Gaussian noise
in one image. (For reference, the large crosses are 4 pixels across.) The mapping H computed using the
DLT algorithm was then applied to transfer a further point into the second image. The 100 projections
of this point are shown with small crosses and the 95% ellipse computed from their scatter matrix is also
shown. (a) are the results without data normalization, and (b) the results with normalization. The leftand
rightmost reference points have (unnormalized) coordinates (130, 108) and (170, 108).
Non-isotropic scaling. Other methods of scaling are also possible. In non-isotropic
scaling, the centroid of the points is translated to the origin as before. After this translation
the points form a cloud about the origin. Scaling is then carried out so that
the two principal moments of the set of points are both equal to unity. Thus, the set
of points will form an approximately symmetric circular cloud of points of radius 1
about the origin. Experimental results given in [Hartley-97c] suggest that the extra effort
required for non-isotropic scaling does not lead to significantly better results than
isotropic scaling.
A further variant on scaling was discussed in [Muehlich-98], based on a statistical
analysis of the estimator, its bias and variance. In that paper it was observed that some
columns of A are not affected by noise. This applies to the third and sixth columns in
(4.3â€“p89), corresponding to the entry wiwâ€²
i = 1. Such error-free entries in A should not
be varied in finding Ë†A, the closest rank-deficient approximation to A. A method known
110 4 Estimation â€“ 2D Projective Transformations
as Total Least Squares - Fixed Columns is used to find the best solution. For estimation
of the fundamental matrix (see chapter 11), [Muehlich-98] reports slightly improved
results compared with non-isotropic scaling.
Scaling with points near infinity. Consider the case of estimation of a homography
between an infinite plane and an image. If the viewing direction is sufficiently oblique,
then very distant points in the plane may be visible in the image â€“ even points at infinity
(vanishing points) if the horizon is visible. In this case it makes no sense to normalize
the coordinates of points in the infinite plane by setting the centroid at the origin,
since the centroid may have very large coordinates, or be undefined. An approach to
normalization in this case is considered in exercise (iii) on page 128.
4.5 Iterative minimization methods
This section describes methods for minimizing the various geometric cost functions
developed in section 4.2 and section 4.3. Minimizing such cost functions requires
the use of iterative techniques. This is unfortunate, because iterative techniques tend
to have certain disadvantages compared to linear algorithms such as the normalized
DLT algorithm 4.2:
(i) They are slower.
(ii) They generally need an initial estimate at which to start the iteration.
(iii) They risk not converging, or converging to a local minimum instead of the
global minimum.
(iv) Selection of a stopping criterion for iteration may be tricky.
Consequently, iterative techniques generally require more careful implementation.
The technique of iterative minimization generally consists of five steps:
(i) Cost function. A cost function is chosen as the basis for minimization. Different
possible cost functions were discussed in section 4.2.
(ii) Parametrization. The transformation (or other entity) to be computed is expressed
in terms of a finite number of parameters. It is not in general necessary
that this be a minimum set of parameters, and there are in fact often advantages
to over-parametrization. (See the discussion below.)
(iii) Function specification. A function must be specified that expresses the cost
in terms of the set of parameters.
(iv) Initialization. A suitable initial parameter estimate is computed. This will
generally be done using a linear algorithm such as the DLT algorithm.
(v) Iteration. Starting from the initial solution, the parameters are iteratively
refined with the goal of minimizing the cost function.
A word about parametrization
For a given cost function, there are often several choices of parametrization. The general
strategy that guides parametrization is to select a set of parameters that cover the
complete space over which one is minimizing, while at the same time allowing one to
4.5 Iterative minimization methods 111
compute the cost function in a convenient manner. For example, H may be parametrized
by 9 parameters â€“ that is, it is over-parametrized, since there are really only 8 degrees
of freedom, overall scale not being significant. A minimal parametrization (i.e. the
same number of parameters as degrees of freedom) would involve only 8 parameters.
In general no bad effects are likely to occur if a minimization problem of this type is
over-parametrized, as long as for all choices of parameters the corresponding object is
of the desired type. In particular for homogeneous objects, such as the 3Ã—3 projection
matrix encountered here, it is usually not necessary or advisable to attempt to use a
minimal parametrization by removing the scale-factor ambiguity.
The reasoning is the following: it is not necessary to use minimal parametrization
because a well-performing non-linear minimization algorithm will â€œnoticeâ€ that it is
not necessary to move in redundant directions, such as the matrix scaling direction.
The algorithm described in Gill and Murray [Gill-78], which is a modification of the
Gaussâ€“Newton method, has an effective strategy for discarding redundant combinations
of the parameters. Similarly, the Levenberg-Marquardt algorithm (see section
A6.2(p600)) handles redundant parametrizations easily. It is not advisable because it
is found empirically that the cost function surface is more complicated when minimal
parametrizations are used. There is then a greater possibility of becoming stuck in a
local minimum.
One other issue that arises in choosing a parametrization is that of restricting the
transformation to a particular class. For example, suppose H is known to be a homology,
then as described in section A7.2(p629) it may be parametrized as
H = I + (Î¼ âˆ’ 1)
vaT
vTa
where Î¼ is a scalar, and v and a 3-vectors. A homology has 5 degrees of freedom which
correspond here to the scalar Î¼ and the directions of v and a. If H is parametrized by its
9 matrix entries, then the estimated H is unlikely to exactly be a homology. However,
if H is parametrized by Î¼, v and a (a total of 7 parameters) then the estimated H is
guaranteed to be a homology. This parametrization is consistent with a homology (it is
also an over-parametrization). We will return to the issues of consistent, local, minimal
and over-parametrization in later chapters. The issues are also discussed further in
appendix A6.9(p623).
Function specification
It has been seen in section 4.2.7 that a general class of estimation problems is concerned
with a measurement space IRN containing a model surface S. Given a measurement
X âˆˆ IRN the estimation task is to find the point bX lying on S closest to X. In the case
where a non-isotropic Gaussian error distribution is imposed on IRN, the word closest
is to be interpreted in terms of Mahalanobis distance. Iterative minimization methods
will now be described in terms of this estimation model. In iterative estimation through
parameter fitting, the model surface S is locally parametrized, and the parameters are
allowed to vary to minimize the distance to the measured point. More specifically,
(i) One has a measurement vector X âˆˆ IRN with covariance matrix .
112 4 Estimation â€“ 2D Projective Transformations
(ii) A set of parameters are represented as a vector P âˆˆ IRM.
(iii) A mapping f : IRM â†’ IRN is defined. The range of this mapping is (at least
locally) the model surface S in IRN representing the set of allowable measurements.
(iv) The cost function to be minimized is the squared Mahalanobis distance
kX âˆ’ f(P)k2
 = (X âˆ’ f(P))T

âˆ’1(X âˆ’ f(P)).
In effect, we are attempting to find a set of parameters P such that f(P) = X, or failing
that, to bring f(P) as close to X as possible, with respect to Mahalanobis distance.
The Levenbergâ€“Marquardt algorithm is a general tool for iterative minimization, when
the cost function to be minimized is of this type. We will now show how the various
different types of cost functions described in this chapter fit into this format.
Error in one image. Here one fixes the coordinates of points xi in the first image, and
varies H so as to minimize cost function (4.6â€“p94), namely
X
i
d(xâ€²
i, HÂ¯xi)2.
The measurement vector X is made up of the 2n inhomogeneous coordinates of the
points xâ€²
i. One may choose as parameters the vector h of entries of the homography
matrix H. The function f is defined by
f : h 7â†’ (Hx1, Hx2, . . . , Hxn)
where it is understood that here, and in the functions below, Hxi indicates the inhomogeneous
coordinates. One verifies that kX âˆ’ f(h)k2 is equal to (4.6â€“p94).
Symmetric transfer error. In the case of the symmetric cost function (4.7â€“p95)
X
i
d(xi, H
âˆ’1xâ€²
i)2 + d(xâ€²
i, Hxi)2
one chooses as measurement vector X the 4n-vector made up of the inhomogeneous
coordinates of the points xi followed by the inhomogeneous coordinates of the points
xâ€²
i. The parameter vector as before is the vector h of entries of H, and the function f is
defined by
f : h 7â†’ (H
âˆ’1xâ€²
1, . . . , H
âˆ’1xâ€²
n, Hx1, . . . , Hxn).
As before, we find that kX âˆ’ f(h)k2 is equal to (4.7â€“p95).
Reprojection error. Minimizing the cost function (4.8â€“p95) is more complex. The
difficulty is that it requires a simultaneous minimization over all choices of points Ë†xi
as well as the entries of the transformation matrix H. If there are many point correspondences,
then this becomes a very large minimization problem. Thus, the problem
may be parametrized by the coordinates of the points Ë†xi and the entries of the matrix
Ë†H â€“ a total of 2n + 9 parameters. The coordinates of Ë†xâ€²
i are not required, since they
are related to the other parameters by Ë†xâ€²
i = Ë†HË†xi. The parameter vector is therefore
4.5 Iterative minimization methods 113
P = (h, Ë†x1, . . . , Ë†xn). The measurement vector contains the inhomogeneous coordinates
of all the points xi and xâ€²
i. The function f is defined by
f : (h, Ë†x1, . . . , Ë†xn) 7â†’ (Ë†x1, Ë†xâ€²
1, . . . , Ë†xn, Ë†xâ€²
n)
where Ë†xâ€²
i = HË†xi. One verifies that kXâˆ’f(P)k2, with X a 4n-vector, is equal to the cost
function (4.8â€“p95). This cost function must be minimized over all 2n + 9 parameters.
Sampson approximation. In contrast with 2n + 9 parameters of reprojection error,
minimizing the error in one image (4.6â€“p94) or symmetric transfer error (4.7â€“p95)
requires a minimization over the 9 entries of the matrix H only â€“ in general a more
tractable problem. The Sampson approximation to reprojection error enables reprojection
error also to be minimized with only 9 parameters.
This is an important consideration, since the iterative solution of an m-parameter
non-linear minimization problem using a method such as Levenbergâ€“Marquardt involves
the solution of an m Ã— m set of linear equations at each iteration step. This is a
problem with complexity O(m3). Hence, it is appropriate to keep the size of m low.
The Sampson error avoids minimizing over the 2n+9 parameters of reprojection error
because effectively it determines the 2n variables {Ë†xi} for each particular choice of
h. Consequently the minimization then only requires the 9 parameters of h. In practice
this approximation gives excellent results provided the errors are small compared
to the measurements.
Initialization
An initial estimate for the parametrization may be found by employing a linear technique.
For example, the normalized DLT algorithm 4.2 directly provides H and thence
the 9-vector h used to parametrize the iterative minimization. In general if there are
n â‰¥ 4 correspondences, then all will be used in the linear solution. However, as will be
seen in section 4.7 on robust estimation, when the correspondences contain outliers it
may be advisable to use a carefully selected minimal set of correspondences (i.e. four
correspondences). Linear techniques or minimal solutions are the two initialization
techniques recommended in this book.
An alternative method that is sometimes used (for instance see [Horn-90, Horn-91])
is to carry out a sufficiently dense sampling of parameter space, iterating from each
sampled starting point and retaining the best result. This is only possible if the dimension
of the parameter space is sufficiently small. Sampling of parameter space may be
done either randomly, or else according to some pattern. Another initialization method
is simply to do without any effective initialization at all, starting the iteration at a given
fixed point in parameter space. This method is not often viable. Iteration is very likely
to fall into a false minimum or not converge. Even in the best case, the number of
iteration steps required will increase the further one starts from the final solution. For
this reason using a good initialization method is the best plan.
114 4 Estimation â€“ 2D Projective Transformations
Objective
Given n > 4 image point correspondences {xi â†” xâ€²
i}, determine the Maximum Likelihood
estimate Ë†H of the homography mapping between the images.
The MLE involves also solving for a set of subsidiary points {Ë†xi}, which minimize
X
i
d(xi, Ë†xi)2 + d(xâ€²
i, Ë†xâ€²
i)2
where Ë†xâ€²
i = Ë†HË†xi.
Algorithm
(i) Initialization: Compute an initial estimate of Ë†H to provide a starting point for the geometric
minimization. For example, use the linear normalized DLT algorithm 4.2, or
use RANSAC (section 4.7.1) to compute Ë†H from four point correspondences.
(ii) Geometric minimization of â€“ either Sampson error:
â€¢ Minimize the Sampson approximation to the geometric error (4.12â€“p99).
â€¢ The cost is minimized using the Newton algorithm of section A6.1(p597) or
Levenbergâ€“Marquardt algorithm of section A6.2(p600) over a suitable parametrization
of Ë†H. For example the matrix may be parametrized by its 9 entries.
or Gold Standard error:
â€¢ Compute an initial estimate of the subsidiary variables {Ë†xi} using the measured
points {xi} or (better) the Sampson correction to these points given by (4.11â€“p99).
â€¢ Minimize the cost X
i
d(xi, Ë†xi)2 + d(xâ€²
i, Ë†x
â€²
i)2
over Ë†H and Ë†xi, i = 1, . . . , n. The cost is minimized using the Levenbergâ€“Marquardt
algorithm over 2n+9 variables: 2n for the n 2D points Ë†xi, and 9 for the homography
matrix Ë†H.
â€¢ If the number of points is large then the sparse method of minimizing this cost function
given in section A6.4(p607) is the recommended approach.
Algorithm 4.3. The Gold Standard algorithm and variations for estimating H from image correspondences.
The Gold Standard algorithm is preferred to the Sampson method for 2D homography computation.
Iteration methods
There are various iterative methods for minimizing the chosen cost function, of which
the most popular are Newton iteration and the Levenbergâ€“Marquardt method. These
methods are described in appendix 6(p597). Other general methods for minimizing
a cost function are available, such as Powellâ€™s method and the simplex method both
described in [Press-88].
Summary. The ideas in this section are collected together in algorithm 4.3, which
describes the Gold Standard and Sampson methods for estimating the homography
mapping between point correspondences in two images.
4.6 Experimental comparison of the algorithms 115
a b c
Fig. 4.5. Three images of a plane which are used to compare methods of computing projective transformations
from corresponding points.
Method Pair 1 Pair 2
figure 4.5 a & b figure 4.5 a & c
Linear normalized 0.4078 0.6602
Gold Standard 0.4078 0.6602
Linear unnormalized 0.4080 26.2056
Homogeneous scaling 0.5708 0.7421
Sampson 0.4077 0.6602
Error in 1 view 0.4077 0.6602
Affine 6.0095 2.8481
Theoretical optimal 0.5477 0.6582
Table 4.1. Residual errors in pixels for the various algorithms.
4.6 Experimental comparison of the algorithms
The algorithms are compared for the images shown in figure 4.5. Table 4.1 shows the
results of testing several of the algorithms described in this chapter. Residual error is
shown for two pairs of images. The methods used are fairly self-explanatory, with a
few exceptions. The method â€œaffineâ€ was an attempt to fit the projective transformation
with an optimal affine mapping. The â€œoptimalâ€ is the ML estimate assuming a noise
level of one pixel.
The first pair of images are (a) and (b) of figure 4.5, with 55 point correspondences.
It appears that all methods work almost equally well (except the affine method). The
optimal residual is greater than the achieved results, because the noise level (unknown)
is less than one pixel.
Image (c) of figure 4.5 was produced synthetically by resampling (a), and the second
pair consists of (a) and (c) with 20 point correspondences. In this case, almost all
methods perform almost optimally, as shown in the table 4.1. The exception is the
affine method (expected to perform badly, since it is not an affine transformation) and
the unnormalized linear method. The unnormalized method is expected to perform
badly (though maybe not this badly). Just why it performs well in the first pair and
very badly for the second pair is not understood. In any case, it is best to avoid this
method and use a normalized linear or Gold Standard method.
A further evaluation is presented in figure 4.6. The transformation to be estimated is
the one that maps the chessboard image shown here to a square grid aligned with the
116 4 Estimation â€“ 2D Projective Transformations
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0 10 20 30 40 50 60 70
Residual Error
Number of Points
a b
0
1
2
3
4
5
6
7
0 2 4 6 8 10
Residual Error
Noise Level
0
0.5
1
1.5
2
0 0.5 1 1.5 2 2.5 3
Residual error
Noise level
c d
Fig. 4.6. Comparison of the DLT and Gold Standard algorithms to the theoretically optimal residual
error. (a) The homography is computed between a chessboard and this image. In all three graphs,
the result for the Gold Standard algorithm overlap and are indistinguishable from the theoretical minimum.
(b) Residual error as a function of the number of points. (c) The effect of varying noise level for
10 points, and (d) 50 points.
axes. As may be seen, the image is substantially distorted, with respect to a square grid.
For the experiments, randomly selected points in the image were chosen and matched
with the corresponding point on the square grid. The (normalized) DLT algorithm
and the Gold Standard algorithm are compared to the theoretical minimum or residual
error (see chapter 5). Note that for noise up to 5 pixels, the DLT algorithm performs
adequately. However, for a noise level of 10 pixels it fails. Note however that in a 200-
pixel image, an error of 10 pixels is extremely high. For less severe homographies,
closer to the identity map, the DLT performs almost as well as the Gold Standard
algorithm.
4.7 Robust estimation
Up to this point it has been assumed that we have been presented with a set of correspondences,
{xi â†” xâ€²
i}, where the only source of error is in the measurement of the
pointâ€™s position, which follows a Gaussian distribution. In many practical situations
this assumption is not valid because points are mismatched. The mismatched points
are outliers to the Gaussian error distribution. These outliers can severely disturb the
4.7 Robust estimation 117
b
d
a
c
a b
Fig. 4.7. Robust line estimation. The solid points are inliers, the open points outliers. (a) A leastsquares
(orthogonal regression) fit to the point data is severely affected by the outliers. (b) In the
RANSAC algorithm the support for lines through randomly selected point pairs is measured by the number
of points within a threshold distance of the lines. The dotted lines indicate the threshold distance.
For the lines shown the support is 10 for line ha, bi (where both of the points a and b are inliers); and
2 for line hc, di where the point c is an outlier.
estimated homography, and consequently should be identified. The goal then is to determine
a set of inliers from the presented â€œcorrespondencesâ€ so that the homography
can then be estimated in an optimal manner from these inliers using the algorithms described
in the previous sections. This is robust estimation since the estimation is robust
(tolerant) to outliers (measurements following a different, and possibly unmodelled,
error distribution).
4.7.1 RANSAC
We start with a simple example that can easily be visualized â€“ estimating a straight
line fit to a set of 2-dimensional points. This can be thought of as estimating a 1-
dimensional affine transformation, xâ€² = ax+b, between corresponding points lying on
two lines.
The problem, which is illustrated in figure 4.7a, is the following: given a set of 2D
data points, find the line which minimizes the sum of squared perpendicular distances
(orthogonal regression), subject to the condition that none of the valid points deviates
from this line by more than t units. This is actually two problems: a line fit to the data;
and a classification of the data into inliers (valid points) and outliers. The threshold t is
set according to the measurement noise (for example t = 3Ïƒ), and is discussed below.
There are many types of robust algorithms and which one to use depends to some extent
on the proportion of outliers. For example, if it is known that there is only one outlier,
then each point can be deleted in turn and the line estimated from the remainder. Here
we describe in detail a general and very successful robust estimator â€“ the RANdom
SAmple Consensus (RANSAC) algorithm of Fischler and Bolles [Fischler-81]. The
RANSAC algorithm is able to cope with a large proportion of outliers.
The idea is very simple: two of the points are selected randomly; these points define
a line. The support for this line is measured by the number of points that lie within a
distance threshold. This random selection is repeated a number of times and the line
with most support is deemed the robust fit. The points within the threshold distance are
the inliers (and constitute the eponymous consensus set). The intuition is that if one of
the points is an outlier then the line will not gain much support, see figure 4.7b.
118 4 Estimation â€“ 2D Projective Transformations
Furthermore, scoring a line by its support has the additional advantage of favouring
better fits. For example, the line ha, bi in figure 4.7b has a support of 10, whereas the
line ha, di, where the sample points are neighbours, has a support of only 4. Consequently,
even though both samples contain no outliers, the line ha, bi will be selected.
More generally, we wish to fit a model, in this case a line, to data, and the random
sample consists of a minimal subset of the data, in this case two points, sufficient to
determine the model. If the model is a planar homography, and the data a set of 2D
point correspondences, then the minimal subset consists of four correspondences. The
application of RANSAC to the estimation of a homography is described below.
As stated by Fischler and Bolles [Fischler-81] â€œThe RANSAC procedure is opposite
to that of conventional smoothing techniques: Rather than using as much of the data as
possible to obtain an initial solution and then attempting to eliminate the invalid data
points, RANSAC uses as small an initial data set as feasible and enlarges this set with
consistent data when possibleâ€.
The RANSAC algorithm is summarized in algorithm 4.4. Three questions immediately
arise:
Objective
Robust fit of a model to a data set S which contains outliers.
Algorithm
(i) Randomly select a sample of s data points from S and instantiate the model from this
subset.
(ii) Determine the set of data points Si which are within a distance threshold t of the model.
The set Si is the consensus set of the sample and defines the inliers of S.
(iii) If the size of Si (the number of inliers) is greater than some threshold T,
re-estimate the model using all the points in Si and terminate.
(iv) If the size of Si is less than T, select a new subset and repeat the above.
(v) After N trials the largest consensus set Si is selected, and the model is
re-estimated using all the points in the subset Si.
Algorithm 4.4. The RANSAC robust estimation algorithm, adapted from [Fischler-81]. A minimum of s
data points are required to instantiate the free parameters of the model. The three algorithm thresholds
t, T, and N are discussed in the text.
1. What is the distance threshold? We would like to choose the distance threshold, t,
such that with a probability Î± the point is an inlier. This calculation requires the probability
distribution for the distance of an inlier from the model. In practice the distance
threshold is usually chosen empirically. However, if it is assumed that the measurement
error is Gaussian with zero mean and standard deviation Ïƒ, then a value for t may
be computed. In this case the square of the point distance, d2
âŠ¥, is a sum of squared
Gaussian variables and follows a Ï‡2
m distribution with m degrees of freedom, where
m equals the codimension of the model. For a line the codimension is 1 â€“ only the
perpendicular distance to the line is measured. If the model is a point the codimension
is 2, and the square of the distance is the sum of squared x and y measurement errors.
4.7 Robust estimation 119
The probability that the value of a Ï‡2
m random variable is less than k2 is given by the cumulative
chi-squared distribution, Fm(k2) =
R k2
0 Ï‡2
m(Î¾)dÎ¾. Both of these distributions
are described in section A2.2(p566). From the cumulative distribution
(
inlier d2
âŠ¥ < t2
outlier d2
âŠ¥ â‰¥ t2 with t2 = Fâˆ’1
m (Î±)Ïƒ2. (4.17)
Usually Î± is chosen as 0.95, so that there is a 95% probability that the point is an inlier.
This means that an inlier will only be incorrectly rejected 5% of the time. Values of t
for Î± = 0.95 and for the models of interest in this book are tabulated in table 4.2.
Codimension m Model t2
1 line, fundamental matrix 3.84 Ïƒ2
2 homography, camera matrix 5.99 Ïƒ2
3 trifocal tensor 7.81 Ïƒ2
Table 4.2. The distance threshold t2 = Fâˆ’1
m (Î±)Ïƒ2 for a probability of Î± = 0.95 that the point (correspondence)
is an inlier.
2. How many samples? It is often computationally infeasible and unnecessary to try
every possible sample. Instead the number of samples N is chosen sufficiently high
to ensure with a probability, p, that at least one of the random samples of s points is
free from outliers. Usually p is chosen at 0.99. Suppose w is the probability that any
selected data point is an inlier, and thus Ç« = 1âˆ’w is the probability that it is an outlier.
Then at least N selections (each of s points) are required, where (1 âˆ’ ws)N = 1 âˆ’ p,
so that
N = log(1 âˆ’ p)/ log(1 âˆ’ (1 âˆ’ Ç«)s). (4.18)
Table 4.3 gives examples of N for p = 0.99 for a given s and Ç«.
Sample size Proportion of outliers Ç«
s 5% 10% 20% 25% 30% 40% 50%
2 2 3 5 6 7 11 17
3 3 4 7 9 11 19 35
4 3 5 9 13 17 34 72
5 4 6 12 17 26 57 146
6 4 7 16 24 37 97 293
7 4 8 20 33 54 163 588
8 5 9 26 44 78 272 1177
Table 4.3. The number N of samples required to ensure, with a probability p = 0.99, that at least one
sample has no outliers for a given size of sample, s, and proportion of outliers, Ç«.
Example 4.4. For the line-fitting problem of figure 4.7 there are n = 12 data points, of
120 4 Estimation â€“ 2D Projective Transformations
which two are outliers so that Ç« = 2/12 = 1/6. From table 4.3 for a minimal subset
of size s = 2, at least N = 5 samples are required. This should be compared with
the cost of exhaustively trying every point pair, in which case (12
2 ) = 66 samples are
required (the notation (n2
) means the number of choices of 2 among n, specifically,
(n2
) = n(n âˆ’ 1)/2). â–³
Note
(i) The number of samples is linked to the proportion rather than number of outliers.
This means that the number of samples required may be smaller than the
number of outliers. Consequently the computational cost of the sampling can
be acceptable even when the number of outliers is large.
(ii) The number of samples increases with the size of the minimal subset (for a
given Ç« and p). It might be thought that it would be advantageous to use more
than the minimal subset, three or more points in the case of a line, because then
a better estimate of the line would be obtained, and the measured support would
more accurately reflect the true support. However, this possible advantage in
measuring support is generally outweighed by the severe increase in computational
cost incurred by the increase in the number of samples.
3. How large is an acceptable consensus set? A rule of thumb is to terminate if the
size of the consensus set is similar to the number of inliers believed to be in the data
set, given the assumed proportion of outliers, i.e. for n data points T = (1 âˆ’ Ç«)n. For
the line-fitting example of figure 4.7 a conservative estimate of Ç« is Ç« = 0.2, so that
T = (1.0 âˆ’ 0.2)12 = 10.
Determining the number of samples adaptively. It is often the case that Ç«, the
fraction of data consisting of outliers, is unknown. In such cases the algorithm is
initialized using a worst case estimate of Ç«, and this estimate can then be updated as
larger consistent sets are found. For example, if the worst case guess is Ç« = 0.5 and
a consensus set with 80% of the data is found as inliers, then the updated estimate is
Ç« = 0.2.
This idea of â€œprobingâ€ the data via the consensus sets can be applied repeatedly in
order to adaptively determine the number of samples, N. To continue the example
above, the worst case estimate of Ç« = 0.5 determines an initial N according to (4.18).
When a consensus set containing more than 50% of the data is found, we then know
that there is at least that proportion of inliers. This updated estimate of Ç« determines a
reduced N from (4.18). This update is repeated at each sample, and whenever a consensus
set with Ç« lower than the current estimate is found, then N is again reduced. The
algorithm terminates as soon as N samples have been performed. It may occur that a
sample is found for which Ç« determines an N less than the number of samples that have
already been performed. In such a case sufficient samples have been performed and the
algorithm terminates. In pseudo-code the adaptive computation of N is summarized
in algorithm 4.5.
This adaptive approach works very well and in practice covers the questions of both
4.7 Robust estimation 121
â€¢ N = âˆ, sample count= 0.
â€¢ While N > sample count Repeat
â€“ Choose a sample and count the number of inliers.
â€“ Set Ç« = 1 âˆ’ (number of inliers)/(total number of points)
â€“ Set N from Ç« and (4.18) with p = 0.99.
â€“ Increment the sample count by 1.
â€¢ Terminate.
Algorithm 4.5. Adaptive algorithm for determining the number of RANSAC samples.
C
D
B
A
C
D
B
A
a b
Fig. 4.8. Robust ML estimation. The grey points are classified as inliers to the line. (a) A line defined
by points hA,Bi has a support of four (from points {A,B,C,D}). (b) The ML line fit (orthogonal
least-squares) to the four points. This is a much improved fit over that defined by hA,Bi. 10 points are
classified as inliers.
the number of samples and terminating the algorithm. The initial Ç« can be chosen
as 1.0, in which case the initial N will be infinite. It is wise to use a conservative
probability p such as 0.99 in (4.18). Table 4.4 on page 127 gives example Ç«â€™s and Nâ€™s
when computing a homography.
4.7.2 Robust Maximum Likelihood estimation
The RANSAC algorithm partitions the data set into inliers (the largest consensus set)
and outliers (the rest of the data set), and also delivers an estimate of the model, M0,
computed from the minimal set with greatest support. The final step of the RANSAC
algorithm is to re-estimate the model using all the inliers. This re-estimation should be
optimal and will involve minimizing a ML cost function, as described in section 4.3.
In the case of a line, ML estimation is equivalent to orthogonal regression, and a closed
form solution is available. In general, though, the ML estimation involves iterative
minimization, and the minimal set estimate, M0, provides the starting point.
The only drawback with this procedure, which is often the one adopted, is that the
inlierâ€“outlier classification is irrevocable. After the model has been optimally fitted to
the consensus set, there may well be additional points which would now be classified
as inliers if the distance threshold was applied to the new model. For example, suppose
the line hA,Bi in figure 4.8 was selected by RANSAC. This line has a support of
four points, all inliers. After the optimal fit to these four points, there are now 10 points
which would correctly be classified as inliers. These two steps: optimal fit to inliers; reclassify
inliers using (4.17); can then be iterated until the number of inliers converges.
122 4 Estimation â€“ 2D Projective Transformations
A least-squares fit with inliers weighted by their distance to the model is often used at
this stage.
Robust cost function. An alternative to minimizing C =
P
i d2
âŠ¥i over the inliers is to
minimize a robust version including all data. A suitable robust cost function is
D =
X
i
Î³ (dâŠ¥i) with Î³(e) =
(
e2 e2 < t2 inlier
t2 e2 â‰¥ t2 outlier
(4.19)
Here dâŠ¥i are point errors and Î³(e) is a robust cost function [Huber-81] where outliers
are given a fixed cost. The Ï‡2 motivation for the threshold is the same as that of (4.17),
where t2 is defined. The quadratic cost for inliers arises from the Gaussian error model,
as described in section 4.3. The constant cost for outliers in the robust cost function
arises from the assumption that outliers follow a diffuse or uniform distribution, the loglikelihood
of which is a constant. It might be thought that outliers could be excluded
from the cost function by simply thresholding on dâŠ¥i. The problem with thresholding
alone is that it would result in only outliers being included because they would incur
no cost.
The cost function D allows the minimization to be conducted on all points whether
they are outliers or inliers. At the start of the iterative minimization D differs from C
only by a constant (given by 4 times the number of outliers). However, as the minimization
progresses outliers can be redesignated inliers, and this typically occurs in
practice. A discussion and comparison of cost functions is given in appendix A6.8-
(p616).
4.7.3 Other robust algorithms
In RANSAC a model instantiated from a minimal set is scored by the number of data
points within a threshold distance. An alternative is to score the model by the median
of the distances to all points in the data. The model with least median is then
selected. This is Least Median of Squares (LMS) estimation, where, as in RANSAC,
minimum size subset samples are selected randomly with the number of samples obtained
from (4.18). The advantage of LMS is that it requires no setting of thresholds or
a priori knowledge of the variance of the error. The disadvantage of LMS is that it fails
if more than half the data is outlying, for then the median distance will be to an outlier.
The solution is to use the proportion of outliers to determine the selection distance. For
example if there are 50% outliers then a distance below the median value (the quartile
say) should be used.
Both the RANSAC and LMS algorithms are able to cope with a large proportion of
outliers. If the number of outliers is small, then other robust methods may well be more
efficient. These include case deletion, where each point in turn is deleted and the model
fitted to the remaining data; and iterative weighted least-squares, where a data pointâ€™s
influence on the fit is weighted inversely by its residual. Generally these methods
are not recommended. Both Torr [Torr-95b] and Xu and Zhang [Xu-96] describe and
compare various robust estimators for estimating the fundamental matrix.
4.8 Automatic computation of a homography 123
Objective
Compute the 2D homography between two images.
Algorithm
(i) Interest points: Compute interest points in each image.
(ii) Putative correspondences: Compute a set of interest point matches based on proximity
and similarity of their intensity neighbourhood.
(iii) RANSAC robust estimation: Repeat for N samples, where N is determined adaptively
as in algorithm 4.5:
(a) Select a random sample of 4 correspondences and compute the homography H.
(b) Calculate the distance dâŠ¥ for each putative correspondence.
(c) Compute the number of inliers consistent with H by the number of correspondences
for which dâŠ¥ < t = âˆš5.99 Ïƒ pixels.
Choose the H with the largest number of inliers. In the case of ties choose the solution
that has the lowest standard deviation of inliers.
(iv) Optimal estimation: re-estimate H from all correspondences classified as inliers, by
minimizing the ML cost function (4.8â€“p95) using the Levenbergâ€“Marquardt algorithm
of section A6.2(p600).
(v) Guided matching: Further interest point correspondences are now determined using
the estimated H to define a search region about the transferred point position.
The last two steps can be iterated until the number of correspondences is stable.
Algorithm 4.6. Automatic estimation of a homography between two images using RANSAC.
4.8 Automatic computation of a homography
This section describes an algorithm to automatically compute a homography between
two images. The input to the algorithm is simply the images, with no other a priori
information required; and the output is the estimated homography together with a set
of interest points in correspondence. The algorithm might be applied, for example, to
two images of a planar surface or two images acquired by rotating a camera about its
centre.
The first step of the algorithm is to compute interest points in each image. We are
then faced with a â€œchicken and eggâ€ problem: once the correspondence between the
interest points is established the homography can be computed; conversely, given the
homography the correspondence between the interest points can easily be established.
This problem is resolved by using robust estimation, here RANSAC, as a â€œsearch engineâ€.
The idea is first to obtain by some means a set of putative point correspondences.
It is expected that a proportion of these correspondences will in fact be mismatches.
RANSAC is designed to deal with exactly this situation â€“ estimate the homography
and also a set of inliers consistent with this estimate (the true correspondences), and
outliers (the mismatches).
The algorithm is summarized in algorithm 4.6, with an example of its use shown
in figure 4.9, and the steps described in more detail below. Algorithms with essentially
the same methodology enable the automatic computation of the fundamental matrix
and trifocal tensor directly from image pairs and triplets respectively. This computation
is described in chapter 11 and chapter 16.
124 4 Estimation â€“ 2D Projective Transformations
Determining putative correspondences. The aim, in the absence of any knowledge
of the homography, is to provide an initial point correspondence set. A good proportion
of these correspondences should be correct, but the aim is not perfect matching, since
RANSAC will later be used to eliminate the mismatches. Think of these as â€œseedâ€
correspondences. These putative correspondences are obtained by detecting interest
points independently in each image, and then matching these interest points using a
combination of proximity and similarity of intensity neighbourhoods as follows. For
brevity, the interest points will be referred to as â€˜cornersâ€™. However, these corners need
not be images of physical corners in the scene. The corners are defined by a minimum
of the image auto-correlation function.
For each corner at (x, y) in image 1 the match with highest neighbourhood crosscorrelation
in image 2 is selected within a square search region centred on (x, y). Symmetrically,
for each corner in image 2 the match is sought in image 1. Occasionally
there will be a conflict where a corner in one image is â€œclaimedâ€ by more than one
corner in the other. In such cases a â€œwinner takes allâ€ scheme is applied and only the
match with highest cross-correlation is retained.
A variation on the similarity measure is to use Squared Sum of intensity Differences
(SSD) instead of (normalized) Cross-Correlation (CC). CC is invariant to the affine
mapping of the intensity values (i.e. I 7â†’ Î±I+Î², scaling plus offset) which often occurs
in practice between images. SSD is not invariant to this mapping. However, SSD is
often preferred when there is small variation in intensity between images, because it is
a more sensitive measure than CC and is computationally cheaper.
RANSAC for a homography. The RANSAC algorithm is applied to the putative
correspondence set to estimate the homography and the (inlier) correspondences which
are consistent with this estimate. The sample size is four, since four correspondences
determine a homography. The number of samples is set adaptively as the proportion of
outliers is determined from each consensus set, as described in algorithm 4.5.
There are two issues: what is the â€œdistanceâ€ in this case? and how should the samples
be selected?
(i) Distance measure: The simplest method of assessing the error of a correspondence
from a homography H is to use the symmetric transfer error, i.e.
d2
transfer = d(x, Hâˆ’1xâ€²)2 + d(xâ€², Hx)2, where x â†” xâ€² is the point correspondence.
A better, though more expensive, distance measure is the reprojection
error, d2
âŠ¥ = d(x, Ë†x)2 +d(xâ€², Ë†xâ€²)2, where Ë†xâ€² = HË†x is the perfect correspondence.
This measure is more expensive because Ë†x must also be computed. A further
alternative is Sampson error.
(ii) Sample selection: There are two issues here. First, degenerate samples should
be disregarded. For example, if three of the four points are collinear then a
homography cannot be computed; second, the sample should consist of points
with a good spatial distribution over the image. This is because of the extrapolation
problem â€“ an estimated homography will accurately map the region
straddled by the computation points, but the accuracy generally deteriorates
4.8 Automatic computation of a homography 125
with distance from this region (think of four points in the very top corner of the
image). Distributed spatial sampling can be implemented by tiling the image
and ensuring, by a suitable weighting of the random sampler, that samples with
points lying in different tiles are the more likely.
Robust ML estimation and guided matching. The aim of this final stage is twofold:
first, to obtain an improved estimate of the homography by using all the inliers in
the estimation (rather than only the four points of the sample); second, to obtain more
inlying matches from the putative correspondence set because a more accurate homography
is available. An improved estimate of the homography is then computed from
the inliers by minimizing an ML cost function. This final stage can be implemented
in two ways. One way is to carry out an ML estimation on the inliers, then recompute
the inliers using the new estimated H, and repeat this cycle until the number of inliers
converges. The ML cost function minimization is carried out using the Levenbergâ€“
Marquardt algorithm described in section A6.2(p600). The alternative is to estimate
the homography and inliers simultaneously by minimizing a robust ML cost function
of (4.19) as described in section 4.7.2. The disadvantage of the simultaneous approach
is the computational effort incurred in the minimization of the cost function. For this
reason the cycle approach is usually the more attractive.
4.8.1 Application domain
The algorithm requires that interest points can be recovered fairly uniformly across the
image, and this in turn requires scenes and resolutions which support this requirement.
Scenes should be lightly textured â€“ images of blank walls are not ideal.
The search window proximity constraint places an upper limit on the image motion
of corners (the disparity) between views. However, the algorithm is not defeated if this
constraint is not applied, and in practice the main role of the proximity constraint is to
reduce computational complexity, as a smaller search window means that fewer corner
matches must be evaluated.
Ultimately the scope of the algorithm is limited by the success of the corner neighbourhood
similarity measure (SSD or CC) in providing disambiguation between correspondences.
Failure generally results from lack of spatial invariance: the measures
are only invariant to image translation, and are severely degraded by transformations
outside this class such as image rotation or significant differences in foreshortening
between images. One solution is to use measures with a greater invariance to the homography
mapping between images, for example measures which are rotationally invariant.
An alternative solution is to use an initial estimate of the homography to map
between intensity neighbourhoods. Details are beyond the scope of this discussion,
but are provided in [Pritchett-98, Schmid-98]. The use of robust estimation confers
moderate immunity to independent motion, changes in shadows, partial occlusions etc.
126 4 Estimation â€“ 2D Projective Transformations
a b
c d
e f
g h
Fig. 4.9. Automatic computation of a homography between two images using RANSAC. The motion
between views is a rotation about the camera centre so the images are exactly related by a homography.
(a) (b) left and right images of Keble College, Oxford. The images are 640 Ã— 480 pixels. (c) (d)
detected corners superimposed on the images. There are approximately 500 corners on each image. The
following results are superimposed on the left image: (e) 268 putative matches shown by the line linking
corners, note the clear mismatches; (f) outliers â€“ 117 of the putative matches; (g) inliers â€“ 151 correspondences
consistent with the estimated H; (h) final set of 262 correspondences after guided matching
and MLE.
4.9 Closure 127
4.8.2 Implementation and run details
Interest points are obtained using the Harris [Harris-88] corner detector. This detector
localizes corners to sub-pixel accuracy, and it has been found empirically that the
correspondence error is usually less than a pixel [Schmid-98].
When obtaining seed correspondences, in the putative correspondence stage of the
algorithm, the threshold on the neighbourhood similarity measure for match acceptance
is deliberately conservative to minimize incorrect matches (the SSD threshold is 20).
For the guided matching stage this threshold is relaxed (it is doubled) so that additional
putative correspondences are available.
Number of 1 âˆ’ Ç« Adaptive
inliers N
6 2% 20,028,244
10 3% 2,595,658
44 16% 6,922
58 21% 2,291
73 26% 911
151 56% 43
Table 4.4. The results of the adaptive algorithm 4.5 used during RANSAC to compute the homography
for figure 4.9. N is the total number of samples required as the algorithm runs for p = 0.99 probability
of no outliers in the sample. The algorithm terminated after 43 samples.
For the example of figure 4.9 the images are 640Ã—480 pixels, and the search window
Â±320 pixels, i.e. the entire image. Of course a much smaller search window could have
been used given the actual point disparities in this case. Often in video sequences a
search window of Â±40 pixels suffices (i.e. a square of side 80 centred on the current
position). The inlier threshold was t = 1.25 pixels.
A total of 43 samples were required, with the sampling run as shown in table 4.4.
The guided matching required two iterations of the MLEâ€“inlier classification cycle.
The RMS values for dâŠ¥ pixel error were 0.23 before the MLE and 0.19 after. The
Levenbergâ€“Marquardt algorithm required 10 iterations.
4.9 Closure
This chapter has illustrated the issues and techniques that apply to estimating the tensors
representing multiple view relations. These ideas will reoccur in each of the computation
chapters throughout the book. In each case there are a minimal number of
correspondences required; degenerate configurations that should be avoided; algebraic
and geometric errors that can be minimized when more than the minimal number of
correspondences are available; parametrizations that enforce internal constraints on the
tensor etc.
4.9.1 The literature
The DLT algorithm dates back at least to Sutherland [Sutherland-63]. Sampsonâ€™s classic
paper on conic fitting (an improvement on the equally classic Bookstein algorithm)
128 4 Estimation â€“ 2D Projective Transformations
appeared in [Sampson-82]. Normalization was made public in the Computer Vision
literature by Hartley [Hartley-97c].
Related reading on numerical methods may be found in the excellent Numerical
Recipes in C [Press-88], and also Gill and Murray [Gill-78] for iterative minimization.
Fischler and Bollesâ€™ [Fischler-81] RANSAC was one of the earliest robust algorithms,
and in fact was developed to solve a Computer Vision problem (pose from
3 points). The original paper is very clearly argued and well worth reading. Other
background material on robustness may be found in Rousseeuw [Rousseeuw-87]. The
primary application of robust estimation in computer vision was to estimating the fundamental
matrix (chapter 11), by Torr and Murray [Torr-93] using RANSAC, and,
Zhang et al. [Zhang-95] using LMS. The automatic ML estimation of a homography
was described by Torr and Zisserman [Torr-98].
4.9.2 Notes and exercises
(i) Computing homographies of IPn. The derivation of (4.1â€“p89) and (4.3â€“p89)
assumed that the dimension of xâ€²
i is three, so that the cross-product is defined.
However, (4.3) may be derived in a way that generalizes to all dimensions.
Assuming that wâ€²
i = 1, we may solve for the unknown scale factor explicitly by
writing Hxi = k(xi, yi, 1)T. From the third coordinate we obtain k = h3Txi,
and substituting this into the original equation gives
 
h1Txi
h2Txi
!
=
 
xâ€²
ih3Txi
yâ€²
ih3Txi
!
which leads directly to (4.3).
(ii) Computing homographies for ideal points. If one of the points xâ€²
i is an
ideal point, so that wâ€²
i = 0, then the pair of equations (4.3) collapses to a single
equation although (4.1) does contain two independent equations. To avoid such
degeneracy, while including only the minimum number of equations, a good
way to proceed is as follows. We may rewrite the equation xâ€²
i = Hxi as
[xâ€²
i]âŠ¥
Hxi = 0
where [xâ€²
i]âŠ¥ is a matrix with rows orthogonal to xâ€²
i so that [xâ€²
i]âŠ¥xâ€²
i = 0. Each
row of [xâ€²
i]âŠ¥ leads to a separate linear equation in the entries of H. The matrix
[xâ€²
i]âŠ¥ may be obtained by deleting the first row of an orthogonal matrix M satisfying
Mxâ€²
i = (1, 0, . . . , 0)T. A Householder matrix (see section A4.1.2(p580))
is an easily constructed matrix with the desired property.
(iii) Scaling unbounded point sets. In the case of points at or near infinity in a
plane, it is neither reasonable nor feasible to normalize coordinates using the
isotropic (or non-isotropic) scaling schemes presented in this chapter, since the
centroid and scale are infinite or near infinite. A method that seems to give
good results is to normalize the set of points xi = (xi, yi,wi)T such that
X
i
xi =
X
i
yi = 0 ;
X
i
x2i
+ y2
i = 2
X
i
w2
i ; x2i
+ y2
i + w2
i = 1âˆ€i
4.9 Closure 129
Note that the coordinates xi and yi appearing here are the homogeneous coordinates,
and the conditions no longer imply that the centroid is at the origin.
Investigate methods of achieving this normalization, and evaluate its properties.
(iv) Transformation invariance of DLT. We consider computation of a 2D homography
by minimizing algebraic error kAhk (see (4.5â€“p94)) subject to various
constraints. Prove the following cases:
(a) If kAhk is minimized subject to the constraint h9 = H33 = 1, then the
result is invariant under change of scale but not translation of coordinates.
(b) If instead the constraint is H2 31 + H2 32 = 1 then the result is similarity
invariant.
(c) Affine case: The same is true for the constraint H31 = H32 = 0; H33 = 1.
(v) Expressions for image coordinate derivatives. For the map xâ€² =
(xâ€², yâ€²,wâ€²)T = Hx, derive the following expressions (where Ëœxâ€² = (Ëœxâ€², Ëœyâ€²)T =
(xâ€²/wâ€², yâ€²/wâ€²)T are the inhomogeneous coordinates of the image point):
(a) Derivative wrt x
âˆ‚Ëœxâ€²/âˆ‚x =
1
wâ€²
"
h1T âˆ’ Ëœxâ€²h3T
h2T âˆ’ Ëœyâ€²h3T
#
(4.20)
where hjT is the jâˆ’th row of H.
(b) Derivative wrt H
âˆ‚Ëœxâ€²/âˆ‚h =
1
wâ€²
"
xT 0 âˆ’Ëœxâ€²xT
0 xT âˆ’Ëœyâ€²xT
#
(4.21)
with h as defined in (4.2â€“p89).
(vi) Sampson error with non-isotropic error distributions. The derivation of
Sampson error in section 4.2.6(p98) assumed that points were measured with
circular error distributions. In the case where the point X = (x, y, xâ€², yâ€²) is
measured with covariance matrix X it is appropriate instead to minimize the
Mahalanobis norm kÎ´Xk2
X = Î´T
X
âˆ’1
X
Î´X. Show that in this case the formulae
corresponding to (4.11â€“p99) and (4.12â€“p99) are
Î´X = âˆ’XJ
T(JXJ
T)âˆ’1Ç« (4.22)
and
kÎ´Xk2
X = Ç«T(JXJT)âˆ’1Ç«. (4.23)
Note that if the measurements in the two images are independent, then the covariance
matrix X will be block-diagonal with two 2 Ã— 2 diagonal blocks corresponding
to the two images.
(vii) Sampson error programming hint. In the case of 2D homography estimation,
and in fact every other similar problem considered in this book, the cost
function CH(X) = A(X)h of section 4.2.6(p98) is multilinear in the coordinates
130 4 Estimation â€“ 2D Projective Transformations
Objective
Given n â‰¥ 4 image point correspondences {xi â†” xâ€²
i}, determine the affine homography HA
which minimizes reprojection error in both images (4.8â€“p95).
Algorithm
(a) Express points as inhomogeneous 2-vectors. Translate the points xi by a translation t
so that their centroid is at the origin. Do the same to the points xâ€²
i by a translation tâ€².
Henceforth work with the translated coordinates.
(b) Form the n Ã— 4 matrix A whose rows are the vectors
XT
i = (xT
i , xâ€²
i
T) = (xi, yi, xâ€²
i, yâ€²
i).
(c) Let V1 and V2 be the right singular-vectors of A corresponding to the two largest (sic)
singular values.
(d) Let H2Ã—2 = CBâˆ’1, where B and C are the 2 Ã— 2 blocks such that
[V1V2] =

B
C

.
(e) The required homography is
HA =

H2Ã—2 H2Ã—2t âˆ’ tâ€²
0T 1

,
and the corresponding estimate of the image points is given by
bX
i = (V1VT
1 + V2VT
2 )Xi
Algorithm 4.7. The Gold Standard Algorithm for estimating an affine homography HA from image correspondences.
of X. This means that the partial derivative âˆ‚CH(X)/âˆ‚X may be very simply
computed. For instance, the derivative
âˆ‚CH(x, y, xâ€², yâ€²)/âˆ‚x = CH(x + 1, y, xâ€², yâ€²) âˆ’ CH(x, y, xâ€², yâ€²)
is exact, not a finite difference approximation. This means that for programming
purposes, one does not need to code a special routine for taking
derivatives â€“ the routine for computing CH(X) will suffice. Denoting by Ei
the vector containing 1 in the i-th position, and otherwise 0, one sees that
âˆ‚CH(X)/âˆ‚Xi = CH(X + Ei) âˆ’ CH(X), and further
JJ
T =
X
i
(CH(X + Ei) âˆ’ CH(X)) (CH(X + Ei) âˆ’ CH(X))T .
Also note that computationally it is more efficient to solve JJTÎ» = âˆ’Ç« directly
for Î», rather than take the inverse as Î» = âˆ’(JJT)âˆ’1Ç«.
(viii) Minimizing geometric error for affine transformations. Given a set of
correspondences (xi, yi) â†” (xâ€²
i, yâ€²
i), find an affine transformation HA that minimizes
geometric error (4.8â€“p95). We will step through the derivation of a linear
algorithm based on Sampsonâ€™s approximation which is exact in this case. The
complete method is summarized in algorithm 4.7.
4.9 Closure 131
(a) Show that the optimum affine transformation takes the centroid of the xi
to the centroid of xâ€²
i, so by translating the points to have their centroid
at the origin, the translation part of the transformation is determined. It
is only necessary then to determine the upper-left 2 Ã— 2 submatrix H2Ã—2
of HA, which represents the linear part of the transformation.
(b) The point Xi = (xT
i , xâ€²
i
T)T lies on VH if and only if [H2Ã—2|âˆ’I2Ã—2]X = 0.
So VH is a codimension-2 subspace of IR4.
(c) Any codimension-2 subspace may be expressed as [H2Ã—2| âˆ’ I]X = 0
for suitable H2Ã—2. Thus given measurements Xi, the estimation task is
equivalent to finding the best-fitting codimension-2 subspace.
(d) Given a matrix M with rows X
T
i , the best-fitting subspace to the Xi is
spanned by the singular vectors V1 and V2 corresponding to the two
largest singular values of M.
(e) The H2Ã—2 corresponding to the subspace spanned by V1 and V2 is found
by solving the equations [H2Ã—2| âˆ’ I][V1V2] = 0.
(ix) Computing homographies of IP3 from line correspondences. Consider
computing a 4 Ã— 4 homography H from lines correspondences alone, assuming
the lines are in general position in IP3. There are two questions: how many
correspondences are required?, and how to formulate the algebraic constraints
to obtain a solution for H? It might be thought that four line correspondences
would be sufficient because each line in IP3 has four degrees of freedom, and
thus four lines should provide 4 Ã— 4 = 16 constraints on the 15 degrees of
freedom of H. However, a configuration of four lines is degenerate (see section
4.1.3(p91)) for computing the transformation, as there is a 2D isotropy subgroup.
This is discussed further in [Hartley-94c]. Equations linear in H can be
obtained in the following way:
Ï€T
i
HXj = 0 , i = 1, 2, j = 1, 2 ,
where H transfers a line defined by the two points (X1,X2) to a line defined
by the intersection of the two planes (Ï€1,Ï€2). This method was derived in
[Oskarsson-02], where more details are to be found.
</p><p>
    </body>
</html>